{
  "best_global_step": 43986,
  "best_metric": 0.8697153379881838,
  "best_model_checkpoint": "./distilbert_lora_email_priority_classifier_v2\\checkpoint-43986",
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 43986,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0034101759650797983,
      "grad_norm": 8.266585350036621,
      "learning_rate": 0.00019977720183694814,
      "loss": 1.3107,
      "step": 50
    },
    {
      "epoch": 0.006820351930159597,
      "grad_norm": 4.278303146362305,
      "learning_rate": 0.00019954985677260947,
      "loss": 1.1795,
      "step": 100
    },
    {
      "epoch": 0.010230527895239394,
      "grad_norm": 5.746740818023682,
      "learning_rate": 0.00019932251170827082,
      "loss": 1.146,
      "step": 150
    },
    {
      "epoch": 0.013640703860319193,
      "grad_norm": 4.569372653961182,
      "learning_rate": 0.00019909516664393218,
      "loss": 1.0731,
      "step": 200
    },
    {
      "epoch": 0.01705087982539899,
      "grad_norm": 4.441202640533447,
      "learning_rate": 0.0001988678215795935,
      "loss": 0.9402,
      "step": 250
    },
    {
      "epoch": 0.02046105579047879,
      "grad_norm": 7.745151996612549,
      "learning_rate": 0.00019864047651525486,
      "loss": 0.9217,
      "step": 300
    },
    {
      "epoch": 0.023871231755558588,
      "grad_norm": 4.186746597290039,
      "learning_rate": 0.0001984131314509162,
      "loss": 0.8703,
      "step": 350
    },
    {
      "epoch": 0.027281407720638386,
      "grad_norm": 2.582688570022583,
      "learning_rate": 0.00019818578638657757,
      "loss": 0.8524,
      "step": 400
    },
    {
      "epoch": 0.03069158368571818,
      "grad_norm": 3.4113364219665527,
      "learning_rate": 0.0001979584413222389,
      "loss": 0.86,
      "step": 450
    },
    {
      "epoch": 0.03410175965079798,
      "grad_norm": 3.434750556945801,
      "learning_rate": 0.00019773109625790026,
      "loss": 0.8349,
      "step": 500
    },
    {
      "epoch": 0.03751193561587778,
      "grad_norm": 4.017282485961914,
      "learning_rate": 0.0001975037511935616,
      "loss": 0.8313,
      "step": 550
    },
    {
      "epoch": 0.04092211158095758,
      "grad_norm": 5.340073585510254,
      "learning_rate": 0.00019727640612922294,
      "loss": 0.7773,
      "step": 600
    },
    {
      "epoch": 0.04433228754603737,
      "grad_norm": 6.071038722991943,
      "learning_rate": 0.0001970490610648843,
      "loss": 0.7896,
      "step": 650
    },
    {
      "epoch": 0.047742463511117175,
      "grad_norm": 2.45681095123291,
      "learning_rate": 0.00019682171600054563,
      "loss": 0.744,
      "step": 700
    },
    {
      "epoch": 0.05115263947619697,
      "grad_norm": 10.198076248168945,
      "learning_rate": 0.00019659437093620698,
      "loss": 0.8083,
      "step": 750
    },
    {
      "epoch": 0.05456281544127677,
      "grad_norm": 5.621272563934326,
      "learning_rate": 0.0001963670258718683,
      "loss": 0.7523,
      "step": 800
    },
    {
      "epoch": 0.05797299140635657,
      "grad_norm": 4.9213337898254395,
      "learning_rate": 0.00019613968080752967,
      "loss": 0.757,
      "step": 850
    },
    {
      "epoch": 0.06138316737143636,
      "grad_norm": 4.327526092529297,
      "learning_rate": 0.00019591233574319102,
      "loss": 0.7252,
      "step": 900
    },
    {
      "epoch": 0.06479334333651617,
      "grad_norm": 6.664350986480713,
      "learning_rate": 0.00019568499067885238,
      "loss": 0.7303,
      "step": 950
    },
    {
      "epoch": 0.06820351930159596,
      "grad_norm": 10.726407051086426,
      "learning_rate": 0.0001954576456145137,
      "loss": 0.7714,
      "step": 1000
    },
    {
      "epoch": 0.07161369526667576,
      "grad_norm": 4.291638374328613,
      "learning_rate": 0.00019523030055017507,
      "loss": 0.7036,
      "step": 1050
    },
    {
      "epoch": 0.07502387123175557,
      "grad_norm": 6.5926971435546875,
      "learning_rate": 0.0001950029554858364,
      "loss": 0.7046,
      "step": 1100
    },
    {
      "epoch": 0.07843404719683536,
      "grad_norm": 5.379021167755127,
      "learning_rate": 0.00019477561042149778,
      "loss": 0.6959,
      "step": 1150
    },
    {
      "epoch": 0.08184422316191516,
      "grad_norm": 2.6018338203430176,
      "learning_rate": 0.0001945482653571591,
      "loss": 0.6931,
      "step": 1200
    },
    {
      "epoch": 0.08525439912699495,
      "grad_norm": 6.098622798919678,
      "learning_rate": 0.00019432092029282046,
      "loss": 0.8133,
      "step": 1250
    },
    {
      "epoch": 0.08866457509207475,
      "grad_norm": 4.701504230499268,
      "learning_rate": 0.0001940935752284818,
      "loss": 0.7241,
      "step": 1300
    },
    {
      "epoch": 0.09207475105715456,
      "grad_norm": 3.881181240081787,
      "learning_rate": 0.00019386623016414315,
      "loss": 0.6819,
      "step": 1350
    },
    {
      "epoch": 0.09548492702223435,
      "grad_norm": 3.6668457984924316,
      "learning_rate": 0.0001936388850998045,
      "loss": 0.7258,
      "step": 1400
    },
    {
      "epoch": 0.09889510298731415,
      "grad_norm": 5.234337329864502,
      "learning_rate": 0.00019341154003546583,
      "loss": 0.6362,
      "step": 1450
    },
    {
      "epoch": 0.10230527895239394,
      "grad_norm": 3.630810260772705,
      "learning_rate": 0.0001931841949711272,
      "loss": 0.7535,
      "step": 1500
    },
    {
      "epoch": 0.10571545491747374,
      "grad_norm": 4.049806594848633,
      "learning_rate": 0.00019295684990678852,
      "loss": 0.6796,
      "step": 1550
    },
    {
      "epoch": 0.10912563088255355,
      "grad_norm": 2.731646776199341,
      "learning_rate": 0.00019272950484244987,
      "loss": 0.7183,
      "step": 1600
    },
    {
      "epoch": 0.11253580684763334,
      "grad_norm": 4.633825778961182,
      "learning_rate": 0.00019250215977811123,
      "loss": 0.7046,
      "step": 1650
    },
    {
      "epoch": 0.11594598281271314,
      "grad_norm": 2.6853275299072266,
      "learning_rate": 0.00019227481471377258,
      "loss": 0.7178,
      "step": 1700
    },
    {
      "epoch": 0.11935615877779293,
      "grad_norm": 5.189800262451172,
      "learning_rate": 0.0001920474696494339,
      "loss": 0.71,
      "step": 1750
    },
    {
      "epoch": 0.12276633474287273,
      "grad_norm": 3.789625644683838,
      "learning_rate": 0.00019182012458509527,
      "loss": 0.7398,
      "step": 1800
    },
    {
      "epoch": 0.12617651070795252,
      "grad_norm": 3.2206430435180664,
      "learning_rate": 0.0001915927795207566,
      "loss": 0.6261,
      "step": 1850
    },
    {
      "epoch": 0.12958668667303233,
      "grad_norm": 4.1258745193481445,
      "learning_rate": 0.00019136543445641798,
      "loss": 0.7131,
      "step": 1900
    },
    {
      "epoch": 0.13299686263811214,
      "grad_norm": 6.2360076904296875,
      "learning_rate": 0.0001911380893920793,
      "loss": 0.6835,
      "step": 1950
    },
    {
      "epoch": 0.13640703860319192,
      "grad_norm": 3.683366537094116,
      "learning_rate": 0.00019091074432774066,
      "loss": 0.7524,
      "step": 2000
    },
    {
      "epoch": 0.13981721456827173,
      "grad_norm": 4.814393520355225,
      "learning_rate": 0.000190683399263402,
      "loss": 0.6211,
      "step": 2050
    },
    {
      "epoch": 0.1432273905333515,
      "grad_norm": 4.151459693908691,
      "learning_rate": 0.00019045605419906335,
      "loss": 0.6273,
      "step": 2100
    },
    {
      "epoch": 0.14663756649843132,
      "grad_norm": 5.81264066696167,
      "learning_rate": 0.0001902287091347247,
      "loss": 0.7293,
      "step": 2150
    },
    {
      "epoch": 0.15004774246351113,
      "grad_norm": 3.5810301303863525,
      "learning_rate": 0.00019000136407038603,
      "loss": 0.6773,
      "step": 2200
    },
    {
      "epoch": 0.1534579184285909,
      "grad_norm": 4.983246803283691,
      "learning_rate": 0.0001897740190060474,
      "loss": 0.6238,
      "step": 2250
    },
    {
      "epoch": 0.15686809439367072,
      "grad_norm": 7.631110668182373,
      "learning_rate": 0.00018954667394170872,
      "loss": 0.6872,
      "step": 2300
    },
    {
      "epoch": 0.1602782703587505,
      "grad_norm": 7.837310314178467,
      "learning_rate": 0.00018931932887737007,
      "loss": 0.6566,
      "step": 2350
    },
    {
      "epoch": 0.1636884463238303,
      "grad_norm": 4.944979667663574,
      "learning_rate": 0.00018909198381303143,
      "loss": 0.6796,
      "step": 2400
    },
    {
      "epoch": 0.16709862228891012,
      "grad_norm": 5.057923316955566,
      "learning_rate": 0.00018886463874869279,
      "loss": 0.6717,
      "step": 2450
    },
    {
      "epoch": 0.1705087982539899,
      "grad_norm": 4.042693614959717,
      "learning_rate": 0.00018863729368435411,
      "loss": 0.7259,
      "step": 2500
    },
    {
      "epoch": 0.1739189742190697,
      "grad_norm": 4.212404251098633,
      "learning_rate": 0.00018840994862001547,
      "loss": 0.6274,
      "step": 2550
    },
    {
      "epoch": 0.1773291501841495,
      "grad_norm": 9.324618339538574,
      "learning_rate": 0.0001881826035556768,
      "loss": 0.6065,
      "step": 2600
    },
    {
      "epoch": 0.1807393261492293,
      "grad_norm": 3.434833526611328,
      "learning_rate": 0.00018795525849133818,
      "loss": 0.617,
      "step": 2650
    },
    {
      "epoch": 0.1841495021143091,
      "grad_norm": 5.356233596801758,
      "learning_rate": 0.0001877279134269995,
      "loss": 0.6386,
      "step": 2700
    },
    {
      "epoch": 0.1875596780793889,
      "grad_norm": 3.7747814655303955,
      "learning_rate": 0.00018750056836266087,
      "loss": 0.6099,
      "step": 2750
    },
    {
      "epoch": 0.1909698540444687,
      "grad_norm": 8.316987991333008,
      "learning_rate": 0.0001872732232983222,
      "loss": 0.6383,
      "step": 2800
    },
    {
      "epoch": 0.19438003000954848,
      "grad_norm": 2.7060201168060303,
      "learning_rate": 0.00018704587823398355,
      "loss": 0.7242,
      "step": 2850
    },
    {
      "epoch": 0.1977902059746283,
      "grad_norm": 4.933772087097168,
      "learning_rate": 0.0001868185331696449,
      "loss": 0.6392,
      "step": 2900
    },
    {
      "epoch": 0.2012003819397081,
      "grad_norm": 6.200394630432129,
      "learning_rate": 0.00018659118810530624,
      "loss": 0.6324,
      "step": 2950
    },
    {
      "epoch": 0.20461055790478788,
      "grad_norm": 2.947542190551758,
      "learning_rate": 0.0001863638430409676,
      "loss": 0.6469,
      "step": 3000
    },
    {
      "epoch": 0.2080207338698677,
      "grad_norm": 6.412920951843262,
      "learning_rate": 0.00018613649797662892,
      "loss": 0.6531,
      "step": 3050
    },
    {
      "epoch": 0.21143090983494747,
      "grad_norm": 3.877812147140503,
      "learning_rate": 0.00018590915291229028,
      "loss": 0.6554,
      "step": 3100
    },
    {
      "epoch": 0.21484108580002728,
      "grad_norm": 5.62726354598999,
      "learning_rate": 0.00018568180784795163,
      "loss": 0.6049,
      "step": 3150
    },
    {
      "epoch": 0.2182512617651071,
      "grad_norm": 6.729239463806152,
      "learning_rate": 0.000185454462783613,
      "loss": 0.6647,
      "step": 3200
    },
    {
      "epoch": 0.22166143773018687,
      "grad_norm": 4.400230884552002,
      "learning_rate": 0.00018522711771927432,
      "loss": 0.6438,
      "step": 3250
    },
    {
      "epoch": 0.22507161369526668,
      "grad_norm": 3.4407312870025635,
      "learning_rate": 0.00018499977265493567,
      "loss": 0.6271,
      "step": 3300
    },
    {
      "epoch": 0.22848178966034646,
      "grad_norm": 6.966365814208984,
      "learning_rate": 0.000184772427590597,
      "loss": 0.6679,
      "step": 3350
    },
    {
      "epoch": 0.23189196562542627,
      "grad_norm": 4.579921245574951,
      "learning_rate": 0.00018454508252625839,
      "loss": 0.6308,
      "step": 3400
    },
    {
      "epoch": 0.23530214159050608,
      "grad_norm": 2.9670605659484863,
      "learning_rate": 0.00018431773746191971,
      "loss": 0.6168,
      "step": 3450
    },
    {
      "epoch": 0.23871231755558586,
      "grad_norm": 8.35934829711914,
      "learning_rate": 0.00018409039239758107,
      "loss": 0.6039,
      "step": 3500
    },
    {
      "epoch": 0.24212249352066567,
      "grad_norm": 3.2142443656921387,
      "learning_rate": 0.0001838630473332424,
      "loss": 0.6797,
      "step": 3550
    },
    {
      "epoch": 0.24553266948574545,
      "grad_norm": 6.829575061798096,
      "learning_rate": 0.00018363570226890375,
      "loss": 0.6452,
      "step": 3600
    },
    {
      "epoch": 0.24894284545082526,
      "grad_norm": 6.064201354980469,
      "learning_rate": 0.0001834083572045651,
      "loss": 0.6379,
      "step": 3650
    },
    {
      "epoch": 0.25235302141590504,
      "grad_norm": 5.631986618041992,
      "learning_rate": 0.00018318101214022644,
      "loss": 0.5951,
      "step": 3700
    },
    {
      "epoch": 0.25576319738098485,
      "grad_norm": 4.068284511566162,
      "learning_rate": 0.0001829536670758878,
      "loss": 0.5087,
      "step": 3750
    },
    {
      "epoch": 0.25917337334606466,
      "grad_norm": 2.6116394996643066,
      "learning_rate": 0.00018272632201154912,
      "loss": 0.6831,
      "step": 3800
    },
    {
      "epoch": 0.26258354931114447,
      "grad_norm": 3.116823196411133,
      "learning_rate": 0.00018249897694721048,
      "loss": 0.6416,
      "step": 3850
    },
    {
      "epoch": 0.2659937252762243,
      "grad_norm": 4.241364002227783,
      "learning_rate": 0.00018227163188287184,
      "loss": 0.7046,
      "step": 3900
    },
    {
      "epoch": 0.26940390124130403,
      "grad_norm": 5.16800594329834,
      "learning_rate": 0.0001820442868185332,
      "loss": 0.6162,
      "step": 3950
    },
    {
      "epoch": 0.27281407720638384,
      "grad_norm": 3.7442033290863037,
      "learning_rate": 0.00018181694175419452,
      "loss": 0.6132,
      "step": 4000
    },
    {
      "epoch": 0.27622425317146365,
      "grad_norm": 4.777984619140625,
      "learning_rate": 0.00018158959668985588,
      "loss": 0.6625,
      "step": 4050
    },
    {
      "epoch": 0.27963442913654346,
      "grad_norm": 6.9991774559021,
      "learning_rate": 0.0001813622516255172,
      "loss": 0.5744,
      "step": 4100
    },
    {
      "epoch": 0.28304460510162327,
      "grad_norm": 4.016238212585449,
      "learning_rate": 0.00018113490656117856,
      "loss": 0.6306,
      "step": 4150
    },
    {
      "epoch": 0.286454781066703,
      "grad_norm": 1.4679911136627197,
      "learning_rate": 0.00018090756149683992,
      "loss": 0.5938,
      "step": 4200
    },
    {
      "epoch": 0.28986495703178283,
      "grad_norm": 4.716958045959473,
      "learning_rate": 0.00018068021643250125,
      "loss": 0.5921,
      "step": 4250
    },
    {
      "epoch": 0.29327513299686264,
      "grad_norm": 4.382480621337891,
      "learning_rate": 0.0001804528713681626,
      "loss": 0.6395,
      "step": 4300
    },
    {
      "epoch": 0.29668530896194245,
      "grad_norm": 5.0738749504089355,
      "learning_rate": 0.00018022552630382393,
      "loss": 0.5472,
      "step": 4350
    },
    {
      "epoch": 0.30009548492702226,
      "grad_norm": 2.0653560161590576,
      "learning_rate": 0.0001799981812394853,
      "loss": 0.6806,
      "step": 4400
    },
    {
      "epoch": 0.303505660892102,
      "grad_norm": 2.821573495864868,
      "learning_rate": 0.00017977083617514664,
      "loss": 0.557,
      "step": 4450
    },
    {
      "epoch": 0.3069158368571818,
      "grad_norm": 4.685956001281738,
      "learning_rate": 0.000179543491110808,
      "loss": 0.5846,
      "step": 4500
    },
    {
      "epoch": 0.31032601282226163,
      "grad_norm": 4.512872695922852,
      "learning_rate": 0.00017931614604646933,
      "loss": 0.6258,
      "step": 4550
    },
    {
      "epoch": 0.31373618878734144,
      "grad_norm": 5.8909831047058105,
      "learning_rate": 0.00017908880098213068,
      "loss": 0.564,
      "step": 4600
    },
    {
      "epoch": 0.31714636475242125,
      "grad_norm": 4.2698845863342285,
      "learning_rate": 0.00017886145591779204,
      "loss": 0.6721,
      "step": 4650
    },
    {
      "epoch": 0.320556540717501,
      "grad_norm": 6.974838733673096,
      "learning_rate": 0.0001786341108534534,
      "loss": 0.5578,
      "step": 4700
    },
    {
      "epoch": 0.3239667166825808,
      "grad_norm": 2.9359333515167236,
      "learning_rate": 0.00017840676578911472,
      "loss": 0.5834,
      "step": 4750
    },
    {
      "epoch": 0.3273768926476606,
      "grad_norm": 3.4700112342834473,
      "learning_rate": 0.00017817942072477608,
      "loss": 0.4911,
      "step": 4800
    },
    {
      "epoch": 0.33078706861274043,
      "grad_norm": 2.7019097805023193,
      "learning_rate": 0.0001779520756604374,
      "loss": 0.6259,
      "step": 4850
    },
    {
      "epoch": 0.33419724457782024,
      "grad_norm": 2.5879292488098145,
      "learning_rate": 0.00017772473059609876,
      "loss": 0.5993,
      "step": 4900
    },
    {
      "epoch": 0.3376074205429,
      "grad_norm": 4.1245293617248535,
      "learning_rate": 0.00017749738553176012,
      "loss": 0.6342,
      "step": 4950
    },
    {
      "epoch": 0.3410175965079798,
      "grad_norm": 4.16182279586792,
      "learning_rate": 0.00017727004046742145,
      "loss": 0.5669,
      "step": 5000
    },
    {
      "epoch": 0.3444277724730596,
      "grad_norm": 8.300979614257812,
      "learning_rate": 0.0001770426954030828,
      "loss": 0.4635,
      "step": 5050
    },
    {
      "epoch": 0.3478379484381394,
      "grad_norm": 2.022709369659424,
      "learning_rate": 0.00017681535033874413,
      "loss": 0.6406,
      "step": 5100
    },
    {
      "epoch": 0.35124812440321923,
      "grad_norm": 2.8537421226501465,
      "learning_rate": 0.00017658800527440552,
      "loss": 0.5675,
      "step": 5150
    },
    {
      "epoch": 0.354658300368299,
      "grad_norm": 3.2413370609283447,
      "learning_rate": 0.00017636066021006685,
      "loss": 0.5898,
      "step": 5200
    },
    {
      "epoch": 0.3580684763333788,
      "grad_norm": 6.9281816482543945,
      "learning_rate": 0.0001761333151457282,
      "loss": 0.6097,
      "step": 5250
    },
    {
      "epoch": 0.3614786522984586,
      "grad_norm": 2.6264398097991943,
      "learning_rate": 0.00017590597008138953,
      "loss": 0.6163,
      "step": 5300
    },
    {
      "epoch": 0.3648888282635384,
      "grad_norm": 3.346616268157959,
      "learning_rate": 0.00017567862501705089,
      "loss": 0.5978,
      "step": 5350
    },
    {
      "epoch": 0.3682990042286182,
      "grad_norm": 4.676401615142822,
      "learning_rate": 0.00017545127995271224,
      "loss": 0.6303,
      "step": 5400
    },
    {
      "epoch": 0.371709180193698,
      "grad_norm": 4.9054036140441895,
      "learning_rate": 0.0001752239348883736,
      "loss": 0.5327,
      "step": 5450
    },
    {
      "epoch": 0.3751193561587778,
      "grad_norm": 6.702650547027588,
      "learning_rate": 0.00017499658982403493,
      "loss": 0.6811,
      "step": 5500
    },
    {
      "epoch": 0.3785295321238576,
      "grad_norm": 5.693346977233887,
      "learning_rate": 0.00017476924475969628,
      "loss": 0.6901,
      "step": 5550
    },
    {
      "epoch": 0.3819397080889374,
      "grad_norm": 3.1401803493499756,
      "learning_rate": 0.0001745418996953576,
      "loss": 0.5641,
      "step": 5600
    },
    {
      "epoch": 0.3853498840540172,
      "grad_norm": 6.725994110107422,
      "learning_rate": 0.00017431455463101897,
      "loss": 0.5657,
      "step": 5650
    },
    {
      "epoch": 0.38876006001909696,
      "grad_norm": 4.647238731384277,
      "learning_rate": 0.00017408720956668032,
      "loss": 0.6404,
      "step": 5700
    },
    {
      "epoch": 0.3921702359841768,
      "grad_norm": 6.93887186050415,
      "learning_rate": 0.00017385986450234165,
      "loss": 0.6397,
      "step": 5750
    },
    {
      "epoch": 0.3955804119492566,
      "grad_norm": 4.132195949554443,
      "learning_rate": 0.000173632519438003,
      "loss": 0.5397,
      "step": 5800
    },
    {
      "epoch": 0.3989905879143364,
      "grad_norm": 4.308523178100586,
      "learning_rate": 0.00017340517437366434,
      "loss": 0.5549,
      "step": 5850
    },
    {
      "epoch": 0.4024007638794162,
      "grad_norm": 4.303738117218018,
      "learning_rate": 0.00017317782930932572,
      "loss": 0.5797,
      "step": 5900
    },
    {
      "epoch": 0.40581093984449595,
      "grad_norm": 3.0409154891967773,
      "learning_rate": 0.00017295048424498705,
      "loss": 0.7227,
      "step": 5950
    },
    {
      "epoch": 0.40922111580957576,
      "grad_norm": 2.696404457092285,
      "learning_rate": 0.0001727231391806484,
      "loss": 0.4563,
      "step": 6000
    },
    {
      "epoch": 0.4126312917746556,
      "grad_norm": 7.137048721313477,
      "learning_rate": 0.00017249579411630973,
      "loss": 0.489,
      "step": 6050
    },
    {
      "epoch": 0.4160414677397354,
      "grad_norm": 8.420626640319824,
      "learning_rate": 0.0001722684490519711,
      "loss": 0.6329,
      "step": 6100
    },
    {
      "epoch": 0.4194516437048152,
      "grad_norm": 2.6202187538146973,
      "learning_rate": 0.00017204110398763244,
      "loss": 0.6822,
      "step": 6150
    },
    {
      "epoch": 0.42286181966989494,
      "grad_norm": 9.227581024169922,
      "learning_rate": 0.0001718137589232938,
      "loss": 0.5228,
      "step": 6200
    },
    {
      "epoch": 0.42627199563497475,
      "grad_norm": 1.6557345390319824,
      "learning_rate": 0.00017158641385895513,
      "loss": 0.5858,
      "step": 6250
    },
    {
      "epoch": 0.42968217160005456,
      "grad_norm": 3.262540578842163,
      "learning_rate": 0.00017135906879461648,
      "loss": 0.4723,
      "step": 6300
    },
    {
      "epoch": 0.4330923475651344,
      "grad_norm": 6.707336902618408,
      "learning_rate": 0.00017113172373027781,
      "loss": 0.6107,
      "step": 6350
    },
    {
      "epoch": 0.4365025235302142,
      "grad_norm": 9.387163162231445,
      "learning_rate": 0.00017090437866593917,
      "loss": 0.5901,
      "step": 6400
    },
    {
      "epoch": 0.43991269949529394,
      "grad_norm": 2.892360210418701,
      "learning_rate": 0.00017067703360160053,
      "loss": 0.5828,
      "step": 6450
    },
    {
      "epoch": 0.44332287546037374,
      "grad_norm": 3.085547685623169,
      "learning_rate": 0.00017044968853726185,
      "loss": 0.4915,
      "step": 6500
    },
    {
      "epoch": 0.44673305142545355,
      "grad_norm": 3.894258975982666,
      "learning_rate": 0.0001702223434729232,
      "loss": 0.5506,
      "step": 6550
    },
    {
      "epoch": 0.45014322739053336,
      "grad_norm": 1.3758810758590698,
      "learning_rate": 0.00016999499840858454,
      "loss": 0.5532,
      "step": 6600
    },
    {
      "epoch": 0.45355340335561317,
      "grad_norm": 5.8421173095703125,
      "learning_rate": 0.00016976765334424592,
      "loss": 0.6145,
      "step": 6650
    },
    {
      "epoch": 0.4569635793206929,
      "grad_norm": 2.1508877277374268,
      "learning_rate": 0.00016954030827990725,
      "loss": 0.6049,
      "step": 6700
    },
    {
      "epoch": 0.46037375528577273,
      "grad_norm": 1.6894422769546509,
      "learning_rate": 0.0001693129632155686,
      "loss": 0.5186,
      "step": 6750
    },
    {
      "epoch": 0.46378393125085254,
      "grad_norm": 3.097674608230591,
      "learning_rate": 0.00016908561815122994,
      "loss": 0.5236,
      "step": 6800
    },
    {
      "epoch": 0.46719410721593235,
      "grad_norm": 5.993048191070557,
      "learning_rate": 0.0001688582730868913,
      "loss": 0.53,
      "step": 6850
    },
    {
      "epoch": 0.47060428318101216,
      "grad_norm": 4.520483016967773,
      "learning_rate": 0.00016863092802255265,
      "loss": 0.5311,
      "step": 6900
    },
    {
      "epoch": 0.4740144591460919,
      "grad_norm": 6.196635723114014,
      "learning_rate": 0.000168403582958214,
      "loss": 0.5267,
      "step": 6950
    },
    {
      "epoch": 0.4774246351111717,
      "grad_norm": 8.493480682373047,
      "learning_rate": 0.00016817623789387533,
      "loss": 0.5832,
      "step": 7000
    },
    {
      "epoch": 0.48083481107625153,
      "grad_norm": 3.5250840187072754,
      "learning_rate": 0.0001679488928295367,
      "loss": 0.5679,
      "step": 7050
    },
    {
      "epoch": 0.48424498704133134,
      "grad_norm": 6.400791645050049,
      "learning_rate": 0.00016772154776519802,
      "loss": 0.6053,
      "step": 7100
    },
    {
      "epoch": 0.48765516300641115,
      "grad_norm": 5.314202785491943,
      "learning_rate": 0.00016749420270085937,
      "loss": 0.5184,
      "step": 7150
    },
    {
      "epoch": 0.4910653389714909,
      "grad_norm": 5.697413921356201,
      "learning_rate": 0.00016726685763652073,
      "loss": 0.5078,
      "step": 7200
    },
    {
      "epoch": 0.4944755149365707,
      "grad_norm": 2.152207612991333,
      "learning_rate": 0.00016703951257218206,
      "loss": 0.4559,
      "step": 7250
    },
    {
      "epoch": 0.4978856909016505,
      "grad_norm": 1.1217572689056396,
      "learning_rate": 0.0001668121675078434,
      "loss": 0.5594,
      "step": 7300
    },
    {
      "epoch": 0.5012958668667303,
      "grad_norm": 9.571451187133789,
      "learning_rate": 0.00016658482244350474,
      "loss": 0.6012,
      "step": 7350
    },
    {
      "epoch": 0.5047060428318101,
      "grad_norm": 1.8719607591629028,
      "learning_rate": 0.00016635747737916612,
      "loss": 0.5044,
      "step": 7400
    },
    {
      "epoch": 0.50811621879689,
      "grad_norm": 4.435731887817383,
      "learning_rate": 0.00016613013231482745,
      "loss": 0.6225,
      "step": 7450
    },
    {
      "epoch": 0.5115263947619697,
      "grad_norm": 2.373957872390747,
      "learning_rate": 0.0001659027872504888,
      "loss": 0.5815,
      "step": 7500
    },
    {
      "epoch": 0.5149365707270496,
      "grad_norm": 7.570865631103516,
      "learning_rate": 0.00016567544218615014,
      "loss": 0.618,
      "step": 7550
    },
    {
      "epoch": 0.5183467466921293,
      "grad_norm": 1.6939805746078491,
      "learning_rate": 0.0001654480971218115,
      "loss": 0.5299,
      "step": 7600
    },
    {
      "epoch": 0.5217569226572091,
      "grad_norm": 3.359837532043457,
      "learning_rate": 0.00016522075205747285,
      "loss": 0.6406,
      "step": 7650
    },
    {
      "epoch": 0.5251670986222889,
      "grad_norm": 6.281620502471924,
      "learning_rate": 0.00016499340699313418,
      "loss": 0.5339,
      "step": 7700
    },
    {
      "epoch": 0.5285772745873687,
      "grad_norm": 4.283625602722168,
      "learning_rate": 0.00016476606192879553,
      "loss": 0.4876,
      "step": 7750
    },
    {
      "epoch": 0.5319874505524486,
      "grad_norm": 1.9548975229263306,
      "learning_rate": 0.00016453871686445686,
      "loss": 0.5188,
      "step": 7800
    },
    {
      "epoch": 0.5353976265175283,
      "grad_norm": 5.0062079429626465,
      "learning_rate": 0.00016431137180011822,
      "loss": 0.6067,
      "step": 7850
    },
    {
      "epoch": 0.5388078024826081,
      "grad_norm": 3.0621142387390137,
      "learning_rate": 0.00016408402673577958,
      "loss": 0.5412,
      "step": 7900
    },
    {
      "epoch": 0.5422179784476879,
      "grad_norm": 1.0248709917068481,
      "learning_rate": 0.00016385668167144093,
      "loss": 0.4478,
      "step": 7950
    },
    {
      "epoch": 0.5456281544127677,
      "grad_norm": 5.944315433502197,
      "learning_rate": 0.00016362933660710226,
      "loss": 0.56,
      "step": 8000
    },
    {
      "epoch": 0.5490383303778475,
      "grad_norm": 4.7076311111450195,
      "learning_rate": 0.00016340199154276362,
      "loss": 0.442,
      "step": 8050
    },
    {
      "epoch": 0.5524485063429273,
      "grad_norm": 2.183709144592285,
      "learning_rate": 0.00016317464647842494,
      "loss": 0.5077,
      "step": 8100
    },
    {
      "epoch": 0.5558586823080071,
      "grad_norm": 4.422790050506592,
      "learning_rate": 0.00016294730141408633,
      "loss": 0.5276,
      "step": 8150
    },
    {
      "epoch": 0.5592688582730869,
      "grad_norm": 3.8972325325012207,
      "learning_rate": 0.00016271995634974766,
      "loss": 0.5599,
      "step": 8200
    },
    {
      "epoch": 0.5626790342381667,
      "grad_norm": 1.8844224214553833,
      "learning_rate": 0.000162492611285409,
      "loss": 0.574,
      "step": 8250
    },
    {
      "epoch": 0.5660892102032465,
      "grad_norm": 5.736959457397461,
      "learning_rate": 0.00016226526622107034,
      "loss": 0.5541,
      "step": 8300
    },
    {
      "epoch": 0.5694993861683263,
      "grad_norm": 1.2332019805908203,
      "learning_rate": 0.0001620379211567317,
      "loss": 0.6469,
      "step": 8350
    },
    {
      "epoch": 0.572909562133406,
      "grad_norm": 4.569895267486572,
      "learning_rate": 0.00016181057609239305,
      "loss": 0.5188,
      "step": 8400
    },
    {
      "epoch": 0.5763197380984859,
      "grad_norm": 2.686091184616089,
      "learning_rate": 0.00016158323102805438,
      "loss": 0.5934,
      "step": 8450
    },
    {
      "epoch": 0.5797299140635657,
      "grad_norm": 3.0451066493988037,
      "learning_rate": 0.00016135588596371574,
      "loss": 0.4826,
      "step": 8500
    },
    {
      "epoch": 0.5831400900286455,
      "grad_norm": 6.963432312011719,
      "learning_rate": 0.00016112854089937707,
      "loss": 0.5399,
      "step": 8550
    },
    {
      "epoch": 0.5865502659937253,
      "grad_norm": 3.1497836112976074,
      "learning_rate": 0.00016090119583503842,
      "loss": 0.6517,
      "step": 8600
    },
    {
      "epoch": 0.589960441958805,
      "grad_norm": 1.3391757011413574,
      "learning_rate": 0.00016067385077069978,
      "loss": 0.5299,
      "step": 8650
    },
    {
      "epoch": 0.5933706179238849,
      "grad_norm": 3.4283430576324463,
      "learning_rate": 0.00016044650570636113,
      "loss": 0.5935,
      "step": 8700
    },
    {
      "epoch": 0.5967807938889647,
      "grad_norm": 4.608673095703125,
      "learning_rate": 0.00016021916064202246,
      "loss": 0.5255,
      "step": 8750
    },
    {
      "epoch": 0.6001909698540445,
      "grad_norm": 6.921573162078857,
      "learning_rate": 0.00015999181557768382,
      "loss": 0.5284,
      "step": 8800
    },
    {
      "epoch": 0.6036011458191243,
      "grad_norm": 2.657743215560913,
      "learning_rate": 0.00015976447051334515,
      "loss": 0.4705,
      "step": 8850
    },
    {
      "epoch": 0.607011321784204,
      "grad_norm": 4.977388381958008,
      "learning_rate": 0.00015953712544900653,
      "loss": 0.5665,
      "step": 8900
    },
    {
      "epoch": 0.6104214977492839,
      "grad_norm": 6.316170692443848,
      "learning_rate": 0.00015930978038466786,
      "loss": 0.5412,
      "step": 8950
    },
    {
      "epoch": 0.6138316737143636,
      "grad_norm": 2.70988392829895,
      "learning_rate": 0.00015908243532032921,
      "loss": 0.5036,
      "step": 9000
    },
    {
      "epoch": 0.6172418496794435,
      "grad_norm": 6.782095432281494,
      "learning_rate": 0.00015885509025599054,
      "loss": 0.5581,
      "step": 9050
    },
    {
      "epoch": 0.6206520256445233,
      "grad_norm": 2.898824691772461,
      "learning_rate": 0.0001586277451916519,
      "loss": 0.5784,
      "step": 9100
    },
    {
      "epoch": 0.624062201609603,
      "grad_norm": 3.41677188873291,
      "learning_rate": 0.00015840040012731326,
      "loss": 0.5738,
      "step": 9150
    },
    {
      "epoch": 0.6274723775746829,
      "grad_norm": 8.091300964355469,
      "learning_rate": 0.00015817305506297458,
      "loss": 0.5708,
      "step": 9200
    },
    {
      "epoch": 0.6308825535397626,
      "grad_norm": 1.4837385416030884,
      "learning_rate": 0.00015794570999863594,
      "loss": 0.488,
      "step": 9250
    },
    {
      "epoch": 0.6342927295048425,
      "grad_norm": 1.3815741539001465,
      "learning_rate": 0.00015771836493429727,
      "loss": 0.549,
      "step": 9300
    },
    {
      "epoch": 0.6377029054699223,
      "grad_norm": 4.3227057456970215,
      "learning_rate": 0.00015749101986995862,
      "loss": 0.5048,
      "step": 9350
    },
    {
      "epoch": 0.641113081435002,
      "grad_norm": 3.8489980697631836,
      "learning_rate": 0.00015726367480561998,
      "loss": 0.5588,
      "step": 9400
    },
    {
      "epoch": 0.6445232574000819,
      "grad_norm": 7.162261962890625,
      "learning_rate": 0.00015703632974128134,
      "loss": 0.5237,
      "step": 9450
    },
    {
      "epoch": 0.6479334333651616,
      "grad_norm": 2.872513771057129,
      "learning_rate": 0.00015680898467694267,
      "loss": 0.5376,
      "step": 9500
    },
    {
      "epoch": 0.6513436093302415,
      "grad_norm": 5.652125358581543,
      "learning_rate": 0.00015658163961260402,
      "loss": 0.6202,
      "step": 9550
    },
    {
      "epoch": 0.6547537852953212,
      "grad_norm": 4.448047161102295,
      "learning_rate": 0.00015635429454826535,
      "loss": 0.5201,
      "step": 9600
    },
    {
      "epoch": 0.658163961260401,
      "grad_norm": 5.281510353088379,
      "learning_rate": 0.00015612694948392673,
      "loss": 0.4918,
      "step": 9650
    },
    {
      "epoch": 0.6615741372254809,
      "grad_norm": 6.330154895782471,
      "learning_rate": 0.00015589960441958806,
      "loss": 0.5564,
      "step": 9700
    },
    {
      "epoch": 0.6649843131905606,
      "grad_norm": 3.0484023094177246,
      "learning_rate": 0.00015567225935524942,
      "loss": 0.5096,
      "step": 9750
    },
    {
      "epoch": 0.6683944891556405,
      "grad_norm": 2.5767197608947754,
      "learning_rate": 0.00015544491429091075,
      "loss": 0.5072,
      "step": 9800
    },
    {
      "epoch": 0.6718046651207202,
      "grad_norm": 7.161823749542236,
      "learning_rate": 0.0001552175692265721,
      "loss": 0.5541,
      "step": 9850
    },
    {
      "epoch": 0.6752148410858,
      "grad_norm": 3.4963274002075195,
      "learning_rate": 0.00015499022416223346,
      "loss": 0.492,
      "step": 9900
    },
    {
      "epoch": 0.6786250170508799,
      "grad_norm": 9.90247631072998,
      "learning_rate": 0.0001547628790978948,
      "loss": 0.5493,
      "step": 9950
    },
    {
      "epoch": 0.6820351930159596,
      "grad_norm": 3.732794761657715,
      "learning_rate": 0.00015453553403355614,
      "loss": 0.5105,
      "step": 10000
    },
    {
      "epoch": 0.6854453689810395,
      "grad_norm": 3.563112735748291,
      "learning_rate": 0.00015430818896921747,
      "loss": 0.4928,
      "step": 10050
    },
    {
      "epoch": 0.6888555449461192,
      "grad_norm": 6.230532646179199,
      "learning_rate": 0.00015408084390487883,
      "loss": 0.5391,
      "step": 10100
    },
    {
      "epoch": 0.692265720911199,
      "grad_norm": 4.717284679412842,
      "learning_rate": 0.00015385349884054018,
      "loss": 0.6076,
      "step": 10150
    },
    {
      "epoch": 0.6956758968762788,
      "grad_norm": 1.5988492965698242,
      "learning_rate": 0.00015362615377620154,
      "loss": 0.579,
      "step": 10200
    },
    {
      "epoch": 0.6990860728413586,
      "grad_norm": 10.493505477905273,
      "learning_rate": 0.00015339880871186287,
      "loss": 0.4947,
      "step": 10250
    },
    {
      "epoch": 0.7024962488064385,
      "grad_norm": 4.584412097930908,
      "learning_rate": 0.00015317146364752422,
      "loss": 0.54,
      "step": 10300
    },
    {
      "epoch": 0.7059064247715182,
      "grad_norm": 3.211653709411621,
      "learning_rate": 0.00015294411858318555,
      "loss": 0.5456,
      "step": 10350
    },
    {
      "epoch": 0.709316600736598,
      "grad_norm": 3.413649559020996,
      "learning_rate": 0.00015271677351884694,
      "loss": 0.5011,
      "step": 10400
    },
    {
      "epoch": 0.7127267767016778,
      "grad_norm": 3.0088231563568115,
      "learning_rate": 0.00015248942845450826,
      "loss": 0.5831,
      "step": 10450
    },
    {
      "epoch": 0.7161369526667576,
      "grad_norm": 2.479114055633545,
      "learning_rate": 0.00015226208339016962,
      "loss": 0.5953,
      "step": 10500
    },
    {
      "epoch": 0.7195471286318375,
      "grad_norm": 3.7112340927124023,
      "learning_rate": 0.00015203473832583095,
      "loss": 0.4711,
      "step": 10550
    },
    {
      "epoch": 0.7229573045969172,
      "grad_norm": 3.3521804809570312,
      "learning_rate": 0.0001518073932614923,
      "loss": 0.4914,
      "step": 10600
    },
    {
      "epoch": 0.726367480561997,
      "grad_norm": 6.322518825531006,
      "learning_rate": 0.00015158004819715366,
      "loss": 0.5151,
      "step": 10650
    },
    {
      "epoch": 0.7297776565270768,
      "grad_norm": 5.79115104675293,
      "learning_rate": 0.000151352703132815,
      "loss": 0.5446,
      "step": 10700
    },
    {
      "epoch": 0.7331878324921566,
      "grad_norm": 9.393169403076172,
      "learning_rate": 0.00015112535806847635,
      "loss": 0.5594,
      "step": 10750
    },
    {
      "epoch": 0.7365980084572364,
      "grad_norm": 4.9113593101501465,
      "learning_rate": 0.00015089801300413767,
      "loss": 0.5369,
      "step": 10800
    },
    {
      "epoch": 0.7400081844223162,
      "grad_norm": 2.066735029220581,
      "learning_rate": 0.00015067066793979903,
      "loss": 0.5099,
      "step": 10850
    },
    {
      "epoch": 0.743418360387396,
      "grad_norm": 7.562488079071045,
      "learning_rate": 0.00015044332287546039,
      "loss": 0.5175,
      "step": 10900
    },
    {
      "epoch": 0.7468285363524758,
      "grad_norm": 3.956282377243042,
      "learning_rate": 0.00015021597781112174,
      "loss": 0.5777,
      "step": 10950
    },
    {
      "epoch": 0.7502387123175556,
      "grad_norm": 5.102017402648926,
      "learning_rate": 0.00014998863274678307,
      "loss": 0.6558,
      "step": 11000
    },
    {
      "epoch": 0.7536488882826354,
      "grad_norm": 1.9244688749313354,
      "learning_rate": 0.00014976128768244443,
      "loss": 0.4711,
      "step": 11050
    },
    {
      "epoch": 0.7570590642477152,
      "grad_norm": 4.8804097175598145,
      "learning_rate": 0.00014953394261810576,
      "loss": 0.5238,
      "step": 11100
    },
    {
      "epoch": 0.7604692402127949,
      "grad_norm": 3.57965350151062,
      "learning_rate": 0.0001493065975537671,
      "loss": 0.5541,
      "step": 11150
    },
    {
      "epoch": 0.7638794161778748,
      "grad_norm": 6.542465686798096,
      "learning_rate": 0.00014907925248942847,
      "loss": 0.5345,
      "step": 11200
    },
    {
      "epoch": 0.7672895921429546,
      "grad_norm": 3.4771485328674316,
      "learning_rate": 0.0001488519074250898,
      "loss": 0.5305,
      "step": 11250
    },
    {
      "epoch": 0.7706997681080344,
      "grad_norm": 3.2458817958831787,
      "learning_rate": 0.00014862456236075115,
      "loss": 0.5165,
      "step": 11300
    },
    {
      "epoch": 0.7741099440731142,
      "grad_norm": 2.2685558795928955,
      "learning_rate": 0.00014839721729641248,
      "loss": 0.4696,
      "step": 11350
    },
    {
      "epoch": 0.7775201200381939,
      "grad_norm": 8.451598167419434,
      "learning_rate": 0.00014816987223207386,
      "loss": 0.5592,
      "step": 11400
    },
    {
      "epoch": 0.7809302960032738,
      "grad_norm": 7.53580379486084,
      "learning_rate": 0.0001479425271677352,
      "loss": 0.496,
      "step": 11450
    },
    {
      "epoch": 0.7843404719683535,
      "grad_norm": 7.037713050842285,
      "learning_rate": 0.00014771518210339655,
      "loss": 0.5856,
      "step": 11500
    },
    {
      "epoch": 0.7877506479334334,
      "grad_norm": 3.354215621948242,
      "learning_rate": 0.00014748783703905788,
      "loss": 0.5214,
      "step": 11550
    },
    {
      "epoch": 0.7911608238985132,
      "grad_norm": 6.031603813171387,
      "learning_rate": 0.00014726049197471923,
      "loss": 0.5656,
      "step": 11600
    },
    {
      "epoch": 0.7945709998635929,
      "grad_norm": 2.9881956577301025,
      "learning_rate": 0.0001470331469103806,
      "loss": 0.4745,
      "step": 11650
    },
    {
      "epoch": 0.7979811758286728,
      "grad_norm": 3.825817108154297,
      "learning_rate": 0.00014680580184604194,
      "loss": 0.5411,
      "step": 11700
    },
    {
      "epoch": 0.8013913517937525,
      "grad_norm": 4.622636318206787,
      "learning_rate": 0.00014657845678170327,
      "loss": 0.5117,
      "step": 11750
    },
    {
      "epoch": 0.8048015277588324,
      "grad_norm": 1.8534528017044067,
      "learning_rate": 0.00014635111171736463,
      "loss": 0.5018,
      "step": 11800
    },
    {
      "epoch": 0.8082117037239122,
      "grad_norm": 1.8155694007873535,
      "learning_rate": 0.00014612376665302596,
      "loss": 0.4165,
      "step": 11850
    },
    {
      "epoch": 0.8116218796889919,
      "grad_norm": 2.4805243015289307,
      "learning_rate": 0.00014589642158868731,
      "loss": 0.4896,
      "step": 11900
    },
    {
      "epoch": 0.8150320556540718,
      "grad_norm": 3.8450238704681396,
      "learning_rate": 0.00014566907652434867,
      "loss": 0.5417,
      "step": 11950
    },
    {
      "epoch": 0.8184422316191515,
      "grad_norm": 5.4871673583984375,
      "learning_rate": 0.00014544173146001,
      "loss": 0.571,
      "step": 12000
    },
    {
      "epoch": 0.8218524075842314,
      "grad_norm": 4.1293625831604,
      "learning_rate": 0.00014521438639567135,
      "loss": 0.4732,
      "step": 12050
    },
    {
      "epoch": 0.8252625835493111,
      "grad_norm": 1.4948612451553345,
      "learning_rate": 0.00014498704133133268,
      "loss": 0.5644,
      "step": 12100
    },
    {
      "epoch": 0.8286727595143909,
      "grad_norm": 2.7908928394317627,
      "learning_rate": 0.00014475969626699407,
      "loss": 0.4763,
      "step": 12150
    },
    {
      "epoch": 0.8320829354794708,
      "grad_norm": 4.504905700683594,
      "learning_rate": 0.0001445323512026554,
      "loss": 0.4813,
      "step": 12200
    },
    {
      "epoch": 0.8354931114445505,
      "grad_norm": 6.458579063415527,
      "learning_rate": 0.00014430500613831675,
      "loss": 0.4842,
      "step": 12250
    },
    {
      "epoch": 0.8389032874096304,
      "grad_norm": 3.4847285747528076,
      "learning_rate": 0.00014407766107397808,
      "loss": 0.6585,
      "step": 12300
    },
    {
      "epoch": 0.8423134633747101,
      "grad_norm": 1.967301845550537,
      "learning_rate": 0.00014385031600963944,
      "loss": 0.4965,
      "step": 12350
    },
    {
      "epoch": 0.8457236393397899,
      "grad_norm": 3.2897496223449707,
      "learning_rate": 0.0001436229709453008,
      "loss": 0.506,
      "step": 12400
    },
    {
      "epoch": 0.8491338153048698,
      "grad_norm": 1.6241425275802612,
      "learning_rate": 0.00014339562588096215,
      "loss": 0.4738,
      "step": 12450
    },
    {
      "epoch": 0.8525439912699495,
      "grad_norm": 1.4080233573913574,
      "learning_rate": 0.00014316828081662348,
      "loss": 0.4185,
      "step": 12500
    },
    {
      "epoch": 0.8559541672350294,
      "grad_norm": 1.3446654081344604,
      "learning_rate": 0.00014294093575228483,
      "loss": 0.4199,
      "step": 12550
    },
    {
      "epoch": 0.8593643432001091,
      "grad_norm": 7.100772380828857,
      "learning_rate": 0.00014271359068794616,
      "loss": 0.573,
      "step": 12600
    },
    {
      "epoch": 0.8627745191651889,
      "grad_norm": 1.595511794090271,
      "learning_rate": 0.00014248624562360752,
      "loss": 0.471,
      "step": 12650
    },
    {
      "epoch": 0.8661846951302687,
      "grad_norm": 6.136267185211182,
      "learning_rate": 0.00014225890055926887,
      "loss": 0.5941,
      "step": 12700
    },
    {
      "epoch": 0.8695948710953485,
      "grad_norm": 4.194272518157959,
      "learning_rate": 0.0001420315554949302,
      "loss": 0.4732,
      "step": 12750
    },
    {
      "epoch": 0.8730050470604284,
      "grad_norm": 3.598219394683838,
      "learning_rate": 0.00014180421043059156,
      "loss": 0.5949,
      "step": 12800
    },
    {
      "epoch": 0.8764152230255081,
      "grad_norm": 2.889256238937378,
      "learning_rate": 0.00014157686536625289,
      "loss": 0.5763,
      "step": 12850
    },
    {
      "epoch": 0.8798253989905879,
      "grad_norm": 6.05254602432251,
      "learning_rate": 0.00014134952030191427,
      "loss": 0.4428,
      "step": 12900
    },
    {
      "epoch": 0.8832355749556677,
      "grad_norm": 2.8352386951446533,
      "learning_rate": 0.0001411221752375756,
      "loss": 0.5287,
      "step": 12950
    },
    {
      "epoch": 0.8866457509207475,
      "grad_norm": 6.208887577056885,
      "learning_rate": 0.00014089483017323695,
      "loss": 0.4819,
      "step": 13000
    },
    {
      "epoch": 0.8900559268858274,
      "grad_norm": 4.236801624298096,
      "learning_rate": 0.00014066748510889828,
      "loss": 0.4475,
      "step": 13050
    },
    {
      "epoch": 0.8934661028509071,
      "grad_norm": 4.340880870819092,
      "learning_rate": 0.00014044014004455964,
      "loss": 0.4552,
      "step": 13100
    },
    {
      "epoch": 0.8968762788159869,
      "grad_norm": 1.1489304304122925,
      "learning_rate": 0.000140212794980221,
      "loss": 0.5364,
      "step": 13150
    },
    {
      "epoch": 0.9002864547810667,
      "grad_norm": 3.3975205421447754,
      "learning_rate": 0.00013998544991588235,
      "loss": 0.4886,
      "step": 13200
    },
    {
      "epoch": 0.9036966307461465,
      "grad_norm": 3.8280599117279053,
      "learning_rate": 0.00013975810485154368,
      "loss": 0.4998,
      "step": 13250
    },
    {
      "epoch": 0.9071068067112263,
      "grad_norm": 4.130757808685303,
      "learning_rate": 0.00013953075978720504,
      "loss": 0.5086,
      "step": 13300
    },
    {
      "epoch": 0.9105169826763061,
      "grad_norm": 6.123864650726318,
      "learning_rate": 0.00013930341472286636,
      "loss": 0.5188,
      "step": 13350
    },
    {
      "epoch": 0.9139271586413859,
      "grad_norm": 2.714372396469116,
      "learning_rate": 0.00013907606965852772,
      "loss": 0.4832,
      "step": 13400
    },
    {
      "epoch": 0.9173373346064657,
      "grad_norm": 4.956056118011475,
      "learning_rate": 0.00013884872459418908,
      "loss": 0.5499,
      "step": 13450
    },
    {
      "epoch": 0.9207475105715455,
      "grad_norm": 5.8628034591674805,
      "learning_rate": 0.0001386213795298504,
      "loss": 0.4388,
      "step": 13500
    },
    {
      "epoch": 0.9241576865366253,
      "grad_norm": 3.811239242553711,
      "learning_rate": 0.00013839403446551176,
      "loss": 0.6102,
      "step": 13550
    },
    {
      "epoch": 0.9275678625017051,
      "grad_norm": 3.0883169174194336,
      "learning_rate": 0.0001381666894011731,
      "loss": 0.4952,
      "step": 13600
    },
    {
      "epoch": 0.9309780384667848,
      "grad_norm": 7.053252220153809,
      "learning_rate": 0.00013793934433683447,
      "loss": 0.5219,
      "step": 13650
    },
    {
      "epoch": 0.9343882144318647,
      "grad_norm": 3.4296107292175293,
      "learning_rate": 0.0001377119992724958,
      "loss": 0.5457,
      "step": 13700
    },
    {
      "epoch": 0.9377983903969445,
      "grad_norm": 1.939153790473938,
      "learning_rate": 0.00013748465420815716,
      "loss": 0.4683,
      "step": 13750
    },
    {
      "epoch": 0.9412085663620243,
      "grad_norm": 5.600878715515137,
      "learning_rate": 0.00013725730914381849,
      "loss": 0.3848,
      "step": 13800
    },
    {
      "epoch": 0.9446187423271041,
      "grad_norm": 0.530109167098999,
      "learning_rate": 0.00013702996407947984,
      "loss": 0.4993,
      "step": 13850
    },
    {
      "epoch": 0.9480289182921838,
      "grad_norm": 1.522309422492981,
      "learning_rate": 0.0001368026190151412,
      "loss": 0.5908,
      "step": 13900
    },
    {
      "epoch": 0.9514390942572637,
      "grad_norm": 2.523498058319092,
      "learning_rate": 0.00013657527395080255,
      "loss": 0.5064,
      "step": 13950
    },
    {
      "epoch": 0.9548492702223434,
      "grad_norm": 4.693229675292969,
      "learning_rate": 0.00013634792888646388,
      "loss": 0.5105,
      "step": 14000
    },
    {
      "epoch": 0.9582594461874233,
      "grad_norm": 4.601716041564941,
      "learning_rate": 0.00013612058382212524,
      "loss": 0.5015,
      "step": 14050
    },
    {
      "epoch": 0.9616696221525031,
      "grad_norm": 3.3283212184906006,
      "learning_rate": 0.00013589323875778657,
      "loss": 0.5062,
      "step": 14100
    },
    {
      "epoch": 0.9650797981175828,
      "grad_norm": 8.203328132629395,
      "learning_rate": 0.00013566589369344792,
      "loss": 0.536,
      "step": 14150
    },
    {
      "epoch": 0.9684899740826627,
      "grad_norm": 8.823223114013672,
      "learning_rate": 0.00013543854862910928,
      "loss": 0.4616,
      "step": 14200
    },
    {
      "epoch": 0.9719001500477424,
      "grad_norm": 1.8540843725204468,
      "learning_rate": 0.0001352112035647706,
      "loss": 0.5343,
      "step": 14250
    },
    {
      "epoch": 0.9753103260128223,
      "grad_norm": 3.9538235664367676,
      "learning_rate": 0.00013498385850043196,
      "loss": 0.5839,
      "step": 14300
    },
    {
      "epoch": 0.9787205019779021,
      "grad_norm": 6.376593112945557,
      "learning_rate": 0.0001347565134360933,
      "loss": 0.5146,
      "step": 14350
    },
    {
      "epoch": 0.9821306779429818,
      "grad_norm": 4.5149922370910645,
      "learning_rate": 0.00013452916837175467,
      "loss": 0.4567,
      "step": 14400
    },
    {
      "epoch": 0.9855408539080617,
      "grad_norm": 6.716584205627441,
      "learning_rate": 0.000134301823307416,
      "loss": 0.45,
      "step": 14450
    },
    {
      "epoch": 0.9889510298731414,
      "grad_norm": 5.664109230041504,
      "learning_rate": 0.00013407447824307736,
      "loss": 0.5194,
      "step": 14500
    },
    {
      "epoch": 0.9923612058382213,
      "grad_norm": 7.993327617645264,
      "learning_rate": 0.0001338471331787387,
      "loss": 0.4511,
      "step": 14550
    },
    {
      "epoch": 0.995771381803301,
      "grad_norm": 6.084619045257568,
      "learning_rate": 0.00013361978811440004,
      "loss": 0.584,
      "step": 14600
    },
    {
      "epoch": 0.9991815577683808,
      "grad_norm": 2.418008327484131,
      "learning_rate": 0.0001333924430500614,
      "loss": 0.4792,
      "step": 14650
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.8235248983349958,
      "eval_loss": 0.4742952585220337,
      "eval_runtime": 114.2809,
      "eval_samples_per_second": 114.044,
      "eval_steps_per_second": 7.132,
      "step": 14662
    },
    {
      "epoch": 1.0025917337334607,
      "grad_norm": 5.223816394805908,
      "learning_rate": 0.00013316509798572273,
      "loss": 0.5302,
      "step": 14700
    },
    {
      "epoch": 1.0060019096985404,
      "grad_norm": 3.156221389770508,
      "learning_rate": 0.00013293775292138408,
      "loss": 0.4843,
      "step": 14750
    },
    {
      "epoch": 1.0094120856636202,
      "grad_norm": 3.3942739963531494,
      "learning_rate": 0.00013271040785704541,
      "loss": 0.3838,
      "step": 14800
    },
    {
      "epoch": 1.0128222616287001,
      "grad_norm": 2.3713552951812744,
      "learning_rate": 0.00013248306279270677,
      "loss": 0.4968,
      "step": 14850
    },
    {
      "epoch": 1.01623243759378,
      "grad_norm": 6.933642864227295,
      "learning_rate": 0.00013225571772836813,
      "loss": 0.4326,
      "step": 14900
    },
    {
      "epoch": 1.0196426135588597,
      "grad_norm": 4.889519214630127,
      "learning_rate": 0.00013202837266402948,
      "loss": 0.4809,
      "step": 14950
    },
    {
      "epoch": 1.0230527895239394,
      "grad_norm": 3.6131832599639893,
      "learning_rate": 0.0001318010275996908,
      "loss": 0.4473,
      "step": 15000
    },
    {
      "epoch": 1.0264629654890192,
      "grad_norm": 5.398589134216309,
      "learning_rate": 0.00013157368253535217,
      "loss": 0.4633,
      "step": 15050
    },
    {
      "epoch": 1.0298731414540991,
      "grad_norm": 4.236259460449219,
      "learning_rate": 0.0001313463374710135,
      "loss": 0.5445,
      "step": 15100
    },
    {
      "epoch": 1.033283317419179,
      "grad_norm": 2.9759695529937744,
      "learning_rate": 0.00013111899240667488,
      "loss": 0.4866,
      "step": 15150
    },
    {
      "epoch": 1.0366934933842586,
      "grad_norm": 3.3126063346862793,
      "learning_rate": 0.0001308916473423362,
      "loss": 0.5587,
      "step": 15200
    },
    {
      "epoch": 1.0401036693493384,
      "grad_norm": 2.6777279376983643,
      "learning_rate": 0.00013066430227799756,
      "loss": 0.5381,
      "step": 15250
    },
    {
      "epoch": 1.0435138453144182,
      "grad_norm": 6.604776859283447,
      "learning_rate": 0.0001304369572136589,
      "loss": 0.454,
      "step": 15300
    },
    {
      "epoch": 1.0469240212794981,
      "grad_norm": 6.666868686676025,
      "learning_rate": 0.00013020961214932025,
      "loss": 0.4616,
      "step": 15350
    },
    {
      "epoch": 1.0503341972445779,
      "grad_norm": 6.713696479797363,
      "learning_rate": 0.0001299822670849816,
      "loss": 0.448,
      "step": 15400
    },
    {
      "epoch": 1.0537443732096576,
      "grad_norm": 3.135852575302124,
      "learning_rate": 0.00012975492202064293,
      "loss": 0.5659,
      "step": 15450
    },
    {
      "epoch": 1.0571545491747374,
      "grad_norm": 4.828426837921143,
      "learning_rate": 0.0001295275769563043,
      "loss": 0.4575,
      "step": 15500
    },
    {
      "epoch": 1.0605647251398171,
      "grad_norm": 2.327592372894287,
      "learning_rate": 0.00012930023189196562,
      "loss": 0.4305,
      "step": 15550
    },
    {
      "epoch": 1.063974901104897,
      "grad_norm": 5.232986927032471,
      "learning_rate": 0.00012907288682762697,
      "loss": 0.551,
      "step": 15600
    },
    {
      "epoch": 1.0673850770699769,
      "grad_norm": 2.4631664752960205,
      "learning_rate": 0.00012884554176328833,
      "loss": 0.3797,
      "step": 15650
    },
    {
      "epoch": 1.0707952530350566,
      "grad_norm": 3.313521146774292,
      "learning_rate": 0.00012861819669894968,
      "loss": 0.6132,
      "step": 15700
    },
    {
      "epoch": 1.0742054290001364,
      "grad_norm": 5.08698034286499,
      "learning_rate": 0.000128390851634611,
      "loss": 0.4299,
      "step": 15750
    },
    {
      "epoch": 1.0776156049652161,
      "grad_norm": 2.5615603923797607,
      "learning_rate": 0.00012816350657027237,
      "loss": 0.4189,
      "step": 15800
    },
    {
      "epoch": 1.0810257809302959,
      "grad_norm": 5.51711368560791,
      "learning_rate": 0.0001279361615059337,
      "loss": 0.5111,
      "step": 15850
    },
    {
      "epoch": 1.0844359568953759,
      "grad_norm": 3.774665355682373,
      "learning_rate": 0.00012770881644159508,
      "loss": 0.5965,
      "step": 15900
    },
    {
      "epoch": 1.0878461328604556,
      "grad_norm": 4.696549415588379,
      "learning_rate": 0.0001274814713772564,
      "loss": 0.4753,
      "step": 15950
    },
    {
      "epoch": 1.0912563088255354,
      "grad_norm": 8.857613563537598,
      "learning_rate": 0.00012725412631291777,
      "loss": 0.5139,
      "step": 16000
    },
    {
      "epoch": 1.0946664847906151,
      "grad_norm": 5.512331485748291,
      "learning_rate": 0.0001270267812485791,
      "loss": 0.3978,
      "step": 16050
    },
    {
      "epoch": 1.0980766607556949,
      "grad_norm": 8.650433540344238,
      "learning_rate": 0.00012679943618424045,
      "loss": 0.485,
      "step": 16100
    },
    {
      "epoch": 1.1014868367207749,
      "grad_norm": 5.426390647888184,
      "learning_rate": 0.0001265720911199018,
      "loss": 0.4987,
      "step": 16150
    },
    {
      "epoch": 1.1048970126858546,
      "grad_norm": 1.0689849853515625,
      "learning_rate": 0.00012634474605556313,
      "loss": 0.3944,
      "step": 16200
    },
    {
      "epoch": 1.1083071886509344,
      "grad_norm": 7.096371173858643,
      "learning_rate": 0.0001261174009912245,
      "loss": 0.5116,
      "step": 16250
    },
    {
      "epoch": 1.1117173646160141,
      "grad_norm": 3.253730058670044,
      "learning_rate": 0.00012589005592688582,
      "loss": 0.4909,
      "step": 16300
    },
    {
      "epoch": 1.1151275405810939,
      "grad_norm": 6.20630407333374,
      "learning_rate": 0.00012566271086254718,
      "loss": 0.5142,
      "step": 16350
    },
    {
      "epoch": 1.1185377165461738,
      "grad_norm": 2.2272801399230957,
      "learning_rate": 0.00012543536579820853,
      "loss": 0.4506,
      "step": 16400
    },
    {
      "epoch": 1.1219478925112536,
      "grad_norm": 4.661386013031006,
      "learning_rate": 0.0001252080207338699,
      "loss": 0.5626,
      "step": 16450
    },
    {
      "epoch": 1.1253580684763334,
      "grad_norm": 4.986692905426025,
      "learning_rate": 0.00012498067566953122,
      "loss": 0.4886,
      "step": 16500
    },
    {
      "epoch": 1.128768244441413,
      "grad_norm": 3.737989664077759,
      "learning_rate": 0.00012475333060519257,
      "loss": 0.4434,
      "step": 16550
    },
    {
      "epoch": 1.1321784204064929,
      "grad_norm": 5.999840259552002,
      "learning_rate": 0.0001245259855408539,
      "loss": 0.4443,
      "step": 16600
    },
    {
      "epoch": 1.1355885963715728,
      "grad_norm": 4.041965007781982,
      "learning_rate": 0.00012429864047651528,
      "loss": 0.411,
      "step": 16650
    },
    {
      "epoch": 1.1389987723366526,
      "grad_norm": 5.7514190673828125,
      "learning_rate": 0.0001240712954121766,
      "loss": 0.4378,
      "step": 16700
    },
    {
      "epoch": 1.1424089483017323,
      "grad_norm": 2.212310791015625,
      "learning_rate": 0.00012384395034783797,
      "loss": 0.5349,
      "step": 16750
    },
    {
      "epoch": 1.145819124266812,
      "grad_norm": 3.1841959953308105,
      "learning_rate": 0.0001236166052834993,
      "loss": 0.4518,
      "step": 16800
    },
    {
      "epoch": 1.1492293002318918,
      "grad_norm": 1.198381781578064,
      "learning_rate": 0.00012338926021916065,
      "loss": 0.4548,
      "step": 16850
    },
    {
      "epoch": 1.1526394761969718,
      "grad_norm": 2.540982484817505,
      "learning_rate": 0.000123161915154822,
      "loss": 0.4503,
      "step": 16900
    },
    {
      "epoch": 1.1560496521620516,
      "grad_norm": 8.75660228729248,
      "learning_rate": 0.00012293457009048334,
      "loss": 0.4412,
      "step": 16950
    },
    {
      "epoch": 1.1594598281271313,
      "grad_norm": 4.4036478996276855,
      "learning_rate": 0.0001227072250261447,
      "loss": 0.3755,
      "step": 17000
    },
    {
      "epoch": 1.162870004092211,
      "grad_norm": 4.106014251708984,
      "learning_rate": 0.00012247987996180602,
      "loss": 0.5094,
      "step": 17050
    },
    {
      "epoch": 1.1662801800572908,
      "grad_norm": 2.7669036388397217,
      "learning_rate": 0.00012225253489746738,
      "loss": 0.3621,
      "step": 17100
    },
    {
      "epoch": 1.1696903560223708,
      "grad_norm": 4.04099702835083,
      "learning_rate": 0.00012202518983312873,
      "loss": 0.4778,
      "step": 17150
    },
    {
      "epoch": 1.1731005319874506,
      "grad_norm": 11.776293754577637,
      "learning_rate": 0.00012179784476879008,
      "loss": 0.3826,
      "step": 17200
    },
    {
      "epoch": 1.1765107079525303,
      "grad_norm": 5.635366916656494,
      "learning_rate": 0.00012157049970445142,
      "loss": 0.4815,
      "step": 17250
    },
    {
      "epoch": 1.17992088391761,
      "grad_norm": 5.663832187652588,
      "learning_rate": 0.00012134315464011276,
      "loss": 0.4303,
      "step": 17300
    },
    {
      "epoch": 1.1833310598826898,
      "grad_norm": 5.9831695556640625,
      "learning_rate": 0.0001211158095757741,
      "loss": 0.4596,
      "step": 17350
    },
    {
      "epoch": 1.1867412358477698,
      "grad_norm": 4.745998382568359,
      "learning_rate": 0.00012088846451143547,
      "loss": 0.3783,
      "step": 17400
    },
    {
      "epoch": 1.1901514118128496,
      "grad_norm": 5.543654441833496,
      "learning_rate": 0.00012066111944709681,
      "loss": 0.5043,
      "step": 17450
    },
    {
      "epoch": 1.1935615877779293,
      "grad_norm": 4.303028583526611,
      "learning_rate": 0.00012043377438275816,
      "loss": 0.4596,
      "step": 17500
    },
    {
      "epoch": 1.196971763743009,
      "grad_norm": 14.583107948303223,
      "learning_rate": 0.0001202064293184195,
      "loss": 0.4862,
      "step": 17550
    },
    {
      "epoch": 1.2003819397080888,
      "grad_norm": 4.688690662384033,
      "learning_rate": 0.00011997908425408084,
      "loss": 0.4379,
      "step": 17600
    },
    {
      "epoch": 1.2037921156731688,
      "grad_norm": 4.457726955413818,
      "learning_rate": 0.00011975173918974221,
      "loss": 0.4966,
      "step": 17650
    },
    {
      "epoch": 1.2072022916382485,
      "grad_norm": 3.082583427429199,
      "learning_rate": 0.00011952439412540355,
      "loss": 0.4589,
      "step": 17700
    },
    {
      "epoch": 1.2106124676033283,
      "grad_norm": 8.324239730834961,
      "learning_rate": 0.0001192970490610649,
      "loss": 0.4303,
      "step": 17750
    },
    {
      "epoch": 1.214022643568408,
      "grad_norm": 11.62354564666748,
      "learning_rate": 0.00011906970399672624,
      "loss": 0.4817,
      "step": 17800
    },
    {
      "epoch": 1.2174328195334878,
      "grad_norm": 7.887782096862793,
      "learning_rate": 0.00011884235893238758,
      "loss": 0.4466,
      "step": 17850
    },
    {
      "epoch": 1.2208429954985678,
      "grad_norm": 4.6556925773620605,
      "learning_rate": 0.00011861501386804894,
      "loss": 0.435,
      "step": 17900
    },
    {
      "epoch": 1.2242531714636475,
      "grad_norm": 3.5575335025787354,
      "learning_rate": 0.00011838766880371028,
      "loss": 0.4615,
      "step": 17950
    },
    {
      "epoch": 1.2276633474287273,
      "grad_norm": 4.686376571655273,
      "learning_rate": 0.00011816032373937162,
      "loss": 0.4598,
      "step": 18000
    },
    {
      "epoch": 1.231073523393807,
      "grad_norm": 8.125433921813965,
      "learning_rate": 0.00011793297867503296,
      "loss": 0.5011,
      "step": 18050
    },
    {
      "epoch": 1.2344836993588868,
      "grad_norm": 4.7625508308410645,
      "learning_rate": 0.0001177056336106943,
      "loss": 0.4146,
      "step": 18100
    },
    {
      "epoch": 1.2378938753239668,
      "grad_norm": 7.995903968811035,
      "learning_rate": 0.00011747828854635568,
      "loss": 0.4519,
      "step": 18150
    },
    {
      "epoch": 1.2413040512890465,
      "grad_norm": 7.8434648513793945,
      "learning_rate": 0.00011725094348201702,
      "loss": 0.4826,
      "step": 18200
    },
    {
      "epoch": 1.2447142272541263,
      "grad_norm": 5.288227558135986,
      "learning_rate": 0.00011702359841767836,
      "loss": 0.4959,
      "step": 18250
    },
    {
      "epoch": 1.248124403219206,
      "grad_norm": 3.023589849472046,
      "learning_rate": 0.0001167962533533397,
      "loss": 0.4567,
      "step": 18300
    },
    {
      "epoch": 1.2515345791842858,
      "grad_norm": 6.924599647521973,
      "learning_rate": 0.00011656890828900104,
      "loss": 0.4941,
      "step": 18350
    },
    {
      "epoch": 1.2549447551493658,
      "grad_norm": 2.299490213394165,
      "learning_rate": 0.00011634156322466241,
      "loss": 0.4244,
      "step": 18400
    },
    {
      "epoch": 1.2583549311144455,
      "grad_norm": 3.8517987728118896,
      "learning_rate": 0.00011611421816032376,
      "loss": 0.3712,
      "step": 18450
    },
    {
      "epoch": 1.2617651070795253,
      "grad_norm": 5.890198230743408,
      "learning_rate": 0.0001158868730959851,
      "loss": 0.5022,
      "step": 18500
    },
    {
      "epoch": 1.265175283044605,
      "grad_norm": 5.729844093322754,
      "learning_rate": 0.00011565952803164644,
      "loss": 0.5038,
      "step": 18550
    },
    {
      "epoch": 1.2685854590096848,
      "grad_norm": 6.529751777648926,
      "learning_rate": 0.00011543218296730778,
      "loss": 0.4491,
      "step": 18600
    },
    {
      "epoch": 1.2719956349747648,
      "grad_norm": 8.750285148620605,
      "learning_rate": 0.00011520483790296914,
      "loss": 0.4806,
      "step": 18650
    },
    {
      "epoch": 1.2754058109398445,
      "grad_norm": 5.137984752655029,
      "learning_rate": 0.00011497749283863048,
      "loss": 0.4941,
      "step": 18700
    },
    {
      "epoch": 1.2788159869049243,
      "grad_norm": 1.6059117317199707,
      "learning_rate": 0.00011475014777429182,
      "loss": 0.4721,
      "step": 18750
    },
    {
      "epoch": 1.282226162870004,
      "grad_norm": 4.693592071533203,
      "learning_rate": 0.00011452280270995317,
      "loss": 0.5225,
      "step": 18800
    },
    {
      "epoch": 1.2856363388350838,
      "grad_norm": 2.59479022026062,
      "learning_rate": 0.00011429545764561451,
      "loss": 0.4183,
      "step": 18850
    },
    {
      "epoch": 1.2890465148001637,
      "grad_norm": 2.746497392654419,
      "learning_rate": 0.00011406811258127588,
      "loss": 0.4623,
      "step": 18900
    },
    {
      "epoch": 1.2924566907652435,
      "grad_norm": 5.984365463256836,
      "learning_rate": 0.00011384076751693722,
      "loss": 0.4006,
      "step": 18950
    },
    {
      "epoch": 1.2958668667303233,
      "grad_norm": 6.966224193572998,
      "learning_rate": 0.00011361342245259856,
      "loss": 0.42,
      "step": 19000
    },
    {
      "epoch": 1.299277042695403,
      "grad_norm": 5.358968257904053,
      "learning_rate": 0.0001133860773882599,
      "loss": 0.5245,
      "step": 19050
    },
    {
      "epoch": 1.3026872186604828,
      "grad_norm": 5.89578914642334,
      "learning_rate": 0.00011315873232392125,
      "loss": 0.4184,
      "step": 19100
    },
    {
      "epoch": 1.3060973946255627,
      "grad_norm": 7.8493971824646,
      "learning_rate": 0.00011293138725958262,
      "loss": 0.5008,
      "step": 19150
    },
    {
      "epoch": 1.3095075705906425,
      "grad_norm": 3.0928006172180176,
      "learning_rate": 0.00011270404219524396,
      "loss": 0.469,
      "step": 19200
    },
    {
      "epoch": 1.3129177465557222,
      "grad_norm": 10.027716636657715,
      "learning_rate": 0.0001124766971309053,
      "loss": 0.4011,
      "step": 19250
    },
    {
      "epoch": 1.316327922520802,
      "grad_norm": 9.237288475036621,
      "learning_rate": 0.00011224935206656664,
      "loss": 0.4409,
      "step": 19300
    },
    {
      "epoch": 1.3197380984858817,
      "grad_norm": 1.8202382326126099,
      "learning_rate": 0.00011202200700222799,
      "loss": 0.4883,
      "step": 19350
    },
    {
      "epoch": 1.3231482744509617,
      "grad_norm": 2.346574544906616,
      "learning_rate": 0.00011179466193788934,
      "loss": 0.4464,
      "step": 19400
    },
    {
      "epoch": 1.3265584504160415,
      "grad_norm": 8.020098686218262,
      "learning_rate": 0.00011156731687355068,
      "loss": 0.4688,
      "step": 19450
    },
    {
      "epoch": 1.3299686263811212,
      "grad_norm": 4.120344638824463,
      "learning_rate": 0.00011133997180921203,
      "loss": 0.5004,
      "step": 19500
    },
    {
      "epoch": 1.333378802346201,
      "grad_norm": 2.1966392993927,
      "learning_rate": 0.00011111262674487337,
      "loss": 0.3888,
      "step": 19550
    },
    {
      "epoch": 1.3367889783112807,
      "grad_norm": 2.4243645668029785,
      "learning_rate": 0.00011088528168053471,
      "loss": 0.4848,
      "step": 19600
    },
    {
      "epoch": 1.3401991542763607,
      "grad_norm": 7.533267974853516,
      "learning_rate": 0.00011065793661619608,
      "loss": 0.4953,
      "step": 19650
    },
    {
      "epoch": 1.3436093302414405,
      "grad_norm": 1.005686640739441,
      "learning_rate": 0.00011043059155185742,
      "loss": 0.4715,
      "step": 19700
    },
    {
      "epoch": 1.3470195062065202,
      "grad_norm": 3.1290037631988525,
      "learning_rate": 0.00011020324648751877,
      "loss": 0.4617,
      "step": 19750
    },
    {
      "epoch": 1.3504296821716,
      "grad_norm": 0.8896505236625671,
      "learning_rate": 0.00010997590142318011,
      "loss": 0.4124,
      "step": 19800
    },
    {
      "epoch": 1.3538398581366797,
      "grad_norm": 4.6585564613342285,
      "learning_rate": 0.00010974855635884145,
      "loss": 0.4355,
      "step": 19850
    },
    {
      "epoch": 1.3572500341017597,
      "grad_norm": 6.566209316253662,
      "learning_rate": 0.0001095212112945028,
      "loss": 0.4773,
      "step": 19900
    },
    {
      "epoch": 1.3606602100668395,
      "grad_norm": 1.5952028036117554,
      "learning_rate": 0.00010929386623016415,
      "loss": 0.4277,
      "step": 19950
    },
    {
      "epoch": 1.3640703860319192,
      "grad_norm": 2.7996091842651367,
      "learning_rate": 0.00010906652116582549,
      "loss": 0.3816,
      "step": 20000
    },
    {
      "epoch": 1.367480561996999,
      "grad_norm": 4.631862640380859,
      "learning_rate": 0.00010883917610148683,
      "loss": 0.4118,
      "step": 20050
    },
    {
      "epoch": 1.3708907379620787,
      "grad_norm": 4.004732131958008,
      "learning_rate": 0.00010861183103714818,
      "loss": 0.4977,
      "step": 20100
    },
    {
      "epoch": 1.3743009139271587,
      "grad_norm": 5.101497650146484,
      "learning_rate": 0.00010838448597280954,
      "loss": 0.5052,
      "step": 20150
    },
    {
      "epoch": 1.3777110898922384,
      "grad_norm": 0.6449785828590393,
      "learning_rate": 0.00010815714090847089,
      "loss": 0.4367,
      "step": 20200
    },
    {
      "epoch": 1.3811212658573182,
      "grad_norm": 1.6587659120559692,
      "learning_rate": 0.00010792979584413223,
      "loss": 0.4268,
      "step": 20250
    },
    {
      "epoch": 1.384531441822398,
      "grad_norm": 1.7802894115447998,
      "learning_rate": 0.00010770245077979357,
      "loss": 0.481,
      "step": 20300
    },
    {
      "epoch": 1.3879416177874777,
      "grad_norm": 7.956582069396973,
      "learning_rate": 0.00010747510571545491,
      "loss": 0.4635,
      "step": 20350
    },
    {
      "epoch": 1.3913517937525577,
      "grad_norm": 6.523194789886475,
      "learning_rate": 0.00010724776065111628,
      "loss": 0.38,
      "step": 20400
    },
    {
      "epoch": 1.3947619697176374,
      "grad_norm": 5.17500638961792,
      "learning_rate": 0.00010702041558677763,
      "loss": 0.4448,
      "step": 20450
    },
    {
      "epoch": 1.3981721456827172,
      "grad_norm": 5.404571056365967,
      "learning_rate": 0.00010679307052243897,
      "loss": 0.4526,
      "step": 20500
    },
    {
      "epoch": 1.401582321647797,
      "grad_norm": 3.402463436126709,
      "learning_rate": 0.00010656572545810031,
      "loss": 0.392,
      "step": 20550
    },
    {
      "epoch": 1.4049924976128767,
      "grad_norm": 4.355383396148682,
      "learning_rate": 0.00010633838039376165,
      "loss": 0.4148,
      "step": 20600
    },
    {
      "epoch": 1.4084026735779567,
      "grad_norm": 4.097492694854736,
      "learning_rate": 0.00010611103532942301,
      "loss": 0.4226,
      "step": 20650
    },
    {
      "epoch": 1.4118128495430364,
      "grad_norm": 8.546916961669922,
      "learning_rate": 0.00010588369026508435,
      "loss": 0.4776,
      "step": 20700
    },
    {
      "epoch": 1.4152230255081162,
      "grad_norm": 6.538926124572754,
      "learning_rate": 0.0001056563452007457,
      "loss": 0.4195,
      "step": 20750
    },
    {
      "epoch": 1.418633201473196,
      "grad_norm": 6.404435157775879,
      "learning_rate": 0.00010542900013640704,
      "loss": 0.4486,
      "step": 20800
    },
    {
      "epoch": 1.4220433774382757,
      "grad_norm": 3.1452906131744385,
      "learning_rate": 0.00010520165507206838,
      "loss": 0.481,
      "step": 20850
    },
    {
      "epoch": 1.4254535534033557,
      "grad_norm": 1.3274197578430176,
      "learning_rate": 0.00010497431000772975,
      "loss": 0.4778,
      "step": 20900
    },
    {
      "epoch": 1.4288637293684354,
      "grad_norm": 2.8196802139282227,
      "learning_rate": 0.00010474696494339109,
      "loss": 0.3609,
      "step": 20950
    },
    {
      "epoch": 1.4322739053335152,
      "grad_norm": 8.60696029663086,
      "learning_rate": 0.00010451961987905243,
      "loss": 0.442,
      "step": 21000
    },
    {
      "epoch": 1.435684081298595,
      "grad_norm": 3.7980544567108154,
      "learning_rate": 0.00010429227481471377,
      "loss": 0.4842,
      "step": 21050
    },
    {
      "epoch": 1.4390942572636747,
      "grad_norm": 8.949333190917969,
      "learning_rate": 0.00010406492975037512,
      "loss": 0.4676,
      "step": 21100
    },
    {
      "epoch": 1.4425044332287547,
      "grad_norm": 5.803653717041016,
      "learning_rate": 0.00010383758468603649,
      "loss": 0.4467,
      "step": 21150
    },
    {
      "epoch": 1.4459146091938344,
      "grad_norm": 5.195112228393555,
      "learning_rate": 0.00010361023962169783,
      "loss": 0.4331,
      "step": 21200
    },
    {
      "epoch": 1.4493247851589142,
      "grad_norm": 4.156452178955078,
      "learning_rate": 0.00010338289455735917,
      "loss": 0.4627,
      "step": 21250
    },
    {
      "epoch": 1.452734961123994,
      "grad_norm": 5.315558433532715,
      "learning_rate": 0.00010315554949302051,
      "loss": 0.4023,
      "step": 21300
    },
    {
      "epoch": 1.4561451370890737,
      "grad_norm": 4.328029155731201,
      "learning_rate": 0.00010292820442868186,
      "loss": 0.4609,
      "step": 21350
    },
    {
      "epoch": 1.4595553130541536,
      "grad_norm": 5.328758716583252,
      "learning_rate": 0.00010270085936434321,
      "loss": 0.491,
      "step": 21400
    },
    {
      "epoch": 1.4629654890192334,
      "grad_norm": 8.02908992767334,
      "learning_rate": 0.00010247351430000455,
      "loss": 0.4703,
      "step": 21450
    },
    {
      "epoch": 1.4663756649843132,
      "grad_norm": 1.4192265272140503,
      "learning_rate": 0.0001022461692356659,
      "loss": 0.4632,
      "step": 21500
    },
    {
      "epoch": 1.469785840949393,
      "grad_norm": 10.498298645019531,
      "learning_rate": 0.00010201882417132724,
      "loss": 0.5014,
      "step": 21550
    },
    {
      "epoch": 1.4731960169144727,
      "grad_norm": 5.691461086273193,
      "learning_rate": 0.00010179147910698858,
      "loss": 0.4686,
      "step": 21600
    },
    {
      "epoch": 1.4766061928795526,
      "grad_norm": 13.602937698364258,
      "learning_rate": 0.00010156413404264995,
      "loss": 0.4598,
      "step": 21650
    },
    {
      "epoch": 1.4800163688446324,
      "grad_norm": 8.978404998779297,
      "learning_rate": 0.00010133678897831129,
      "loss": 0.4758,
      "step": 21700
    },
    {
      "epoch": 1.4834265448097121,
      "grad_norm": 12.764100074768066,
      "learning_rate": 0.00010110944391397264,
      "loss": 0.4014,
      "step": 21750
    },
    {
      "epoch": 1.486836720774792,
      "grad_norm": 1.2064381837844849,
      "learning_rate": 0.00010088209884963398,
      "loss": 0.4777,
      "step": 21800
    },
    {
      "epoch": 1.4902468967398717,
      "grad_norm": 5.121671199798584,
      "learning_rate": 0.00010065475378529532,
      "loss": 0.4676,
      "step": 21850
    },
    {
      "epoch": 1.4936570727049516,
      "grad_norm": 5.453814506530762,
      "learning_rate": 0.00010042740872095669,
      "loss": 0.3946,
      "step": 21900
    },
    {
      "epoch": 1.4970672486700314,
      "grad_norm": 4.221184730529785,
      "learning_rate": 0.00010020006365661803,
      "loss": 0.4444,
      "step": 21950
    },
    {
      "epoch": 1.5004774246351111,
      "grad_norm": 2.719599723815918,
      "learning_rate": 9.997271859227937e-05,
      "loss": 0.4702,
      "step": 22000
    },
    {
      "epoch": 1.503887600600191,
      "grad_norm": 5.348362445831299,
      "learning_rate": 9.974537352794072e-05,
      "loss": 0.4061,
      "step": 22050
    },
    {
      "epoch": 1.5072977765652706,
      "grad_norm": 6.2696146965026855,
      "learning_rate": 9.951802846360206e-05,
      "loss": 0.403,
      "step": 22100
    },
    {
      "epoch": 1.5107079525303506,
      "grad_norm": 4.0646257400512695,
      "learning_rate": 9.92906833992634e-05,
      "loss": 0.4012,
      "step": 22150
    },
    {
      "epoch": 1.5141181284954304,
      "grad_norm": 1.811536192893982,
      "learning_rate": 9.906333833492474e-05,
      "loss": 0.4242,
      "step": 22200
    },
    {
      "epoch": 1.5175283044605101,
      "grad_norm": 2.591693162918091,
      "learning_rate": 9.88359932705861e-05,
      "loss": 0.4027,
      "step": 22250
    },
    {
      "epoch": 1.52093848042559,
      "grad_norm": 4.228127479553223,
      "learning_rate": 9.860864820624744e-05,
      "loss": 0.4039,
      "step": 22300
    },
    {
      "epoch": 1.5243486563906696,
      "grad_norm": 5.139016151428223,
      "learning_rate": 9.83813031419088e-05,
      "loss": 0.439,
      "step": 22350
    },
    {
      "epoch": 1.5277588323557496,
      "grad_norm": 9.20366382598877,
      "learning_rate": 9.815395807757014e-05,
      "loss": 0.4333,
      "step": 22400
    },
    {
      "epoch": 1.5311690083208294,
      "grad_norm": 2.7850582599639893,
      "learning_rate": 9.792661301323148e-05,
      "loss": 0.3923,
      "step": 22450
    },
    {
      "epoch": 1.5345791842859091,
      "grad_norm": 2.6045377254486084,
      "learning_rate": 9.769926794889284e-05,
      "loss": 0.354,
      "step": 22500
    },
    {
      "epoch": 1.537989360250989,
      "grad_norm": 5.8575310707092285,
      "learning_rate": 9.747192288455418e-05,
      "loss": 0.3425,
      "step": 22550
    },
    {
      "epoch": 1.5413995362160686,
      "grad_norm": 5.871147632598877,
      "learning_rate": 9.724457782021554e-05,
      "loss": 0.4249,
      "step": 22600
    },
    {
      "epoch": 1.5448097121811486,
      "grad_norm": 0.98600172996521,
      "learning_rate": 9.701723275587688e-05,
      "loss": 0.482,
      "step": 22650
    },
    {
      "epoch": 1.5482198881462284,
      "grad_norm": 3.716148853302002,
      "learning_rate": 9.678988769153822e-05,
      "loss": 0.4246,
      "step": 22700
    },
    {
      "epoch": 1.551630064111308,
      "grad_norm": 7.612657070159912,
      "learning_rate": 9.656254262719958e-05,
      "loss": 0.3477,
      "step": 22750
    },
    {
      "epoch": 1.555040240076388,
      "grad_norm": 4.451781749725342,
      "learning_rate": 9.633519756286092e-05,
      "loss": 0.5073,
      "step": 22800
    },
    {
      "epoch": 1.5584504160414676,
      "grad_norm": 7.568390846252441,
      "learning_rate": 9.610785249852226e-05,
      "loss": 0.4279,
      "step": 22850
    },
    {
      "epoch": 1.5618605920065476,
      "grad_norm": 2.771697998046875,
      "learning_rate": 9.58805074341836e-05,
      "loss": 0.3204,
      "step": 22900
    },
    {
      "epoch": 1.5652707679716273,
      "grad_norm": 8.566515922546387,
      "learning_rate": 9.565316236984495e-05,
      "loss": 0.4351,
      "step": 22950
    },
    {
      "epoch": 1.568680943936707,
      "grad_norm": 4.804766654968262,
      "learning_rate": 9.54258173055063e-05,
      "loss": 0.5084,
      "step": 23000
    },
    {
      "epoch": 1.572091119901787,
      "grad_norm": 4.586226940155029,
      "learning_rate": 9.519847224116764e-05,
      "loss": 0.3458,
      "step": 23050
    },
    {
      "epoch": 1.5755012958668666,
      "grad_norm": 7.224055290222168,
      "learning_rate": 9.4971127176829e-05,
      "loss": 0.4856,
      "step": 23100
    },
    {
      "epoch": 1.5789114718319466,
      "grad_norm": 8.99008560180664,
      "learning_rate": 9.474378211249034e-05,
      "loss": 0.4204,
      "step": 23150
    },
    {
      "epoch": 1.5823216477970263,
      "grad_norm": 1.6256636381149292,
      "learning_rate": 9.451643704815168e-05,
      "loss": 0.4153,
      "step": 23200
    },
    {
      "epoch": 1.585731823762106,
      "grad_norm": 4.042881488800049,
      "learning_rate": 9.428909198381304e-05,
      "loss": 0.4747,
      "step": 23250
    },
    {
      "epoch": 1.589141999727186,
      "grad_norm": 2.938539743423462,
      "learning_rate": 9.406174691947438e-05,
      "loss": 0.3992,
      "step": 23300
    },
    {
      "epoch": 1.5925521756922656,
      "grad_norm": 3.213783025741577,
      "learning_rate": 9.383440185513574e-05,
      "loss": 0.3658,
      "step": 23350
    },
    {
      "epoch": 1.5959623516573456,
      "grad_norm": 4.3798699378967285,
      "learning_rate": 9.360705679079708e-05,
      "loss": 0.4243,
      "step": 23400
    },
    {
      "epoch": 1.5993725276224253,
      "grad_norm": 5.450114727020264,
      "learning_rate": 9.337971172645842e-05,
      "loss": 0.4847,
      "step": 23450
    },
    {
      "epoch": 1.602782703587505,
      "grad_norm": 6.423104763031006,
      "learning_rate": 9.315236666211977e-05,
      "loss": 0.3815,
      "step": 23500
    },
    {
      "epoch": 1.606192879552585,
      "grad_norm": 5.685016632080078,
      "learning_rate": 9.292502159778111e-05,
      "loss": 0.3982,
      "step": 23550
    },
    {
      "epoch": 1.6096030555176646,
      "grad_norm": 6.542948246002197,
      "learning_rate": 9.269767653344246e-05,
      "loss": 0.551,
      "step": 23600
    },
    {
      "epoch": 1.6130132314827446,
      "grad_norm": 7.951673984527588,
      "learning_rate": 9.24703314691038e-05,
      "loss": 0.4001,
      "step": 23650
    },
    {
      "epoch": 1.6164234074478243,
      "grad_norm": 1.3227850198745728,
      "learning_rate": 9.224298640476515e-05,
      "loss": 0.469,
      "step": 23700
    },
    {
      "epoch": 1.619833583412904,
      "grad_norm": 10.194079399108887,
      "learning_rate": 9.20156413404265e-05,
      "loss": 0.4285,
      "step": 23750
    },
    {
      "epoch": 1.623243759377984,
      "grad_norm": 8.266427993774414,
      "learning_rate": 9.178829627608785e-05,
      "loss": 0.4053,
      "step": 23800
    },
    {
      "epoch": 1.6266539353430636,
      "grad_norm": 0.554442822933197,
      "learning_rate": 9.15609512117492e-05,
      "loss": 0.2897,
      "step": 23850
    },
    {
      "epoch": 1.6300641113081435,
      "grad_norm": 2.5004093647003174,
      "learning_rate": 9.133360614741055e-05,
      "loss": 0.4492,
      "step": 23900
    },
    {
      "epoch": 1.6334742872732233,
      "grad_norm": 1.26726233959198,
      "learning_rate": 9.110626108307189e-05,
      "loss": 0.4902,
      "step": 23950
    },
    {
      "epoch": 1.636884463238303,
      "grad_norm": 2.218615770339966,
      "learning_rate": 9.087891601873324e-05,
      "loss": 0.4506,
      "step": 24000
    },
    {
      "epoch": 1.640294639203383,
      "grad_norm": 5.238223075866699,
      "learning_rate": 9.065157095439459e-05,
      "loss": 0.5334,
      "step": 24050
    },
    {
      "epoch": 1.6437048151684626,
      "grad_norm": 2.9476046562194824,
      "learning_rate": 9.042422589005594e-05,
      "loss": 0.3899,
      "step": 24100
    },
    {
      "epoch": 1.6471149911335425,
      "grad_norm": 3.268075466156006,
      "learning_rate": 9.019688082571728e-05,
      "loss": 0.3857,
      "step": 24150
    },
    {
      "epoch": 1.6505251670986223,
      "grad_norm": 8.956716537475586,
      "learning_rate": 8.996953576137863e-05,
      "loss": 0.4954,
      "step": 24200
    },
    {
      "epoch": 1.653935343063702,
      "grad_norm": 3.02978515625,
      "learning_rate": 8.974219069703997e-05,
      "loss": 0.5012,
      "step": 24250
    },
    {
      "epoch": 1.657345519028782,
      "grad_norm": 4.907317638397217,
      "learning_rate": 8.951484563270131e-05,
      "loss": 0.4254,
      "step": 24300
    },
    {
      "epoch": 1.6607556949938616,
      "grad_norm": 4.575362205505371,
      "learning_rate": 8.928750056836267e-05,
      "loss": 0.4206,
      "step": 24350
    },
    {
      "epoch": 1.6641658709589415,
      "grad_norm": 17.565977096557617,
      "learning_rate": 8.906015550402401e-05,
      "loss": 0.486,
      "step": 24400
    },
    {
      "epoch": 1.6675760469240213,
      "grad_norm": 5.8525800704956055,
      "learning_rate": 8.883281043968535e-05,
      "loss": 0.3933,
      "step": 24450
    },
    {
      "epoch": 1.670986222889101,
      "grad_norm": 8.028573989868164,
      "learning_rate": 8.860546537534671e-05,
      "loss": 0.3761,
      "step": 24500
    },
    {
      "epoch": 1.674396398854181,
      "grad_norm": 4.106091022491455,
      "learning_rate": 8.837812031100805e-05,
      "loss": 0.4727,
      "step": 24550
    },
    {
      "epoch": 1.6778065748192605,
      "grad_norm": 1.579793095588684,
      "learning_rate": 8.81507752466694e-05,
      "loss": 0.4134,
      "step": 24600
    },
    {
      "epoch": 1.6812167507843405,
      "grad_norm": 1.2686973810195923,
      "learning_rate": 8.792343018233075e-05,
      "loss": 0.5079,
      "step": 24650
    },
    {
      "epoch": 1.6846269267494203,
      "grad_norm": 7.02952766418457,
      "learning_rate": 8.769608511799209e-05,
      "loss": 0.5239,
      "step": 24700
    },
    {
      "epoch": 1.6880371027145,
      "grad_norm": 8.208442687988281,
      "learning_rate": 8.746874005365345e-05,
      "loss": 0.4351,
      "step": 24750
    },
    {
      "epoch": 1.69144727867958,
      "grad_norm": 5.775270938873291,
      "learning_rate": 8.724139498931479e-05,
      "loss": 0.4499,
      "step": 24800
    },
    {
      "epoch": 1.6948574546446595,
      "grad_norm": 0.8410020470619202,
      "learning_rate": 8.701404992497613e-05,
      "loss": 0.4427,
      "step": 24850
    },
    {
      "epoch": 1.6982676306097395,
      "grad_norm": 3.716224193572998,
      "learning_rate": 8.678670486063747e-05,
      "loss": 0.4447,
      "step": 24900
    },
    {
      "epoch": 1.7016778065748193,
      "grad_norm": 3.271707773208618,
      "learning_rate": 8.655935979629882e-05,
      "loss": 0.4163,
      "step": 24950
    },
    {
      "epoch": 1.705087982539899,
      "grad_norm": 1.4520227909088135,
      "learning_rate": 8.633201473196017e-05,
      "loss": 0.4464,
      "step": 25000
    },
    {
      "epoch": 1.708498158504979,
      "grad_norm": 4.272363185882568,
      "learning_rate": 8.610466966762151e-05,
      "loss": 0.4142,
      "step": 25050
    },
    {
      "epoch": 1.7119083344700585,
      "grad_norm": 3.6379542350769043,
      "learning_rate": 8.587732460328287e-05,
      "loss": 0.3677,
      "step": 25100
    },
    {
      "epoch": 1.7153185104351385,
      "grad_norm": 3.2372307777404785,
      "learning_rate": 8.564997953894421e-05,
      "loss": 0.3712,
      "step": 25150
    },
    {
      "epoch": 1.7187286864002183,
      "grad_norm": 5.743680953979492,
      "learning_rate": 8.542263447460555e-05,
      "loss": 0.3953,
      "step": 25200
    },
    {
      "epoch": 1.722138862365298,
      "grad_norm": 1.4058570861816406,
      "learning_rate": 8.519528941026691e-05,
      "loss": 0.4485,
      "step": 25250
    },
    {
      "epoch": 1.725549038330378,
      "grad_norm": 3.49969482421875,
      "learning_rate": 8.496794434592825e-05,
      "loss": 0.4103,
      "step": 25300
    },
    {
      "epoch": 1.7289592142954575,
      "grad_norm": 0.7701848745346069,
      "learning_rate": 8.474059928158961e-05,
      "loss": 0.3976,
      "step": 25350
    },
    {
      "epoch": 1.7323693902605375,
      "grad_norm": 2.4572596549987793,
      "learning_rate": 8.451325421725095e-05,
      "loss": 0.4108,
      "step": 25400
    },
    {
      "epoch": 1.7357795662256172,
      "grad_norm": 5.1824822425842285,
      "learning_rate": 8.42859091529123e-05,
      "loss": 0.4739,
      "step": 25450
    },
    {
      "epoch": 1.739189742190697,
      "grad_norm": 9.113360404968262,
      "learning_rate": 8.405856408857365e-05,
      "loss": 0.4737,
      "step": 25500
    },
    {
      "epoch": 1.742599918155777,
      "grad_norm": 6.157170295715332,
      "learning_rate": 8.383121902423499e-05,
      "loss": 0.3623,
      "step": 25550
    },
    {
      "epoch": 1.7460100941208565,
      "grad_norm": 8.619439125061035,
      "learning_rate": 8.360387395989633e-05,
      "loss": 0.5103,
      "step": 25600
    },
    {
      "epoch": 1.7494202700859365,
      "grad_norm": 1.166980504989624,
      "learning_rate": 8.337652889555768e-05,
      "loss": 0.4471,
      "step": 25650
    },
    {
      "epoch": 1.7528304460510162,
      "grad_norm": 4.520885944366455,
      "learning_rate": 8.314918383121902e-05,
      "loss": 0.4608,
      "step": 25700
    },
    {
      "epoch": 1.756240622016096,
      "grad_norm": 4.964014053344727,
      "learning_rate": 8.292183876688037e-05,
      "loss": 0.452,
      "step": 25750
    },
    {
      "epoch": 1.759650797981176,
      "grad_norm": 3.484419584274292,
      "learning_rate": 8.269449370254172e-05,
      "loss": 0.4018,
      "step": 25800
    },
    {
      "epoch": 1.7630609739462555,
      "grad_norm": 8.565823554992676,
      "learning_rate": 8.246714863820307e-05,
      "loss": 0.4131,
      "step": 25850
    },
    {
      "epoch": 1.7664711499113355,
      "grad_norm": 4.219388961791992,
      "learning_rate": 8.223980357386441e-05,
      "loss": 0.406,
      "step": 25900
    },
    {
      "epoch": 1.7698813258764152,
      "grad_norm": 4.99745512008667,
      "learning_rate": 8.201245850952576e-05,
      "loss": 0.4531,
      "step": 25950
    },
    {
      "epoch": 1.773291501841495,
      "grad_norm": 4.999008655548096,
      "learning_rate": 8.178511344518711e-05,
      "loss": 0.5167,
      "step": 26000
    },
    {
      "epoch": 1.776701677806575,
      "grad_norm": 3.893268585205078,
      "learning_rate": 8.155776838084846e-05,
      "loss": 0.3947,
      "step": 26050
    },
    {
      "epoch": 1.7801118537716545,
      "grad_norm": 3.9992408752441406,
      "learning_rate": 8.133042331650981e-05,
      "loss": 0.3953,
      "step": 26100
    },
    {
      "epoch": 1.7835220297367345,
      "grad_norm": 4.521263122558594,
      "learning_rate": 8.110307825217115e-05,
      "loss": 0.3852,
      "step": 26150
    },
    {
      "epoch": 1.7869322057018142,
      "grad_norm": 1.873976230621338,
      "learning_rate": 8.08757331878325e-05,
      "loss": 0.4384,
      "step": 26200
    },
    {
      "epoch": 1.790342381666894,
      "grad_norm": 5.163737773895264,
      "learning_rate": 8.064838812349385e-05,
      "loss": 0.3828,
      "step": 26250
    },
    {
      "epoch": 1.793752557631974,
      "grad_norm": 7.451559543609619,
      "learning_rate": 8.04210430591552e-05,
      "loss": 0.3426,
      "step": 26300
    },
    {
      "epoch": 1.7971627335970535,
      "grad_norm": 5.569664478302002,
      "learning_rate": 8.019369799481654e-05,
      "loss": 0.4668,
      "step": 26350
    },
    {
      "epoch": 1.8005729095621335,
      "grad_norm": 6.0873565673828125,
      "learning_rate": 7.996635293047788e-05,
      "loss": 0.5108,
      "step": 26400
    },
    {
      "epoch": 1.8039830855272132,
      "grad_norm": 5.956562042236328,
      "learning_rate": 7.973900786613922e-05,
      "loss": 0.5326,
      "step": 26450
    },
    {
      "epoch": 1.807393261492293,
      "grad_norm": 3.2514891624450684,
      "learning_rate": 7.951166280180058e-05,
      "loss": 0.49,
      "step": 26500
    },
    {
      "epoch": 1.810803437457373,
      "grad_norm": 2.9351673126220703,
      "learning_rate": 7.928431773746192e-05,
      "loss": 0.4597,
      "step": 26550
    },
    {
      "epoch": 1.8142136134224525,
      "grad_norm": 6.005100250244141,
      "learning_rate": 7.905697267312328e-05,
      "loss": 0.3719,
      "step": 26600
    },
    {
      "epoch": 1.8176237893875324,
      "grad_norm": 5.065925598144531,
      "learning_rate": 7.882962760878462e-05,
      "loss": 0.4972,
      "step": 26650
    },
    {
      "epoch": 1.8210339653526122,
      "grad_norm": 3.9613325595855713,
      "learning_rate": 7.860228254444596e-05,
      "loss": 0.4622,
      "step": 26700
    },
    {
      "epoch": 1.824444141317692,
      "grad_norm": 9.722103118896484,
      "learning_rate": 7.837493748010732e-05,
      "loss": 0.4233,
      "step": 26750
    },
    {
      "epoch": 1.827854317282772,
      "grad_norm": 2.013824701309204,
      "learning_rate": 7.814759241576866e-05,
      "loss": 0.3575,
      "step": 26800
    },
    {
      "epoch": 1.8312644932478515,
      "grad_norm": 3.1865525245666504,
      "learning_rate": 7.792024735143001e-05,
      "loss": 0.4843,
      "step": 26850
    },
    {
      "epoch": 1.8346746692129314,
      "grad_norm": 1.7535450458526611,
      "learning_rate": 7.769290228709136e-05,
      "loss": 0.4487,
      "step": 26900
    },
    {
      "epoch": 1.8380848451780112,
      "grad_norm": 5.162252426147461,
      "learning_rate": 7.74655572227527e-05,
      "loss": 0.4037,
      "step": 26950
    },
    {
      "epoch": 1.841495021143091,
      "grad_norm": 5.42517614364624,
      "learning_rate": 7.723821215841404e-05,
      "loss": 0.4307,
      "step": 27000
    },
    {
      "epoch": 1.844905197108171,
      "grad_norm": 5.2209272384643555,
      "learning_rate": 7.701086709407538e-05,
      "loss": 0.4201,
      "step": 27050
    },
    {
      "epoch": 1.8483153730732504,
      "grad_norm": 2.5472357273101807,
      "learning_rate": 7.678352202973674e-05,
      "loss": 0.4098,
      "step": 27100
    },
    {
      "epoch": 1.8517255490383304,
      "grad_norm": 8.299898147583008,
      "learning_rate": 7.655617696539808e-05,
      "loss": 0.3772,
      "step": 27150
    },
    {
      "epoch": 1.8551357250034102,
      "grad_norm": 4.0980963706970215,
      "learning_rate": 7.632883190105942e-05,
      "loss": 0.3877,
      "step": 27200
    },
    {
      "epoch": 1.85854590096849,
      "grad_norm": 4.909539699554443,
      "learning_rate": 7.610148683672078e-05,
      "loss": 0.4552,
      "step": 27250
    },
    {
      "epoch": 1.86195607693357,
      "grad_norm": 9.400200843811035,
      "learning_rate": 7.587414177238212e-05,
      "loss": 0.4234,
      "step": 27300
    },
    {
      "epoch": 1.8653662528986494,
      "grad_norm": 5.142818450927734,
      "learning_rate": 7.564679670804348e-05,
      "loss": 0.4597,
      "step": 27350
    },
    {
      "epoch": 1.8687764288637294,
      "grad_norm": 2.539374828338623,
      "learning_rate": 7.541945164370482e-05,
      "loss": 0.3934,
      "step": 27400
    },
    {
      "epoch": 1.8721866048288092,
      "grad_norm": 4.135684967041016,
      "learning_rate": 7.519210657936616e-05,
      "loss": 0.3888,
      "step": 27450
    },
    {
      "epoch": 1.875596780793889,
      "grad_norm": 4.245568752288818,
      "learning_rate": 7.496476151502752e-05,
      "loss": 0.4523,
      "step": 27500
    },
    {
      "epoch": 1.879006956758969,
      "grad_norm": 3.7724456787109375,
      "learning_rate": 7.473741645068886e-05,
      "loss": 0.3844,
      "step": 27550
    },
    {
      "epoch": 1.8824171327240484,
      "grad_norm": 7.537285804748535,
      "learning_rate": 7.451007138635022e-05,
      "loss": 0.3799,
      "step": 27600
    },
    {
      "epoch": 1.8858273086891284,
      "grad_norm": 3.130601644515991,
      "learning_rate": 7.428272632201156e-05,
      "loss": 0.3799,
      "step": 27650
    },
    {
      "epoch": 1.8892374846542082,
      "grad_norm": 7.90205717086792,
      "learning_rate": 7.40553812576729e-05,
      "loss": 0.3985,
      "step": 27700
    },
    {
      "epoch": 1.892647660619288,
      "grad_norm": 2.5056097507476807,
      "learning_rate": 7.382803619333424e-05,
      "loss": 0.423,
      "step": 27750
    },
    {
      "epoch": 1.8960578365843679,
      "grad_norm": 7.984147548675537,
      "learning_rate": 7.360069112899559e-05,
      "loss": 0.411,
      "step": 27800
    },
    {
      "epoch": 1.8994680125494474,
      "grad_norm": 3.3401424884796143,
      "learning_rate": 7.337334606465694e-05,
      "loss": 0.3856,
      "step": 27850
    },
    {
      "epoch": 1.9028781885145274,
      "grad_norm": 7.288693428039551,
      "learning_rate": 7.314600100031828e-05,
      "loss": 0.4136,
      "step": 27900
    },
    {
      "epoch": 1.9062883644796071,
      "grad_norm": 5.859524726867676,
      "learning_rate": 7.291865593597963e-05,
      "loss": 0.4171,
      "step": 27950
    },
    {
      "epoch": 1.909698540444687,
      "grad_norm": 7.308797359466553,
      "learning_rate": 7.269131087164098e-05,
      "loss": 0.3812,
      "step": 28000
    },
    {
      "epoch": 1.9131087164097669,
      "grad_norm": 1.2245045900344849,
      "learning_rate": 7.246396580730233e-05,
      "loss": 0.3812,
      "step": 28050
    },
    {
      "epoch": 1.9165188923748464,
      "grad_norm": 5.9979119300842285,
      "learning_rate": 7.223662074296368e-05,
      "loss": 0.4816,
      "step": 28100
    },
    {
      "epoch": 1.9199290683399264,
      "grad_norm": 3.829751491546631,
      "learning_rate": 7.200927567862502e-05,
      "loss": 0.432,
      "step": 28150
    },
    {
      "epoch": 1.9233392443050061,
      "grad_norm": 1.997638463973999,
      "learning_rate": 7.178193061428637e-05,
      "loss": 0.423,
      "step": 28200
    },
    {
      "epoch": 1.9267494202700859,
      "grad_norm": 7.752503395080566,
      "learning_rate": 7.155458554994772e-05,
      "loss": 0.4436,
      "step": 28250
    },
    {
      "epoch": 1.9301595962351659,
      "grad_norm": 6.34194803237915,
      "learning_rate": 7.132724048560906e-05,
      "loss": 0.4508,
      "step": 28300
    },
    {
      "epoch": 1.9335697722002454,
      "grad_norm": 9.84952449798584,
      "learning_rate": 7.10998954212704e-05,
      "loss": 0.4154,
      "step": 28350
    },
    {
      "epoch": 1.9369799481653254,
      "grad_norm": 3.4124345779418945,
      "learning_rate": 7.087255035693175e-05,
      "loss": 0.4076,
      "step": 28400
    },
    {
      "epoch": 1.9403901241304051,
      "grad_norm": 3.250133514404297,
      "learning_rate": 7.064520529259309e-05,
      "loss": 0.4838,
      "step": 28450
    },
    {
      "epoch": 1.9438003000954849,
      "grad_norm": 4.474167823791504,
      "learning_rate": 7.041786022825445e-05,
      "loss": 0.3888,
      "step": 28500
    },
    {
      "epoch": 1.9472104760605649,
      "grad_norm": 1.3447681665420532,
      "learning_rate": 7.019051516391579e-05,
      "loss": 0.4724,
      "step": 28550
    },
    {
      "epoch": 1.9506206520256444,
      "grad_norm": 8.763943672180176,
      "learning_rate": 6.996317009957715e-05,
      "loss": 0.4136,
      "step": 28600
    },
    {
      "epoch": 1.9540308279907244,
      "grad_norm": 3.618436098098755,
      "learning_rate": 6.973582503523849e-05,
      "loss": 0.344,
      "step": 28650
    },
    {
      "epoch": 1.9574410039558041,
      "grad_norm": 6.3321661949157715,
      "learning_rate": 6.950847997089983e-05,
      "loss": 0.4364,
      "step": 28700
    },
    {
      "epoch": 1.9608511799208839,
      "grad_norm": 11.925025939941406,
      "learning_rate": 6.928113490656119e-05,
      "loss": 0.4014,
      "step": 28750
    },
    {
      "epoch": 1.9642613558859638,
      "grad_norm": 4.3375959396362305,
      "learning_rate": 6.905378984222253e-05,
      "loss": 0.342,
      "step": 28800
    },
    {
      "epoch": 1.9676715318510434,
      "grad_norm": 8.92586898803711,
      "learning_rate": 6.882644477788388e-05,
      "loss": 0.3988,
      "step": 28850
    },
    {
      "epoch": 1.9710817078161234,
      "grad_norm": 7.2613325119018555,
      "learning_rate": 6.859909971354523e-05,
      "loss": 0.4206,
      "step": 28900
    },
    {
      "epoch": 1.974491883781203,
      "grad_norm": 5.534372806549072,
      "learning_rate": 6.837175464920657e-05,
      "loss": 0.4612,
      "step": 28950
    },
    {
      "epoch": 1.9779020597462829,
      "grad_norm": 0.8175675272941589,
      "learning_rate": 6.814440958486792e-05,
      "loss": 0.4726,
      "step": 29000
    },
    {
      "epoch": 1.9813122357113628,
      "grad_norm": 2.9346461296081543,
      "learning_rate": 6.791706452052927e-05,
      "loss": 0.3451,
      "step": 29050
    },
    {
      "epoch": 1.9847224116764424,
      "grad_norm": 0.828588604927063,
      "learning_rate": 6.768971945619061e-05,
      "loss": 0.3388,
      "step": 29100
    },
    {
      "epoch": 1.9881325876415223,
      "grad_norm": 4.194701671600342,
      "learning_rate": 6.746237439185195e-05,
      "loss": 0.5296,
      "step": 29150
    },
    {
      "epoch": 1.991542763606602,
      "grad_norm": 0.964814305305481,
      "learning_rate": 6.72350293275133e-05,
      "loss": 0.3859,
      "step": 29200
    },
    {
      "epoch": 1.9949529395716818,
      "grad_norm": 4.220222473144531,
      "learning_rate": 6.700768426317465e-05,
      "loss": 0.4189,
      "step": 29250
    },
    {
      "epoch": 1.9983631155367618,
      "grad_norm": 5.064876556396484,
      "learning_rate": 6.678033919883599e-05,
      "loss": 0.4149,
      "step": 29300
    },
    {
      "epoch": 2.0,
      "eval_accuracy": 0.8567482544310596,
      "eval_loss": 0.4144425392150879,
      "eval_runtime": 114.3045,
      "eval_samples_per_second": 114.02,
      "eval_steps_per_second": 7.13,
      "step": 29324
    },
    {
      "epoch": 2.0017732915018414,
      "grad_norm": 3.9522814750671387,
      "learning_rate": 6.655299413449735e-05,
      "loss": 0.5054,
      "step": 29350
    },
    {
      "epoch": 2.0051834674669213,
      "grad_norm": 3.078972816467285,
      "learning_rate": 6.632564907015869e-05,
      "loss": 0.3626,
      "step": 29400
    },
    {
      "epoch": 2.0085936434320013,
      "grad_norm": 11.839739799499512,
      "learning_rate": 6.609830400582003e-05,
      "loss": 0.4137,
      "step": 29450
    },
    {
      "epoch": 2.012003819397081,
      "grad_norm": 0.5130984783172607,
      "learning_rate": 6.587095894148139e-05,
      "loss": 0.3252,
      "step": 29500
    },
    {
      "epoch": 2.015413995362161,
      "grad_norm": 5.730861186981201,
      "learning_rate": 6.564361387714273e-05,
      "loss": 0.3842,
      "step": 29550
    },
    {
      "epoch": 2.0188241713272403,
      "grad_norm": 3.613720655441284,
      "learning_rate": 6.541626881280409e-05,
      "loss": 0.4342,
      "step": 29600
    },
    {
      "epoch": 2.0222343472923203,
      "grad_norm": 5.583342552185059,
      "learning_rate": 6.518892374846543e-05,
      "loss": 0.3777,
      "step": 29650
    },
    {
      "epoch": 2.0256445232574003,
      "grad_norm": 16.373367309570312,
      "learning_rate": 6.496157868412677e-05,
      "loss": 0.3524,
      "step": 29700
    },
    {
      "epoch": 2.02905469922248,
      "grad_norm": 6.571567535400391,
      "learning_rate": 6.473423361978813e-05,
      "loss": 0.3169,
      "step": 29750
    },
    {
      "epoch": 2.03246487518756,
      "grad_norm": 7.6189422607421875,
      "learning_rate": 6.450688855544947e-05,
      "loss": 0.4198,
      "step": 29800
    },
    {
      "epoch": 2.0358750511526393,
      "grad_norm": 1.3408464193344116,
      "learning_rate": 6.427954349111081e-05,
      "loss": 0.4109,
      "step": 29850
    },
    {
      "epoch": 2.0392852271177193,
      "grad_norm": 3.352360725402832,
      "learning_rate": 6.405219842677215e-05,
      "loss": 0.3108,
      "step": 29900
    },
    {
      "epoch": 2.0426954030827993,
      "grad_norm": 8.714844703674316,
      "learning_rate": 6.38248533624335e-05,
      "loss": 0.4285,
      "step": 29950
    },
    {
      "epoch": 2.046105579047879,
      "grad_norm": 5.907309055328369,
      "learning_rate": 6.359750829809485e-05,
      "loss": 0.4215,
      "step": 30000
    },
    {
      "epoch": 2.049515755012959,
      "grad_norm": 9.282758712768555,
      "learning_rate": 6.33701632337562e-05,
      "loss": 0.4163,
      "step": 30050
    },
    {
      "epoch": 2.0529259309780383,
      "grad_norm": 7.932212829589844,
      "learning_rate": 6.314281816941755e-05,
      "loss": 0.3808,
      "step": 30100
    },
    {
      "epoch": 2.0563361069431183,
      "grad_norm": 4.0319366455078125,
      "learning_rate": 6.291547310507889e-05,
      "loss": 0.4196,
      "step": 30150
    },
    {
      "epoch": 2.0597462829081983,
      "grad_norm": 9.972138404846191,
      "learning_rate": 6.268812804074024e-05,
      "loss": 0.3407,
      "step": 30200
    },
    {
      "epoch": 2.063156458873278,
      "grad_norm": 7.506594181060791,
      "learning_rate": 6.246078297640159e-05,
      "loss": 0.5021,
      "step": 30250
    },
    {
      "epoch": 2.066566634838358,
      "grad_norm": 4.1438117027282715,
      "learning_rate": 6.223343791206293e-05,
      "loss": 0.377,
      "step": 30300
    },
    {
      "epoch": 2.0699768108034373,
      "grad_norm": 0.9792596697807312,
      "learning_rate": 6.200609284772429e-05,
      "loss": 0.3785,
      "step": 30350
    },
    {
      "epoch": 2.0733869867685173,
      "grad_norm": 10.952330589294434,
      "learning_rate": 6.177874778338563e-05,
      "loss": 0.3283,
      "step": 30400
    },
    {
      "epoch": 2.0767971627335973,
      "grad_norm": 4.471826553344727,
      "learning_rate": 6.155140271904697e-05,
      "loss": 0.3939,
      "step": 30450
    },
    {
      "epoch": 2.080207338698677,
      "grad_norm": 6.172761917114258,
      "learning_rate": 6.132405765470832e-05,
      "loss": 0.4304,
      "step": 30500
    },
    {
      "epoch": 2.0836175146637568,
      "grad_norm": 8.686532974243164,
      "learning_rate": 6.109671259036966e-05,
      "loss": 0.3599,
      "step": 30550
    },
    {
      "epoch": 2.0870276906288363,
      "grad_norm": 2.6859025955200195,
      "learning_rate": 6.086936752603102e-05,
      "loss": 0.4253,
      "step": 30600
    },
    {
      "epoch": 2.0904378665939163,
      "grad_norm": 1.5022592544555664,
      "learning_rate": 6.0642022461692364e-05,
      "loss": 0.351,
      "step": 30650
    },
    {
      "epoch": 2.0938480425589963,
      "grad_norm": 0.46062660217285156,
      "learning_rate": 6.0414677397353706e-05,
      "loss": 0.3449,
      "step": 30700
    },
    {
      "epoch": 2.097258218524076,
      "grad_norm": 3.143202066421509,
      "learning_rate": 6.0187332333015055e-05,
      "loss": 0.2938,
      "step": 30750
    },
    {
      "epoch": 2.1006683944891558,
      "grad_norm": 1.9543594121932983,
      "learning_rate": 5.99599872686764e-05,
      "loss": 0.4064,
      "step": 30800
    },
    {
      "epoch": 2.1040785704542353,
      "grad_norm": 5.69590425491333,
      "learning_rate": 5.973264220433775e-05,
      "loss": 0.4688,
      "step": 30850
    },
    {
      "epoch": 2.1074887464193153,
      "grad_norm": 9.559335708618164,
      "learning_rate": 5.9505297139999096e-05,
      "loss": 0.4014,
      "step": 30900
    },
    {
      "epoch": 2.1108989223843952,
      "grad_norm": 7.856595993041992,
      "learning_rate": 5.927795207566044e-05,
      "loss": 0.3973,
      "step": 30950
    },
    {
      "epoch": 2.114309098349475,
      "grad_norm": 3.703407049179077,
      "learning_rate": 5.905060701132179e-05,
      "loss": 0.4415,
      "step": 31000
    },
    {
      "epoch": 2.1177192743145548,
      "grad_norm": 13.920633316040039,
      "learning_rate": 5.882326194698313e-05,
      "loss": 0.4453,
      "step": 31050
    },
    {
      "epoch": 2.1211294502796343,
      "grad_norm": 2.2178900241851807,
      "learning_rate": 5.8595916882644485e-05,
      "loss": 0.3987,
      "step": 31100
    },
    {
      "epoch": 2.1245396262447143,
      "grad_norm": 3.8847787380218506,
      "learning_rate": 5.836857181830583e-05,
      "loss": 0.4416,
      "step": 31150
    },
    {
      "epoch": 2.127949802209794,
      "grad_norm": 7.040185451507568,
      "learning_rate": 5.814122675396717e-05,
      "loss": 0.3585,
      "step": 31200
    },
    {
      "epoch": 2.1313599781748738,
      "grad_norm": 6.282078266143799,
      "learning_rate": 5.7913881689628526e-05,
      "loss": 0.3848,
      "step": 31250
    },
    {
      "epoch": 2.1347701541399537,
      "grad_norm": 12.88172721862793,
      "learning_rate": 5.768653662528987e-05,
      "loss": 0.4617,
      "step": 31300
    },
    {
      "epoch": 2.1381803301050333,
      "grad_norm": 8.84205436706543,
      "learning_rate": 5.745919156095122e-05,
      "loss": 0.3288,
      "step": 31350
    },
    {
      "epoch": 2.1415905060701133,
      "grad_norm": 10.73091983795166,
      "learning_rate": 5.723184649661256e-05,
      "loss": 0.4106,
      "step": 31400
    },
    {
      "epoch": 2.1450006820351932,
      "grad_norm": 9.695930480957031,
      "learning_rate": 5.70045014322739e-05,
      "loss": 0.4052,
      "step": 31450
    },
    {
      "epoch": 2.1484108580002728,
      "grad_norm": 6.7725605964660645,
      "learning_rate": 5.677715636793526e-05,
      "loss": 0.3486,
      "step": 31500
    },
    {
      "epoch": 2.1518210339653527,
      "grad_norm": 6.69489860534668,
      "learning_rate": 5.65498113035966e-05,
      "loss": 0.4381,
      "step": 31550
    },
    {
      "epoch": 2.1552312099304323,
      "grad_norm": 8.656521797180176,
      "learning_rate": 5.6322466239257956e-05,
      "loss": 0.4023,
      "step": 31600
    },
    {
      "epoch": 2.1586413858955122,
      "grad_norm": 0.6373201012611389,
      "learning_rate": 5.60951211749193e-05,
      "loss": 0.3763,
      "step": 31650
    },
    {
      "epoch": 2.1620515618605918,
      "grad_norm": 6.376648902893066,
      "learning_rate": 5.586777611058064e-05,
      "loss": 0.4125,
      "step": 31700
    },
    {
      "epoch": 2.1654617378256718,
      "grad_norm": 5.488105297088623,
      "learning_rate": 5.564043104624199e-05,
      "loss": 0.3447,
      "step": 31750
    },
    {
      "epoch": 2.1688719137907517,
      "grad_norm": 1.9535151720046997,
      "learning_rate": 5.541308598190333e-05,
      "loss": 0.477,
      "step": 31800
    },
    {
      "epoch": 2.1722820897558313,
      "grad_norm": 2.0804049968719482,
      "learning_rate": 5.518574091756469e-05,
      "loss": 0.3033,
      "step": 31850
    },
    {
      "epoch": 2.1756922657209112,
      "grad_norm": 6.27207612991333,
      "learning_rate": 5.495839585322603e-05,
      "loss": 0.3491,
      "step": 31900
    },
    {
      "epoch": 2.179102441685991,
      "grad_norm": 7.941009998321533,
      "learning_rate": 5.473105078888737e-05,
      "loss": 0.4822,
      "step": 31950
    },
    {
      "epoch": 2.1825126176510707,
      "grad_norm": 9.29938793182373,
      "learning_rate": 5.450370572454873e-05,
      "loss": 0.3939,
      "step": 32000
    },
    {
      "epoch": 2.1859227936161507,
      "grad_norm": 5.465115547180176,
      "learning_rate": 5.427636066021007e-05,
      "loss": 0.3413,
      "step": 32050
    },
    {
      "epoch": 2.1893329695812302,
      "grad_norm": 1.0772368907928467,
      "learning_rate": 5.404901559587142e-05,
      "loss": 0.4885,
      "step": 32100
    },
    {
      "epoch": 2.1927431455463102,
      "grad_norm": 9.227721214294434,
      "learning_rate": 5.382167053153276e-05,
      "loss": 0.3591,
      "step": 32150
    },
    {
      "epoch": 2.1961533215113898,
      "grad_norm": 8.187066078186035,
      "learning_rate": 5.3594325467194105e-05,
      "loss": 0.3329,
      "step": 32200
    },
    {
      "epoch": 2.1995634974764697,
      "grad_norm": 6.7933878898620605,
      "learning_rate": 5.336698040285546e-05,
      "loss": 0.444,
      "step": 32250
    },
    {
      "epoch": 2.2029736734415497,
      "grad_norm": 0.580599308013916,
      "learning_rate": 5.31396353385168e-05,
      "loss": 0.3853,
      "step": 32300
    },
    {
      "epoch": 2.2063838494066292,
      "grad_norm": 3.579516887664795,
      "learning_rate": 5.291229027417816e-05,
      "loss": 0.3751,
      "step": 32350
    },
    {
      "epoch": 2.209794025371709,
      "grad_norm": 1.8561357259750366,
      "learning_rate": 5.26849452098395e-05,
      "loss": 0.386,
      "step": 32400
    },
    {
      "epoch": 2.213204201336789,
      "grad_norm": 0.19515855610370636,
      "learning_rate": 5.2457600145500844e-05,
      "loss": 0.4035,
      "step": 32450
    },
    {
      "epoch": 2.2166143773018687,
      "grad_norm": 1.757863998413086,
      "learning_rate": 5.223025508116219e-05,
      "loss": 0.3673,
      "step": 32500
    },
    {
      "epoch": 2.2200245532669487,
      "grad_norm": 3.1431996822357178,
      "learning_rate": 5.2002910016823535e-05,
      "loss": 0.4565,
      "step": 32550
    },
    {
      "epoch": 2.2234347292320282,
      "grad_norm": 4.610400199890137,
      "learning_rate": 5.177556495248488e-05,
      "loss": 0.3869,
      "step": 32600
    },
    {
      "epoch": 2.226844905197108,
      "grad_norm": 3.8816611766815186,
      "learning_rate": 5.154821988814623e-05,
      "loss": 0.3903,
      "step": 32650
    },
    {
      "epoch": 2.2302550811621877,
      "grad_norm": 3.703284740447998,
      "learning_rate": 5.1320874823807576e-05,
      "loss": 0.4977,
      "step": 32700
    },
    {
      "epoch": 2.2336652571272677,
      "grad_norm": 12.771332740783691,
      "learning_rate": 5.1093529759468925e-05,
      "loss": 0.451,
      "step": 32750
    },
    {
      "epoch": 2.2370754330923477,
      "grad_norm": 10.085227966308594,
      "learning_rate": 5.086618469513027e-05,
      "loss": 0.3769,
      "step": 32800
    },
    {
      "epoch": 2.240485609057427,
      "grad_norm": 1.6378734111785889,
      "learning_rate": 5.063883963079161e-05,
      "loss": 0.3332,
      "step": 32850
    },
    {
      "epoch": 2.243895785022507,
      "grad_norm": 6.755098819732666,
      "learning_rate": 5.0411494566452965e-05,
      "loss": 0.3336,
      "step": 32900
    },
    {
      "epoch": 2.247305960987587,
      "grad_norm": 7.79683780670166,
      "learning_rate": 5.018414950211431e-05,
      "loss": 0.367,
      "step": 32950
    },
    {
      "epoch": 2.2507161369526667,
      "grad_norm": 4.6917619705200195,
      "learning_rate": 4.995680443777566e-05,
      "loss": 0.3933,
      "step": 33000
    },
    {
      "epoch": 2.2541263129177467,
      "grad_norm": 4.441407680511475,
      "learning_rate": 4.9729459373437006e-05,
      "loss": 0.358,
      "step": 33050
    },
    {
      "epoch": 2.257536488882826,
      "grad_norm": 2.7374746799468994,
      "learning_rate": 4.9502114309098355e-05,
      "loss": 0.3616,
      "step": 33100
    },
    {
      "epoch": 2.260946664847906,
      "grad_norm": 5.315662860870361,
      "learning_rate": 4.92747692447597e-05,
      "loss": 0.3806,
      "step": 33150
    },
    {
      "epoch": 2.2643568408129857,
      "grad_norm": 1.8816808462142944,
      "learning_rate": 4.9047424180421046e-05,
      "loss": 0.2994,
      "step": 33200
    },
    {
      "epoch": 2.2677670167780657,
      "grad_norm": 7.06082820892334,
      "learning_rate": 4.882007911608239e-05,
      "loss": 0.3448,
      "step": 33250
    },
    {
      "epoch": 2.2711771927431457,
      "grad_norm": 8.98924446105957,
      "learning_rate": 4.859273405174374e-05,
      "loss": 0.3454,
      "step": 33300
    },
    {
      "epoch": 2.274587368708225,
      "grad_norm": 1.1015856266021729,
      "learning_rate": 4.836538898740509e-05,
      "loss": 0.4619,
      "step": 33350
    },
    {
      "epoch": 2.277997544673305,
      "grad_norm": 1.572353720664978,
      "learning_rate": 4.8138043923066436e-05,
      "loss": 0.4384,
      "step": 33400
    },
    {
      "epoch": 2.281407720638385,
      "grad_norm": 3.995262384414673,
      "learning_rate": 4.791069885872778e-05,
      "loss": 0.4506,
      "step": 33450
    },
    {
      "epoch": 2.2848178966034647,
      "grad_norm": 14.503534317016602,
      "learning_rate": 4.768335379438912e-05,
      "loss": 0.399,
      "step": 33500
    },
    {
      "epoch": 2.2882280725685447,
      "grad_norm": 5.4630866050720215,
      "learning_rate": 4.745600873005047e-05,
      "loss": 0.3938,
      "step": 33550
    },
    {
      "epoch": 2.291638248533624,
      "grad_norm": 3.0507073402404785,
      "learning_rate": 4.722866366571182e-05,
      "loss": 0.4015,
      "step": 33600
    },
    {
      "epoch": 2.295048424498704,
      "grad_norm": 2.9902870655059814,
      "learning_rate": 4.700131860137317e-05,
      "loss": 0.3597,
      "step": 33650
    },
    {
      "epoch": 2.2984586004637837,
      "grad_norm": 4.114216327667236,
      "learning_rate": 4.677397353703452e-05,
      "loss": 0.3562,
      "step": 33700
    },
    {
      "epoch": 2.3018687764288637,
      "grad_norm": 9.705781936645508,
      "learning_rate": 4.654662847269586e-05,
      "loss": 0.3893,
      "step": 33750
    },
    {
      "epoch": 2.3052789523939436,
      "grad_norm": 10.52817440032959,
      "learning_rate": 4.631928340835721e-05,
      "loss": 0.37,
      "step": 33800
    },
    {
      "epoch": 2.308689128359023,
      "grad_norm": 3.2047526836395264,
      "learning_rate": 4.609193834401855e-05,
      "loss": 0.334,
      "step": 33850
    },
    {
      "epoch": 2.312099304324103,
      "grad_norm": 7.556942462921143,
      "learning_rate": 4.58645932796799e-05,
      "loss": 0.534,
      "step": 33900
    },
    {
      "epoch": 2.315509480289183,
      "grad_norm": 3.959796667098999,
      "learning_rate": 4.563724821534125e-05,
      "loss": 0.3597,
      "step": 33950
    },
    {
      "epoch": 2.3189196562542627,
      "grad_norm": 15.833536148071289,
      "learning_rate": 4.540990315100259e-05,
      "loss": 0.4361,
      "step": 34000
    },
    {
      "epoch": 2.3223298322193426,
      "grad_norm": 6.278857707977295,
      "learning_rate": 4.518255808666394e-05,
      "loss": 0.3016,
      "step": 34050
    },
    {
      "epoch": 2.325740008184422,
      "grad_norm": 2.7598276138305664,
      "learning_rate": 4.495521302232529e-05,
      "loss": 0.3797,
      "step": 34100
    },
    {
      "epoch": 2.329150184149502,
      "grad_norm": 9.934642791748047,
      "learning_rate": 4.472786795798664e-05,
      "loss": 0.4377,
      "step": 34150
    },
    {
      "epoch": 2.3325603601145817,
      "grad_norm": 4.2752203941345215,
      "learning_rate": 4.450052289364798e-05,
      "loss": 0.381,
      "step": 34200
    },
    {
      "epoch": 2.3359705360796617,
      "grad_norm": 6.949687957763672,
      "learning_rate": 4.4273177829309324e-05,
      "loss": 0.4094,
      "step": 34250
    },
    {
      "epoch": 2.3393807120447416,
      "grad_norm": 8.78080940246582,
      "learning_rate": 4.404583276497067e-05,
      "loss": 0.4307,
      "step": 34300
    },
    {
      "epoch": 2.342790888009821,
      "grad_norm": 1.7227896451950073,
      "learning_rate": 4.381848770063202e-05,
      "loss": 0.3446,
      "step": 34350
    },
    {
      "epoch": 2.346201063974901,
      "grad_norm": 3.458815097808838,
      "learning_rate": 4.359114263629337e-05,
      "loss": 0.3928,
      "step": 34400
    },
    {
      "epoch": 2.349611239939981,
      "grad_norm": 8.467832565307617,
      "learning_rate": 4.336379757195472e-05,
      "loss": 0.3468,
      "step": 34450
    },
    {
      "epoch": 2.3530214159050606,
      "grad_norm": 9.237287521362305,
      "learning_rate": 4.313645250761606e-05,
      "loss": 0.4046,
      "step": 34500
    },
    {
      "epoch": 2.3564315918701406,
      "grad_norm": 3.347724676132202,
      "learning_rate": 4.2909107443277405e-05,
      "loss": 0.3607,
      "step": 34550
    },
    {
      "epoch": 2.35984176783522,
      "grad_norm": 4.194701194763184,
      "learning_rate": 4.2681762378938754e-05,
      "loss": 0.3645,
      "step": 34600
    },
    {
      "epoch": 2.3632519438003,
      "grad_norm": 4.061727046966553,
      "learning_rate": 4.24544173146001e-05,
      "loss": 0.4153,
      "step": 34650
    },
    {
      "epoch": 2.3666621197653797,
      "grad_norm": 9.086857795715332,
      "learning_rate": 4.222707225026145e-05,
      "loss": 0.3738,
      "step": 34700
    },
    {
      "epoch": 2.3700722957304596,
      "grad_norm": 11.006861686706543,
      "learning_rate": 4.1999727185922794e-05,
      "loss": 0.4,
      "step": 34750
    },
    {
      "epoch": 2.3734824716955396,
      "grad_norm": 0.1435566246509552,
      "learning_rate": 4.1772382121584143e-05,
      "loss": 0.3743,
      "step": 34800
    },
    {
      "epoch": 2.376892647660619,
      "grad_norm": 2.5634565353393555,
      "learning_rate": 4.154503705724549e-05,
      "loss": 0.2945,
      "step": 34850
    },
    {
      "epoch": 2.380302823625699,
      "grad_norm": 9.799736022949219,
      "learning_rate": 4.1317691992906835e-05,
      "loss": 0.4786,
      "step": 34900
    },
    {
      "epoch": 2.383712999590779,
      "grad_norm": 5.748486042022705,
      "learning_rate": 4.1090346928568184e-05,
      "loss": 0.5389,
      "step": 34950
    },
    {
      "epoch": 2.3871231755558586,
      "grad_norm": 22.35447120666504,
      "learning_rate": 4.0863001864229526e-05,
      "loss": 0.3393,
      "step": 35000
    },
    {
      "epoch": 2.3905333515209386,
      "grad_norm": 6.414355278015137,
      "learning_rate": 4.0635656799890875e-05,
      "loss": 0.3776,
      "step": 35050
    },
    {
      "epoch": 2.393943527486018,
      "grad_norm": 3.45737361907959,
      "learning_rate": 4.0408311735552225e-05,
      "loss": 0.3675,
      "step": 35100
    },
    {
      "epoch": 2.397353703451098,
      "grad_norm": 8.550771713256836,
      "learning_rate": 4.0180966671213574e-05,
      "loss": 0.3949,
      "step": 35150
    },
    {
      "epoch": 2.4007638794161776,
      "grad_norm": 8.65937614440918,
      "learning_rate": 3.9953621606874916e-05,
      "loss": 0.3828,
      "step": 35200
    },
    {
      "epoch": 2.4041740553812576,
      "grad_norm": 6.676918029785156,
      "learning_rate": 3.972627654253626e-05,
      "loss": 0.3519,
      "step": 35250
    },
    {
      "epoch": 2.4075842313463376,
      "grad_norm": 5.779773235321045,
      "learning_rate": 3.949893147819761e-05,
      "loss": 0.3732,
      "step": 35300
    },
    {
      "epoch": 2.410994407311417,
      "grad_norm": 5.904169082641602,
      "learning_rate": 3.9271586413858957e-05,
      "loss": 0.3951,
      "step": 35350
    },
    {
      "epoch": 2.414404583276497,
      "grad_norm": 7.000365734100342,
      "learning_rate": 3.9044241349520306e-05,
      "loss": 0.3501,
      "step": 35400
    },
    {
      "epoch": 2.417814759241577,
      "grad_norm": 0.8954529762268066,
      "learning_rate": 3.8816896285181655e-05,
      "loss": 0.3126,
      "step": 35450
    },
    {
      "epoch": 2.4212249352066566,
      "grad_norm": 7.438472747802734,
      "learning_rate": 3.8589551220843e-05,
      "loss": 0.4474,
      "step": 35500
    },
    {
      "epoch": 2.4246351111717366,
      "grad_norm": 8.76956558227539,
      "learning_rate": 3.8362206156504346e-05,
      "loss": 0.3755,
      "step": 35550
    },
    {
      "epoch": 2.428045287136816,
      "grad_norm": 8.664963722229004,
      "learning_rate": 3.813486109216569e-05,
      "loss": 0.4035,
      "step": 35600
    },
    {
      "epoch": 2.431455463101896,
      "grad_norm": 7.546303749084473,
      "learning_rate": 3.790751602782704e-05,
      "loss": 0.3114,
      "step": 35650
    },
    {
      "epoch": 2.4348656390669756,
      "grad_norm": 9.399025917053223,
      "learning_rate": 3.768017096348839e-05,
      "loss": 0.3606,
      "step": 35700
    },
    {
      "epoch": 2.4382758150320556,
      "grad_norm": 5.927799701690674,
      "learning_rate": 3.745282589914973e-05,
      "loss": 0.4184,
      "step": 35750
    },
    {
      "epoch": 2.4416859909971356,
      "grad_norm": 4.512916564941406,
      "learning_rate": 3.722548083481108e-05,
      "loss": 0.3835,
      "step": 35800
    },
    {
      "epoch": 2.445096166962215,
      "grad_norm": 0.8737319707870483,
      "learning_rate": 3.699813577047243e-05,
      "loss": 0.3807,
      "step": 35850
    },
    {
      "epoch": 2.448506342927295,
      "grad_norm": 1.879919171333313,
      "learning_rate": 3.677079070613377e-05,
      "loss": 0.347,
      "step": 35900
    },
    {
      "epoch": 2.451916518892375,
      "grad_norm": 8.37722110748291,
      "learning_rate": 3.654344564179512e-05,
      "loss": 0.3842,
      "step": 35950
    },
    {
      "epoch": 2.4553266948574546,
      "grad_norm": 5.190518379211426,
      "learning_rate": 3.631610057745646e-05,
      "loss": 0.3531,
      "step": 36000
    },
    {
      "epoch": 2.4587368708225346,
      "grad_norm": 1.554817795753479,
      "learning_rate": 3.608875551311781e-05,
      "loss": 0.303,
      "step": 36050
    },
    {
      "epoch": 2.462147046787614,
      "grad_norm": 1.0911973714828491,
      "learning_rate": 3.586141044877916e-05,
      "loss": 0.3989,
      "step": 36100
    },
    {
      "epoch": 2.465557222752694,
      "grad_norm": 2.087831497192383,
      "learning_rate": 3.563406538444051e-05,
      "loss": 0.356,
      "step": 36150
    },
    {
      "epoch": 2.4689673987177736,
      "grad_norm": 4.0204949378967285,
      "learning_rate": 3.540672032010186e-05,
      "loss": 0.4102,
      "step": 36200
    },
    {
      "epoch": 2.4723775746828536,
      "grad_norm": 1.8820849657058716,
      "learning_rate": 3.51793752557632e-05,
      "loss": 0.3536,
      "step": 36250
    },
    {
      "epoch": 2.4757877506479335,
      "grad_norm": 5.539511680603027,
      "learning_rate": 3.495203019142454e-05,
      "loss": 0.3225,
      "step": 36300
    },
    {
      "epoch": 2.479197926613013,
      "grad_norm": 7.101624488830566,
      "learning_rate": 3.472468512708589e-05,
      "loss": 0.4048,
      "step": 36350
    },
    {
      "epoch": 2.482608102578093,
      "grad_norm": 7.745348930358887,
      "learning_rate": 3.449734006274724e-05,
      "loss": 0.4269,
      "step": 36400
    },
    {
      "epoch": 2.486018278543173,
      "grad_norm": 7.805912494659424,
      "learning_rate": 3.426999499840859e-05,
      "loss": 0.3766,
      "step": 36450
    },
    {
      "epoch": 2.4894284545082526,
      "grad_norm": 8.80556583404541,
      "learning_rate": 3.404264993406993e-05,
      "loss": 0.3569,
      "step": 36500
    },
    {
      "epoch": 2.4928386304733325,
      "grad_norm": 10.321298599243164,
      "learning_rate": 3.381530486973128e-05,
      "loss": 0.3843,
      "step": 36550
    },
    {
      "epoch": 2.496248806438412,
      "grad_norm": 9.465293884277344,
      "learning_rate": 3.358795980539263e-05,
      "loss": 0.3452,
      "step": 36600
    },
    {
      "epoch": 2.499658982403492,
      "grad_norm": 1.107124924659729,
      "learning_rate": 3.336061474105397e-05,
      "loss": 0.4024,
      "step": 36650
    },
    {
      "epoch": 2.5030691583685716,
      "grad_norm": 1.49710214138031,
      "learning_rate": 3.313326967671532e-05,
      "loss": 0.3123,
      "step": 36700
    },
    {
      "epoch": 2.5064793343336516,
      "grad_norm": 2.63553786277771,
      "learning_rate": 3.2905924612376664e-05,
      "loss": 0.3552,
      "step": 36750
    },
    {
      "epoch": 2.5098895102987315,
      "grad_norm": 2.1144049167633057,
      "learning_rate": 3.267857954803801e-05,
      "loss": 0.3367,
      "step": 36800
    },
    {
      "epoch": 2.513299686263811,
      "grad_norm": 7.661304950714111,
      "learning_rate": 3.245123448369936e-05,
      "loss": 0.4113,
      "step": 36850
    },
    {
      "epoch": 2.516709862228891,
      "grad_norm": 0.9276424050331116,
      "learning_rate": 3.222388941936071e-05,
      "loss": 0.3718,
      "step": 36900
    },
    {
      "epoch": 2.520120038193971,
      "grad_norm": 8.07295036315918,
      "learning_rate": 3.1996544355022054e-05,
      "loss": 0.3424,
      "step": 36950
    },
    {
      "epoch": 2.5235302141590505,
      "grad_norm": 2.995096445083618,
      "learning_rate": 3.1769199290683396e-05,
      "loss": 0.3405,
      "step": 37000
    },
    {
      "epoch": 2.5269403901241305,
      "grad_norm": 7.176470756530762,
      "learning_rate": 3.1541854226344745e-05,
      "loss": 0.432,
      "step": 37050
    },
    {
      "epoch": 2.53035056608921,
      "grad_norm": 10.69936466217041,
      "learning_rate": 3.1314509162006094e-05,
      "loss": 0.3488,
      "step": 37100
    },
    {
      "epoch": 2.53376074205429,
      "grad_norm": 3.330214262008667,
      "learning_rate": 3.108716409766744e-05,
      "loss": 0.3245,
      "step": 37150
    },
    {
      "epoch": 2.5371709180193696,
      "grad_norm": 4.786140441894531,
      "learning_rate": 3.085981903332879e-05,
      "loss": 0.4841,
      "step": 37200
    },
    {
      "epoch": 2.5405810939844495,
      "grad_norm": 6.332691669464111,
      "learning_rate": 3.0632473968990135e-05,
      "loss": 0.322,
      "step": 37250
    },
    {
      "epoch": 2.5439912699495295,
      "grad_norm": 10.613682746887207,
      "learning_rate": 3.040512890465148e-05,
      "loss": 0.4038,
      "step": 37300
    },
    {
      "epoch": 2.547401445914609,
      "grad_norm": 1.3459672927856445,
      "learning_rate": 3.017778384031283e-05,
      "loss": 0.4101,
      "step": 37350
    },
    {
      "epoch": 2.550811621879689,
      "grad_norm": 7.002206325531006,
      "learning_rate": 2.9950438775974175e-05,
      "loss": 0.3633,
      "step": 37400
    },
    {
      "epoch": 2.554221797844769,
      "grad_norm": 2.2223000526428223,
      "learning_rate": 2.9723093711635524e-05,
      "loss": 0.412,
      "step": 37450
    },
    {
      "epoch": 2.5576319738098485,
      "grad_norm": 2.5226004123687744,
      "learning_rate": 2.9495748647296867e-05,
      "loss": 0.3138,
      "step": 37500
    },
    {
      "epoch": 2.5610421497749285,
      "grad_norm": 2.6836488246917725,
      "learning_rate": 2.9268403582958216e-05,
      "loss": 0.3972,
      "step": 37550
    },
    {
      "epoch": 2.564452325740008,
      "grad_norm": 8.688497543334961,
      "learning_rate": 2.904105851861956e-05,
      "loss": 0.385,
      "step": 37600
    },
    {
      "epoch": 2.567862501705088,
      "grad_norm": 10.176358222961426,
      "learning_rate": 2.881371345428091e-05,
      "loss": 0.4245,
      "step": 37650
    },
    {
      "epoch": 2.5712726776701675,
      "grad_norm": 6.08339786529541,
      "learning_rate": 2.8586368389942253e-05,
      "loss": 0.4562,
      "step": 37700
    },
    {
      "epoch": 2.5746828536352475,
      "grad_norm": 10.693866729736328,
      "learning_rate": 2.8359023325603602e-05,
      "loss": 0.3568,
      "step": 37750
    },
    {
      "epoch": 2.5780930296003275,
      "grad_norm": 0.6011192202568054,
      "learning_rate": 2.8131678261264948e-05,
      "loss": 0.3461,
      "step": 37800
    },
    {
      "epoch": 2.581503205565407,
      "grad_norm": 5.975288391113281,
      "learning_rate": 2.7904333196926297e-05,
      "loss": 0.2903,
      "step": 37850
    },
    {
      "epoch": 2.584913381530487,
      "grad_norm": 10.186901092529297,
      "learning_rate": 2.7676988132587646e-05,
      "loss": 0.3656,
      "step": 37900
    },
    {
      "epoch": 2.588323557495567,
      "grad_norm": 1.2206248044967651,
      "learning_rate": 2.744964306824899e-05,
      "loss": 0.4581,
      "step": 37950
    },
    {
      "epoch": 2.5917337334606465,
      "grad_norm": 6.059571266174316,
      "learning_rate": 2.7222298003910334e-05,
      "loss": 0.3603,
      "step": 38000
    },
    {
      "epoch": 2.5951439094257265,
      "grad_norm": 7.9155592918396,
      "learning_rate": 2.6994952939571683e-05,
      "loss": 0.4246,
      "step": 38050
    },
    {
      "epoch": 2.598554085390806,
      "grad_norm": 19.813161849975586,
      "learning_rate": 2.676760787523303e-05,
      "loss": 0.3623,
      "step": 38100
    },
    {
      "epoch": 2.601964261355886,
      "grad_norm": 5.666364669799805,
      "learning_rate": 2.6540262810894378e-05,
      "loss": 0.3832,
      "step": 38150
    },
    {
      "epoch": 2.6053744373209655,
      "grad_norm": 0.6093296408653259,
      "learning_rate": 2.631291774655572e-05,
      "loss": 0.3816,
      "step": 38200
    },
    {
      "epoch": 2.6087846132860455,
      "grad_norm": 2.236583709716797,
      "learning_rate": 2.608557268221707e-05,
      "loss": 0.3912,
      "step": 38250
    },
    {
      "epoch": 2.6121947892511255,
      "grad_norm": 10.450582504272461,
      "learning_rate": 2.5858227617878415e-05,
      "loss": 0.3379,
      "step": 38300
    },
    {
      "epoch": 2.615604965216205,
      "grad_norm": 5.977991104125977,
      "learning_rate": 2.5630882553539764e-05,
      "loss": 0.3981,
      "step": 38350
    },
    {
      "epoch": 2.619015141181285,
      "grad_norm": 3.689974784851074,
      "learning_rate": 2.5403537489201113e-05,
      "loss": 0.3987,
      "step": 38400
    },
    {
      "epoch": 2.622425317146365,
      "grad_norm": 4.683026313781738,
      "learning_rate": 2.5176192424862456e-05,
      "loss": 0.3586,
      "step": 38450
    },
    {
      "epoch": 2.6258354931114445,
      "grad_norm": 9.902985572814941,
      "learning_rate": 2.4948847360523805e-05,
      "loss": 0.3938,
      "step": 38500
    },
    {
      "epoch": 2.6292456690765245,
      "grad_norm": 6.1804351806640625,
      "learning_rate": 2.472150229618515e-05,
      "loss": 0.4254,
      "step": 38550
    },
    {
      "epoch": 2.632655845041604,
      "grad_norm": 5.895055294036865,
      "learning_rate": 2.44941572318465e-05,
      "loss": 0.3515,
      "step": 38600
    },
    {
      "epoch": 2.636066021006684,
      "grad_norm": 6.2009077072143555,
      "learning_rate": 2.4266812167507842e-05,
      "loss": 0.3275,
      "step": 38650
    },
    {
      "epoch": 2.6394761969717635,
      "grad_norm": 6.105666160583496,
      "learning_rate": 2.403946710316919e-05,
      "loss": 0.296,
      "step": 38700
    },
    {
      "epoch": 2.6428863729368435,
      "grad_norm": 1.1840766668319702,
      "learning_rate": 2.381212203883054e-05,
      "loss": 0.3006,
      "step": 38750
    },
    {
      "epoch": 2.6462965489019235,
      "grad_norm": 5.901210784912109,
      "learning_rate": 2.3584776974491886e-05,
      "loss": 0.2331,
      "step": 38800
    },
    {
      "epoch": 2.649706724867003,
      "grad_norm": 0.4060484766960144,
      "learning_rate": 2.3357431910153232e-05,
      "loss": 0.4004,
      "step": 38850
    },
    {
      "epoch": 2.653116900832083,
      "grad_norm": 0.6502436995506287,
      "learning_rate": 2.3130086845814577e-05,
      "loss": 0.4436,
      "step": 38900
    },
    {
      "epoch": 2.656527076797163,
      "grad_norm": 5.884072303771973,
      "learning_rate": 2.2902741781475927e-05,
      "loss": 0.4164,
      "step": 38950
    },
    {
      "epoch": 2.6599372527622425,
      "grad_norm": 4.531895637512207,
      "learning_rate": 2.2675396717137272e-05,
      "loss": 0.447,
      "step": 39000
    },
    {
      "epoch": 2.6633474287273224,
      "grad_norm": 12.380245208740234,
      "learning_rate": 2.2448051652798618e-05,
      "loss": 0.3197,
      "step": 39050
    },
    {
      "epoch": 2.666757604692402,
      "grad_norm": 2.877122163772583,
      "learning_rate": 2.2220706588459967e-05,
      "loss": 0.4086,
      "step": 39100
    },
    {
      "epoch": 2.670167780657482,
      "grad_norm": 6.018792152404785,
      "learning_rate": 2.1993361524121313e-05,
      "loss": 0.3407,
      "step": 39150
    },
    {
      "epoch": 2.6735779566225615,
      "grad_norm": 4.7056427001953125,
      "learning_rate": 2.176601645978266e-05,
      "loss": 0.3484,
      "step": 39200
    },
    {
      "epoch": 2.6769881325876415,
      "grad_norm": 2.625115156173706,
      "learning_rate": 2.1538671395444008e-05,
      "loss": 0.4296,
      "step": 39250
    },
    {
      "epoch": 2.6803983085527214,
      "grad_norm": 0.6976128220558167,
      "learning_rate": 2.1311326331105353e-05,
      "loss": 0.3486,
      "step": 39300
    },
    {
      "epoch": 2.683808484517801,
      "grad_norm": 6.216698169708252,
      "learning_rate": 2.10839812667667e-05,
      "loss": 0.3628,
      "step": 39350
    },
    {
      "epoch": 2.687218660482881,
      "grad_norm": 5.162588119506836,
      "learning_rate": 2.0856636202428045e-05,
      "loss": 0.3302,
      "step": 39400
    },
    {
      "epoch": 2.690628836447961,
      "grad_norm": 17.87155532836914,
      "learning_rate": 2.0629291138089394e-05,
      "loss": 0.3307,
      "step": 39450
    },
    {
      "epoch": 2.6940390124130404,
      "grad_norm": 8.731464385986328,
      "learning_rate": 2.040194607375074e-05,
      "loss": 0.3244,
      "step": 39500
    },
    {
      "epoch": 2.6974491883781204,
      "grad_norm": 2.952174186706543,
      "learning_rate": 2.0174601009412085e-05,
      "loss": 0.3722,
      "step": 39550
    },
    {
      "epoch": 2.7008593643432,
      "grad_norm": 4.484750747680664,
      "learning_rate": 1.9947255945073435e-05,
      "loss": 0.3312,
      "step": 39600
    },
    {
      "epoch": 2.70426954030828,
      "grad_norm": 8.20029067993164,
      "learning_rate": 1.971991088073478e-05,
      "loss": 0.4961,
      "step": 39650
    },
    {
      "epoch": 2.7076797162733595,
      "grad_norm": 10.636069297790527,
      "learning_rate": 1.9492565816396126e-05,
      "loss": 0.3455,
      "step": 39700
    },
    {
      "epoch": 2.7110898922384394,
      "grad_norm": 5.5793867111206055,
      "learning_rate": 1.9265220752057475e-05,
      "loss": 0.3627,
      "step": 39750
    },
    {
      "epoch": 2.7145000682035194,
      "grad_norm": 5.587434768676758,
      "learning_rate": 1.903787568771882e-05,
      "loss": 0.293,
      "step": 39800
    },
    {
      "epoch": 2.717910244168599,
      "grad_norm": 5.918201446533203,
      "learning_rate": 1.8810530623380167e-05,
      "loss": 0.3319,
      "step": 39850
    },
    {
      "epoch": 2.721320420133679,
      "grad_norm": 4.188772201538086,
      "learning_rate": 1.8583185559041512e-05,
      "loss": 0.3951,
      "step": 39900
    },
    {
      "epoch": 2.724730596098759,
      "grad_norm": 2.543696880340576,
      "learning_rate": 1.835584049470286e-05,
      "loss": 0.4262,
      "step": 39950
    },
    {
      "epoch": 2.7281407720638384,
      "grad_norm": 4.1772027015686035,
      "learning_rate": 1.8128495430364207e-05,
      "loss": 0.4267,
      "step": 40000
    },
    {
      "epoch": 2.7315509480289184,
      "grad_norm": 11.85822868347168,
      "learning_rate": 1.7901150366025553e-05,
      "loss": 0.3602,
      "step": 40050
    },
    {
      "epoch": 2.734961123993998,
      "grad_norm": 3.378671169281006,
      "learning_rate": 1.7673805301686902e-05,
      "loss": 0.3591,
      "step": 40100
    },
    {
      "epoch": 2.738371299959078,
      "grad_norm": 9.74979305267334,
      "learning_rate": 1.7446460237348248e-05,
      "loss": 0.4359,
      "step": 40150
    },
    {
      "epoch": 2.7417814759241574,
      "grad_norm": 8.110605239868164,
      "learning_rate": 1.7219115173009597e-05,
      "loss": 0.3535,
      "step": 40200
    },
    {
      "epoch": 2.7451916518892374,
      "grad_norm": 8.566062927246094,
      "learning_rate": 1.699177010867094e-05,
      "loss": 0.4314,
      "step": 40250
    },
    {
      "epoch": 2.7486018278543174,
      "grad_norm": 6.7759809494018555,
      "learning_rate": 1.6764425044332288e-05,
      "loss": 0.4263,
      "step": 40300
    },
    {
      "epoch": 2.752012003819397,
      "grad_norm": 6.6416192054748535,
      "learning_rate": 1.6537079979993637e-05,
      "loss": 0.3181,
      "step": 40350
    },
    {
      "epoch": 2.755422179784477,
      "grad_norm": 3.4022622108459473,
      "learning_rate": 1.630973491565498e-05,
      "loss": 0.3409,
      "step": 40400
    },
    {
      "epoch": 2.758832355749557,
      "grad_norm": 8.049613952636719,
      "learning_rate": 1.608238985131633e-05,
      "loss": 0.4111,
      "step": 40450
    },
    {
      "epoch": 2.7622425317146364,
      "grad_norm": 2.145328998565674,
      "learning_rate": 1.5855044786977675e-05,
      "loss": 0.2641,
      "step": 40500
    },
    {
      "epoch": 2.7656527076797164,
      "grad_norm": 4.274640083312988,
      "learning_rate": 1.5627699722639024e-05,
      "loss": 0.3287,
      "step": 40550
    },
    {
      "epoch": 2.769062883644796,
      "grad_norm": 15.80679702758789,
      "learning_rate": 1.540035465830037e-05,
      "loss": 0.3765,
      "step": 40600
    },
    {
      "epoch": 2.772473059609876,
      "grad_norm": 5.914567947387695,
      "learning_rate": 1.5173009593961715e-05,
      "loss": 0.3245,
      "step": 40650
    },
    {
      "epoch": 2.7758832355749554,
      "grad_norm": 6.218752861022949,
      "learning_rate": 1.4945664529623062e-05,
      "loss": 0.394,
      "step": 40700
    },
    {
      "epoch": 2.7792934115400354,
      "grad_norm": 5.814619064331055,
      "learning_rate": 1.4718319465284408e-05,
      "loss": 0.4095,
      "step": 40750
    },
    {
      "epoch": 2.7827035875051154,
      "grad_norm": 11.349270820617676,
      "learning_rate": 1.4490974400945756e-05,
      "loss": 0.3531,
      "step": 40800
    },
    {
      "epoch": 2.786113763470195,
      "grad_norm": 8.174430847167969,
      "learning_rate": 1.4263629336607103e-05,
      "loss": 0.4882,
      "step": 40850
    },
    {
      "epoch": 2.789523939435275,
      "grad_norm": 16.994091033935547,
      "learning_rate": 1.4036284272268449e-05,
      "loss": 0.3419,
      "step": 40900
    },
    {
      "epoch": 2.792934115400355,
      "grad_norm": 2.6578469276428223,
      "learning_rate": 1.3808939207929796e-05,
      "loss": 0.3296,
      "step": 40950
    },
    {
      "epoch": 2.7963442913654344,
      "grad_norm": 9.493252754211426,
      "learning_rate": 1.3581594143591142e-05,
      "loss": 0.3092,
      "step": 41000
    },
    {
      "epoch": 2.7997544673305144,
      "grad_norm": 1.4019283056259155,
      "learning_rate": 1.335424907925249e-05,
      "loss": 0.3926,
      "step": 41050
    },
    {
      "epoch": 2.803164643295594,
      "grad_norm": 4.9089531898498535,
      "learning_rate": 1.3126904014913838e-05,
      "loss": 0.3695,
      "step": 41100
    },
    {
      "epoch": 2.806574819260674,
      "grad_norm": 6.015876293182373,
      "learning_rate": 1.2899558950575182e-05,
      "loss": 0.3308,
      "step": 41150
    },
    {
      "epoch": 2.8099849952257534,
      "grad_norm": 1.0673370361328125,
      "learning_rate": 1.2672213886236532e-05,
      "loss": 0.368,
      "step": 41200
    },
    {
      "epoch": 2.8133951711908334,
      "grad_norm": 1.0244500637054443,
      "learning_rate": 1.2444868821897877e-05,
      "loss": 0.4086,
      "step": 41250
    },
    {
      "epoch": 2.8168053471559134,
      "grad_norm": 7.787458896636963,
      "learning_rate": 1.2217523757559223e-05,
      "loss": 0.4005,
      "step": 41300
    },
    {
      "epoch": 2.820215523120993,
      "grad_norm": 7.650581359863281,
      "learning_rate": 1.199017869322057e-05,
      "loss": 0.3734,
      "step": 41350
    },
    {
      "epoch": 2.823625699086073,
      "grad_norm": 7.792172431945801,
      "learning_rate": 1.1762833628881916e-05,
      "loss": 0.3477,
      "step": 41400
    },
    {
      "epoch": 2.827035875051153,
      "grad_norm": 1.8397272825241089,
      "learning_rate": 1.1535488564543265e-05,
      "loss": 0.376,
      "step": 41450
    },
    {
      "epoch": 2.8304460510162324,
      "grad_norm": 3.322849988937378,
      "learning_rate": 1.1308143500204611e-05,
      "loss": 0.4551,
      "step": 41500
    },
    {
      "epoch": 2.8338562269813123,
      "grad_norm": 3.815298080444336,
      "learning_rate": 1.1080798435865958e-05,
      "loss": 0.3593,
      "step": 41550
    },
    {
      "epoch": 2.837266402946392,
      "grad_norm": 5.5623297691345215,
      "learning_rate": 1.0853453371527304e-05,
      "loss": 0.4109,
      "step": 41600
    },
    {
      "epoch": 2.840676578911472,
      "grad_norm": 12.190099716186523,
      "learning_rate": 1.0626108307188652e-05,
      "loss": 0.373,
      "step": 41650
    },
    {
      "epoch": 2.8440867548765514,
      "grad_norm": 10.303080558776855,
      "learning_rate": 1.0398763242849999e-05,
      "loss": 0.3855,
      "step": 41700
    },
    {
      "epoch": 2.8474969308416314,
      "grad_norm": 2.506258726119995,
      "learning_rate": 1.0171418178511345e-05,
      "loss": 0.3188,
      "step": 41750
    },
    {
      "epoch": 2.8509071068067113,
      "grad_norm": 7.51561164855957,
      "learning_rate": 9.944073114172692e-06,
      "loss": 0.2907,
      "step": 41800
    },
    {
      "epoch": 2.854317282771791,
      "grad_norm": 2.5312962532043457,
      "learning_rate": 9.716728049834038e-06,
      "loss": 0.3567,
      "step": 41850
    },
    {
      "epoch": 2.857727458736871,
      "grad_norm": 6.946571350097656,
      "learning_rate": 9.489382985495385e-06,
      "loss": 0.3301,
      "step": 41900
    },
    {
      "epoch": 2.861137634701951,
      "grad_norm": 6.735598564147949,
      "learning_rate": 9.262037921156733e-06,
      "loss": 0.3261,
      "step": 41950
    },
    {
      "epoch": 2.8645478106670303,
      "grad_norm": 2.8392255306243896,
      "learning_rate": 9.034692856818078e-06,
      "loss": 0.3941,
      "step": 42000
    },
    {
      "epoch": 2.8679579866321103,
      "grad_norm": 4.284955024719238,
      "learning_rate": 8.807347792479426e-06,
      "loss": 0.3996,
      "step": 42050
    },
    {
      "epoch": 2.87136816259719,
      "grad_norm": 1.089365005493164,
      "learning_rate": 8.580002728140772e-06,
      "loss": 0.3739,
      "step": 42100
    },
    {
      "epoch": 2.87477833856227,
      "grad_norm": 12.229735374450684,
      "learning_rate": 8.352657663802119e-06,
      "loss": 0.3837,
      "step": 42150
    },
    {
      "epoch": 2.8781885145273494,
      "grad_norm": 0.8178938031196594,
      "learning_rate": 8.125312599463466e-06,
      "loss": 0.3648,
      "step": 42200
    },
    {
      "epoch": 2.8815986904924293,
      "grad_norm": 5.675065040588379,
      "learning_rate": 7.897967535124814e-06,
      "loss": 0.3159,
      "step": 42250
    },
    {
      "epoch": 2.8850088664575093,
      "grad_norm": 9.637990951538086,
      "learning_rate": 7.67062247078616e-06,
      "loss": 0.4334,
      "step": 42300
    },
    {
      "epoch": 2.888419042422589,
      "grad_norm": 6.751488208770752,
      "learning_rate": 7.443277406447506e-06,
      "loss": 0.3337,
      "step": 42350
    },
    {
      "epoch": 2.891829218387669,
      "grad_norm": 9.69372844696045,
      "learning_rate": 7.215932342108853e-06,
      "loss": 0.3797,
      "step": 42400
    },
    {
      "epoch": 2.895239394352749,
      "grad_norm": 8.41993236541748,
      "learning_rate": 6.988587277770201e-06,
      "loss": 0.4379,
      "step": 42450
    },
    {
      "epoch": 2.8986495703178283,
      "grad_norm": 5.307299613952637,
      "learning_rate": 6.761242213431547e-06,
      "loss": 0.3497,
      "step": 42500
    },
    {
      "epoch": 2.9020597462829083,
      "grad_norm": 11.579049110412598,
      "learning_rate": 6.533897149092893e-06,
      "loss": 0.3525,
      "step": 42550
    },
    {
      "epoch": 2.905469922247988,
      "grad_norm": 2.767430305480957,
      "learning_rate": 6.30655208475424e-06,
      "loss": 0.3511,
      "step": 42600
    },
    {
      "epoch": 2.908880098213068,
      "grad_norm": 4.541929721832275,
      "learning_rate": 6.079207020415587e-06,
      "loss": 0.3938,
      "step": 42650
    },
    {
      "epoch": 2.9122902741781473,
      "grad_norm": 6.68655252456665,
      "learning_rate": 5.851861956076934e-06,
      "loss": 0.4271,
      "step": 42700
    },
    {
      "epoch": 2.9157004501432273,
      "grad_norm": 14.027064323425293,
      "learning_rate": 5.624516891738281e-06,
      "loss": 0.4307,
      "step": 42750
    },
    {
      "epoch": 2.9191106261083073,
      "grad_norm": 4.701550483703613,
      "learning_rate": 5.397171827399628e-06,
      "loss": 0.3943,
      "step": 42800
    },
    {
      "epoch": 2.922520802073387,
      "grad_norm": 8.454629898071289,
      "learning_rate": 5.169826763060974e-06,
      "loss": 0.4176,
      "step": 42850
    },
    {
      "epoch": 2.925930978038467,
      "grad_norm": 2.0434858798980713,
      "learning_rate": 4.942481698722321e-06,
      "loss": 0.2986,
      "step": 42900
    },
    {
      "epoch": 2.9293411540035468,
      "grad_norm": 5.775343894958496,
      "learning_rate": 4.7151366343836675e-06,
      "loss": 0.3422,
      "step": 42950
    },
    {
      "epoch": 2.9327513299686263,
      "grad_norm": 12.91862678527832,
      "learning_rate": 4.487791570045015e-06,
      "loss": 0.3347,
      "step": 43000
    },
    {
      "epoch": 2.9361615059337063,
      "grad_norm": 4.523195266723633,
      "learning_rate": 4.2604465057063615e-06,
      "loss": 0.373,
      "step": 43050
    },
    {
      "epoch": 2.939571681898786,
      "grad_norm": 8.610758781433105,
      "learning_rate": 4.033101441367708e-06,
      "loss": 0.3505,
      "step": 43100
    },
    {
      "epoch": 2.942981857863866,
      "grad_norm": 1.756851315498352,
      "learning_rate": 3.805756377029055e-06,
      "loss": 0.3078,
      "step": 43150
    },
    {
      "epoch": 2.9463920338289453,
      "grad_norm": 8.342292785644531,
      "learning_rate": 3.5784113126904016e-06,
      "loss": 0.3337,
      "step": 43200
    },
    {
      "epoch": 2.9498022097940253,
      "grad_norm": 4.379770278930664,
      "learning_rate": 3.351066248351748e-06,
      "loss": 0.4422,
      "step": 43250
    },
    {
      "epoch": 2.9532123857591053,
      "grad_norm": 4.562490463256836,
      "learning_rate": 3.123721184013095e-06,
      "loss": 0.3665,
      "step": 43300
    },
    {
      "epoch": 2.956622561724185,
      "grad_norm": 8.223601341247559,
      "learning_rate": 2.8963761196744417e-06,
      "loss": 0.3768,
      "step": 43350
    },
    {
      "epoch": 2.960032737689265,
      "grad_norm": 0.2801828682422638,
      "learning_rate": 2.6690310553357887e-06,
      "loss": 0.3221,
      "step": 43400
    },
    {
      "epoch": 2.9634429136543448,
      "grad_norm": 5.111627578735352,
      "learning_rate": 2.4416859909971357e-06,
      "loss": 0.3398,
      "step": 43450
    },
    {
      "epoch": 2.9668530896194243,
      "grad_norm": 2.0084776878356934,
      "learning_rate": 2.2143409266584823e-06,
      "loss": 0.4232,
      "step": 43500
    },
    {
      "epoch": 2.9702632655845043,
      "grad_norm": 3.862727642059326,
      "learning_rate": 1.9869958623198293e-06,
      "loss": 0.4221,
      "step": 43550
    },
    {
      "epoch": 2.973673441549584,
      "grad_norm": 2.5169012546539307,
      "learning_rate": 1.7596507979811759e-06,
      "loss": 0.428,
      "step": 43600
    },
    {
      "epoch": 2.9770836175146638,
      "grad_norm": 7.298698425292969,
      "learning_rate": 1.5323057336425226e-06,
      "loss": 0.4028,
      "step": 43650
    },
    {
      "epoch": 2.9804937934797433,
      "grad_norm": 2.251605272293091,
      "learning_rate": 1.3049606693038694e-06,
      "loss": 0.3877,
      "step": 43700
    },
    {
      "epoch": 2.9839039694448233,
      "grad_norm": 4.230984687805176,
      "learning_rate": 1.0776156049652164e-06,
      "loss": 0.3192,
      "step": 43750
    },
    {
      "epoch": 2.9873141454099033,
      "grad_norm": 7.38530158996582,
      "learning_rate": 8.50270540626563e-07,
      "loss": 0.4441,
      "step": 43800
    },
    {
      "epoch": 2.990724321374983,
      "grad_norm": 5.306605815887451,
      "learning_rate": 6.229254762879098e-07,
      "loss": 0.3427,
      "step": 43850
    },
    {
      "epoch": 2.9941344973400628,
      "grad_norm": 8.556792259216309,
      "learning_rate": 3.9558041194925654e-07,
      "loss": 0.3396,
      "step": 43900
    },
    {
      "epoch": 2.9975446733051427,
      "grad_norm": 2.094953775405884,
      "learning_rate": 1.6823534761060338e-07,
      "loss": 0.34,
      "step": 43950
    },
    {
      "epoch": 3.0,
      "eval_accuracy": 0.8697153379881838,
      "eval_loss": 0.39468786120414734,
      "eval_runtime": 113.8565,
      "eval_samples_per_second": 114.469,
      "eval_steps_per_second": 7.158,
      "step": 43986
    }
  ],
  "logging_steps": 50,
  "max_steps": 43986,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 4.2353763005715416e+16,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
