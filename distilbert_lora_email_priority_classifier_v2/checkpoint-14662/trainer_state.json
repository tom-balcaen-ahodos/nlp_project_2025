{
  "best_global_step": 14662,
  "best_metric": 0.8235248983349958,
  "best_model_checkpoint": "./distilbert_lora_email_priority_classifier_v2\\checkpoint-14662",
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 14662,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0034101759650797983,
      "grad_norm": 8.266585350036621,
      "learning_rate": 0.00019977720183694814,
      "loss": 1.3107,
      "step": 50
    },
    {
      "epoch": 0.006820351930159597,
      "grad_norm": 4.278303146362305,
      "learning_rate": 0.00019954985677260947,
      "loss": 1.1795,
      "step": 100
    },
    {
      "epoch": 0.010230527895239394,
      "grad_norm": 5.746740818023682,
      "learning_rate": 0.00019932251170827082,
      "loss": 1.146,
      "step": 150
    },
    {
      "epoch": 0.013640703860319193,
      "grad_norm": 4.569372653961182,
      "learning_rate": 0.00019909516664393218,
      "loss": 1.0731,
      "step": 200
    },
    {
      "epoch": 0.01705087982539899,
      "grad_norm": 4.441202640533447,
      "learning_rate": 0.0001988678215795935,
      "loss": 0.9402,
      "step": 250
    },
    {
      "epoch": 0.02046105579047879,
      "grad_norm": 7.745151996612549,
      "learning_rate": 0.00019864047651525486,
      "loss": 0.9217,
      "step": 300
    },
    {
      "epoch": 0.023871231755558588,
      "grad_norm": 4.186746597290039,
      "learning_rate": 0.0001984131314509162,
      "loss": 0.8703,
      "step": 350
    },
    {
      "epoch": 0.027281407720638386,
      "grad_norm": 2.582688570022583,
      "learning_rate": 0.00019818578638657757,
      "loss": 0.8524,
      "step": 400
    },
    {
      "epoch": 0.03069158368571818,
      "grad_norm": 3.4113364219665527,
      "learning_rate": 0.0001979584413222389,
      "loss": 0.86,
      "step": 450
    },
    {
      "epoch": 0.03410175965079798,
      "grad_norm": 3.434750556945801,
      "learning_rate": 0.00019773109625790026,
      "loss": 0.8349,
      "step": 500
    },
    {
      "epoch": 0.03751193561587778,
      "grad_norm": 4.017282485961914,
      "learning_rate": 0.0001975037511935616,
      "loss": 0.8313,
      "step": 550
    },
    {
      "epoch": 0.04092211158095758,
      "grad_norm": 5.340073585510254,
      "learning_rate": 0.00019727640612922294,
      "loss": 0.7773,
      "step": 600
    },
    {
      "epoch": 0.04433228754603737,
      "grad_norm": 6.071038722991943,
      "learning_rate": 0.0001970490610648843,
      "loss": 0.7896,
      "step": 650
    },
    {
      "epoch": 0.047742463511117175,
      "grad_norm": 2.45681095123291,
      "learning_rate": 0.00019682171600054563,
      "loss": 0.744,
      "step": 700
    },
    {
      "epoch": 0.05115263947619697,
      "grad_norm": 10.198076248168945,
      "learning_rate": 0.00019659437093620698,
      "loss": 0.8083,
      "step": 750
    },
    {
      "epoch": 0.05456281544127677,
      "grad_norm": 5.621272563934326,
      "learning_rate": 0.0001963670258718683,
      "loss": 0.7523,
      "step": 800
    },
    {
      "epoch": 0.05797299140635657,
      "grad_norm": 4.9213337898254395,
      "learning_rate": 0.00019613968080752967,
      "loss": 0.757,
      "step": 850
    },
    {
      "epoch": 0.06138316737143636,
      "grad_norm": 4.327526092529297,
      "learning_rate": 0.00019591233574319102,
      "loss": 0.7252,
      "step": 900
    },
    {
      "epoch": 0.06479334333651617,
      "grad_norm": 6.664350986480713,
      "learning_rate": 0.00019568499067885238,
      "loss": 0.7303,
      "step": 950
    },
    {
      "epoch": 0.06820351930159596,
      "grad_norm": 10.726407051086426,
      "learning_rate": 0.0001954576456145137,
      "loss": 0.7714,
      "step": 1000
    },
    {
      "epoch": 0.07161369526667576,
      "grad_norm": 4.291638374328613,
      "learning_rate": 0.00019523030055017507,
      "loss": 0.7036,
      "step": 1050
    },
    {
      "epoch": 0.07502387123175557,
      "grad_norm": 6.5926971435546875,
      "learning_rate": 0.0001950029554858364,
      "loss": 0.7046,
      "step": 1100
    },
    {
      "epoch": 0.07843404719683536,
      "grad_norm": 5.379021167755127,
      "learning_rate": 0.00019477561042149778,
      "loss": 0.6959,
      "step": 1150
    },
    {
      "epoch": 0.08184422316191516,
      "grad_norm": 2.6018338203430176,
      "learning_rate": 0.0001945482653571591,
      "loss": 0.6931,
      "step": 1200
    },
    {
      "epoch": 0.08525439912699495,
      "grad_norm": 6.098622798919678,
      "learning_rate": 0.00019432092029282046,
      "loss": 0.8133,
      "step": 1250
    },
    {
      "epoch": 0.08866457509207475,
      "grad_norm": 4.701504230499268,
      "learning_rate": 0.0001940935752284818,
      "loss": 0.7241,
      "step": 1300
    },
    {
      "epoch": 0.09207475105715456,
      "grad_norm": 3.881181240081787,
      "learning_rate": 0.00019386623016414315,
      "loss": 0.6819,
      "step": 1350
    },
    {
      "epoch": 0.09548492702223435,
      "grad_norm": 3.6668457984924316,
      "learning_rate": 0.0001936388850998045,
      "loss": 0.7258,
      "step": 1400
    },
    {
      "epoch": 0.09889510298731415,
      "grad_norm": 5.234337329864502,
      "learning_rate": 0.00019341154003546583,
      "loss": 0.6362,
      "step": 1450
    },
    {
      "epoch": 0.10230527895239394,
      "grad_norm": 3.630810260772705,
      "learning_rate": 0.0001931841949711272,
      "loss": 0.7535,
      "step": 1500
    },
    {
      "epoch": 0.10571545491747374,
      "grad_norm": 4.049806594848633,
      "learning_rate": 0.00019295684990678852,
      "loss": 0.6796,
      "step": 1550
    },
    {
      "epoch": 0.10912563088255355,
      "grad_norm": 2.731646776199341,
      "learning_rate": 0.00019272950484244987,
      "loss": 0.7183,
      "step": 1600
    },
    {
      "epoch": 0.11253580684763334,
      "grad_norm": 4.633825778961182,
      "learning_rate": 0.00019250215977811123,
      "loss": 0.7046,
      "step": 1650
    },
    {
      "epoch": 0.11594598281271314,
      "grad_norm": 2.6853275299072266,
      "learning_rate": 0.00019227481471377258,
      "loss": 0.7178,
      "step": 1700
    },
    {
      "epoch": 0.11935615877779293,
      "grad_norm": 5.189800262451172,
      "learning_rate": 0.0001920474696494339,
      "loss": 0.71,
      "step": 1750
    },
    {
      "epoch": 0.12276633474287273,
      "grad_norm": 3.789625644683838,
      "learning_rate": 0.00019182012458509527,
      "loss": 0.7398,
      "step": 1800
    },
    {
      "epoch": 0.12617651070795252,
      "grad_norm": 3.2206430435180664,
      "learning_rate": 0.0001915927795207566,
      "loss": 0.6261,
      "step": 1850
    },
    {
      "epoch": 0.12958668667303233,
      "grad_norm": 4.1258745193481445,
      "learning_rate": 0.00019136543445641798,
      "loss": 0.7131,
      "step": 1900
    },
    {
      "epoch": 0.13299686263811214,
      "grad_norm": 6.2360076904296875,
      "learning_rate": 0.0001911380893920793,
      "loss": 0.6835,
      "step": 1950
    },
    {
      "epoch": 0.13640703860319192,
      "grad_norm": 3.683366537094116,
      "learning_rate": 0.00019091074432774066,
      "loss": 0.7524,
      "step": 2000
    },
    {
      "epoch": 0.13981721456827173,
      "grad_norm": 4.814393520355225,
      "learning_rate": 0.000190683399263402,
      "loss": 0.6211,
      "step": 2050
    },
    {
      "epoch": 0.1432273905333515,
      "grad_norm": 4.151459693908691,
      "learning_rate": 0.00019045605419906335,
      "loss": 0.6273,
      "step": 2100
    },
    {
      "epoch": 0.14663756649843132,
      "grad_norm": 5.81264066696167,
      "learning_rate": 0.0001902287091347247,
      "loss": 0.7293,
      "step": 2150
    },
    {
      "epoch": 0.15004774246351113,
      "grad_norm": 3.5810301303863525,
      "learning_rate": 0.00019000136407038603,
      "loss": 0.6773,
      "step": 2200
    },
    {
      "epoch": 0.1534579184285909,
      "grad_norm": 4.983246803283691,
      "learning_rate": 0.0001897740190060474,
      "loss": 0.6238,
      "step": 2250
    },
    {
      "epoch": 0.15686809439367072,
      "grad_norm": 7.631110668182373,
      "learning_rate": 0.00018954667394170872,
      "loss": 0.6872,
      "step": 2300
    },
    {
      "epoch": 0.1602782703587505,
      "grad_norm": 7.837310314178467,
      "learning_rate": 0.00018931932887737007,
      "loss": 0.6566,
      "step": 2350
    },
    {
      "epoch": 0.1636884463238303,
      "grad_norm": 4.944979667663574,
      "learning_rate": 0.00018909198381303143,
      "loss": 0.6796,
      "step": 2400
    },
    {
      "epoch": 0.16709862228891012,
      "grad_norm": 5.057923316955566,
      "learning_rate": 0.00018886463874869279,
      "loss": 0.6717,
      "step": 2450
    },
    {
      "epoch": 0.1705087982539899,
      "grad_norm": 4.042693614959717,
      "learning_rate": 0.00018863729368435411,
      "loss": 0.7259,
      "step": 2500
    },
    {
      "epoch": 0.1739189742190697,
      "grad_norm": 4.212404251098633,
      "learning_rate": 0.00018840994862001547,
      "loss": 0.6274,
      "step": 2550
    },
    {
      "epoch": 0.1773291501841495,
      "grad_norm": 9.324618339538574,
      "learning_rate": 0.0001881826035556768,
      "loss": 0.6065,
      "step": 2600
    },
    {
      "epoch": 0.1807393261492293,
      "grad_norm": 3.434833526611328,
      "learning_rate": 0.00018795525849133818,
      "loss": 0.617,
      "step": 2650
    },
    {
      "epoch": 0.1841495021143091,
      "grad_norm": 5.356233596801758,
      "learning_rate": 0.0001877279134269995,
      "loss": 0.6386,
      "step": 2700
    },
    {
      "epoch": 0.1875596780793889,
      "grad_norm": 3.7747814655303955,
      "learning_rate": 0.00018750056836266087,
      "loss": 0.6099,
      "step": 2750
    },
    {
      "epoch": 0.1909698540444687,
      "grad_norm": 8.316987991333008,
      "learning_rate": 0.0001872732232983222,
      "loss": 0.6383,
      "step": 2800
    },
    {
      "epoch": 0.19438003000954848,
      "grad_norm": 2.7060201168060303,
      "learning_rate": 0.00018704587823398355,
      "loss": 0.7242,
      "step": 2850
    },
    {
      "epoch": 0.1977902059746283,
      "grad_norm": 4.933772087097168,
      "learning_rate": 0.0001868185331696449,
      "loss": 0.6392,
      "step": 2900
    },
    {
      "epoch": 0.2012003819397081,
      "grad_norm": 6.200394630432129,
      "learning_rate": 0.00018659118810530624,
      "loss": 0.6324,
      "step": 2950
    },
    {
      "epoch": 0.20461055790478788,
      "grad_norm": 2.947542190551758,
      "learning_rate": 0.0001863638430409676,
      "loss": 0.6469,
      "step": 3000
    },
    {
      "epoch": 0.2080207338698677,
      "grad_norm": 6.412920951843262,
      "learning_rate": 0.00018613649797662892,
      "loss": 0.6531,
      "step": 3050
    },
    {
      "epoch": 0.21143090983494747,
      "grad_norm": 3.877812147140503,
      "learning_rate": 0.00018590915291229028,
      "loss": 0.6554,
      "step": 3100
    },
    {
      "epoch": 0.21484108580002728,
      "grad_norm": 5.62726354598999,
      "learning_rate": 0.00018568180784795163,
      "loss": 0.6049,
      "step": 3150
    },
    {
      "epoch": 0.2182512617651071,
      "grad_norm": 6.729239463806152,
      "learning_rate": 0.000185454462783613,
      "loss": 0.6647,
      "step": 3200
    },
    {
      "epoch": 0.22166143773018687,
      "grad_norm": 4.400230884552002,
      "learning_rate": 0.00018522711771927432,
      "loss": 0.6438,
      "step": 3250
    },
    {
      "epoch": 0.22507161369526668,
      "grad_norm": 3.4407312870025635,
      "learning_rate": 0.00018499977265493567,
      "loss": 0.6271,
      "step": 3300
    },
    {
      "epoch": 0.22848178966034646,
      "grad_norm": 6.966365814208984,
      "learning_rate": 0.000184772427590597,
      "loss": 0.6679,
      "step": 3350
    },
    {
      "epoch": 0.23189196562542627,
      "grad_norm": 4.579921245574951,
      "learning_rate": 0.00018454508252625839,
      "loss": 0.6308,
      "step": 3400
    },
    {
      "epoch": 0.23530214159050608,
      "grad_norm": 2.9670605659484863,
      "learning_rate": 0.00018431773746191971,
      "loss": 0.6168,
      "step": 3450
    },
    {
      "epoch": 0.23871231755558586,
      "grad_norm": 8.35934829711914,
      "learning_rate": 0.00018409039239758107,
      "loss": 0.6039,
      "step": 3500
    },
    {
      "epoch": 0.24212249352066567,
      "grad_norm": 3.2142443656921387,
      "learning_rate": 0.0001838630473332424,
      "loss": 0.6797,
      "step": 3550
    },
    {
      "epoch": 0.24553266948574545,
      "grad_norm": 6.829575061798096,
      "learning_rate": 0.00018363570226890375,
      "loss": 0.6452,
      "step": 3600
    },
    {
      "epoch": 0.24894284545082526,
      "grad_norm": 6.064201354980469,
      "learning_rate": 0.0001834083572045651,
      "loss": 0.6379,
      "step": 3650
    },
    {
      "epoch": 0.25235302141590504,
      "grad_norm": 5.631986618041992,
      "learning_rate": 0.00018318101214022644,
      "loss": 0.5951,
      "step": 3700
    },
    {
      "epoch": 0.25576319738098485,
      "grad_norm": 4.068284511566162,
      "learning_rate": 0.0001829536670758878,
      "loss": 0.5087,
      "step": 3750
    },
    {
      "epoch": 0.25917337334606466,
      "grad_norm": 2.6116394996643066,
      "learning_rate": 0.00018272632201154912,
      "loss": 0.6831,
      "step": 3800
    },
    {
      "epoch": 0.26258354931114447,
      "grad_norm": 3.116823196411133,
      "learning_rate": 0.00018249897694721048,
      "loss": 0.6416,
      "step": 3850
    },
    {
      "epoch": 0.2659937252762243,
      "grad_norm": 4.241364002227783,
      "learning_rate": 0.00018227163188287184,
      "loss": 0.7046,
      "step": 3900
    },
    {
      "epoch": 0.26940390124130403,
      "grad_norm": 5.16800594329834,
      "learning_rate": 0.0001820442868185332,
      "loss": 0.6162,
      "step": 3950
    },
    {
      "epoch": 0.27281407720638384,
      "grad_norm": 3.7442033290863037,
      "learning_rate": 0.00018181694175419452,
      "loss": 0.6132,
      "step": 4000
    },
    {
      "epoch": 0.27622425317146365,
      "grad_norm": 4.777984619140625,
      "learning_rate": 0.00018158959668985588,
      "loss": 0.6625,
      "step": 4050
    },
    {
      "epoch": 0.27963442913654346,
      "grad_norm": 6.9991774559021,
      "learning_rate": 0.0001813622516255172,
      "loss": 0.5744,
      "step": 4100
    },
    {
      "epoch": 0.28304460510162327,
      "grad_norm": 4.016238212585449,
      "learning_rate": 0.00018113490656117856,
      "loss": 0.6306,
      "step": 4150
    },
    {
      "epoch": 0.286454781066703,
      "grad_norm": 1.4679911136627197,
      "learning_rate": 0.00018090756149683992,
      "loss": 0.5938,
      "step": 4200
    },
    {
      "epoch": 0.28986495703178283,
      "grad_norm": 4.716958045959473,
      "learning_rate": 0.00018068021643250125,
      "loss": 0.5921,
      "step": 4250
    },
    {
      "epoch": 0.29327513299686264,
      "grad_norm": 4.382480621337891,
      "learning_rate": 0.0001804528713681626,
      "loss": 0.6395,
      "step": 4300
    },
    {
      "epoch": 0.29668530896194245,
      "grad_norm": 5.0738749504089355,
      "learning_rate": 0.00018022552630382393,
      "loss": 0.5472,
      "step": 4350
    },
    {
      "epoch": 0.30009548492702226,
      "grad_norm": 2.0653560161590576,
      "learning_rate": 0.0001799981812394853,
      "loss": 0.6806,
      "step": 4400
    },
    {
      "epoch": 0.303505660892102,
      "grad_norm": 2.821573495864868,
      "learning_rate": 0.00017977083617514664,
      "loss": 0.557,
      "step": 4450
    },
    {
      "epoch": 0.3069158368571818,
      "grad_norm": 4.685956001281738,
      "learning_rate": 0.000179543491110808,
      "loss": 0.5846,
      "step": 4500
    },
    {
      "epoch": 0.31032601282226163,
      "grad_norm": 4.512872695922852,
      "learning_rate": 0.00017931614604646933,
      "loss": 0.6258,
      "step": 4550
    },
    {
      "epoch": 0.31373618878734144,
      "grad_norm": 5.8909831047058105,
      "learning_rate": 0.00017908880098213068,
      "loss": 0.564,
      "step": 4600
    },
    {
      "epoch": 0.31714636475242125,
      "grad_norm": 4.2698845863342285,
      "learning_rate": 0.00017886145591779204,
      "loss": 0.6721,
      "step": 4650
    },
    {
      "epoch": 0.320556540717501,
      "grad_norm": 6.974838733673096,
      "learning_rate": 0.0001786341108534534,
      "loss": 0.5578,
      "step": 4700
    },
    {
      "epoch": 0.3239667166825808,
      "grad_norm": 2.9359333515167236,
      "learning_rate": 0.00017840676578911472,
      "loss": 0.5834,
      "step": 4750
    },
    {
      "epoch": 0.3273768926476606,
      "grad_norm": 3.4700112342834473,
      "learning_rate": 0.00017817942072477608,
      "loss": 0.4911,
      "step": 4800
    },
    {
      "epoch": 0.33078706861274043,
      "grad_norm": 2.7019097805023193,
      "learning_rate": 0.0001779520756604374,
      "loss": 0.6259,
      "step": 4850
    },
    {
      "epoch": 0.33419724457782024,
      "grad_norm": 2.5879292488098145,
      "learning_rate": 0.00017772473059609876,
      "loss": 0.5993,
      "step": 4900
    },
    {
      "epoch": 0.3376074205429,
      "grad_norm": 4.1245293617248535,
      "learning_rate": 0.00017749738553176012,
      "loss": 0.6342,
      "step": 4950
    },
    {
      "epoch": 0.3410175965079798,
      "grad_norm": 4.16182279586792,
      "learning_rate": 0.00017727004046742145,
      "loss": 0.5669,
      "step": 5000
    },
    {
      "epoch": 0.3444277724730596,
      "grad_norm": 8.300979614257812,
      "learning_rate": 0.0001770426954030828,
      "loss": 0.4635,
      "step": 5050
    },
    {
      "epoch": 0.3478379484381394,
      "grad_norm": 2.022709369659424,
      "learning_rate": 0.00017681535033874413,
      "loss": 0.6406,
      "step": 5100
    },
    {
      "epoch": 0.35124812440321923,
      "grad_norm": 2.8537421226501465,
      "learning_rate": 0.00017658800527440552,
      "loss": 0.5675,
      "step": 5150
    },
    {
      "epoch": 0.354658300368299,
      "grad_norm": 3.2413370609283447,
      "learning_rate": 0.00017636066021006685,
      "loss": 0.5898,
      "step": 5200
    },
    {
      "epoch": 0.3580684763333788,
      "grad_norm": 6.9281816482543945,
      "learning_rate": 0.0001761333151457282,
      "loss": 0.6097,
      "step": 5250
    },
    {
      "epoch": 0.3614786522984586,
      "grad_norm": 2.6264398097991943,
      "learning_rate": 0.00017590597008138953,
      "loss": 0.6163,
      "step": 5300
    },
    {
      "epoch": 0.3648888282635384,
      "grad_norm": 3.346616268157959,
      "learning_rate": 0.00017567862501705089,
      "loss": 0.5978,
      "step": 5350
    },
    {
      "epoch": 0.3682990042286182,
      "grad_norm": 4.676401615142822,
      "learning_rate": 0.00017545127995271224,
      "loss": 0.6303,
      "step": 5400
    },
    {
      "epoch": 0.371709180193698,
      "grad_norm": 4.9054036140441895,
      "learning_rate": 0.0001752239348883736,
      "loss": 0.5327,
      "step": 5450
    },
    {
      "epoch": 0.3751193561587778,
      "grad_norm": 6.702650547027588,
      "learning_rate": 0.00017499658982403493,
      "loss": 0.6811,
      "step": 5500
    },
    {
      "epoch": 0.3785295321238576,
      "grad_norm": 5.693346977233887,
      "learning_rate": 0.00017476924475969628,
      "loss": 0.6901,
      "step": 5550
    },
    {
      "epoch": 0.3819397080889374,
      "grad_norm": 3.1401803493499756,
      "learning_rate": 0.0001745418996953576,
      "loss": 0.5641,
      "step": 5600
    },
    {
      "epoch": 0.3853498840540172,
      "grad_norm": 6.725994110107422,
      "learning_rate": 0.00017431455463101897,
      "loss": 0.5657,
      "step": 5650
    },
    {
      "epoch": 0.38876006001909696,
      "grad_norm": 4.647238731384277,
      "learning_rate": 0.00017408720956668032,
      "loss": 0.6404,
      "step": 5700
    },
    {
      "epoch": 0.3921702359841768,
      "grad_norm": 6.93887186050415,
      "learning_rate": 0.00017385986450234165,
      "loss": 0.6397,
      "step": 5750
    },
    {
      "epoch": 0.3955804119492566,
      "grad_norm": 4.132195949554443,
      "learning_rate": 0.000173632519438003,
      "loss": 0.5397,
      "step": 5800
    },
    {
      "epoch": 0.3989905879143364,
      "grad_norm": 4.308523178100586,
      "learning_rate": 0.00017340517437366434,
      "loss": 0.5549,
      "step": 5850
    },
    {
      "epoch": 0.4024007638794162,
      "grad_norm": 4.303738117218018,
      "learning_rate": 0.00017317782930932572,
      "loss": 0.5797,
      "step": 5900
    },
    {
      "epoch": 0.40581093984449595,
      "grad_norm": 3.0409154891967773,
      "learning_rate": 0.00017295048424498705,
      "loss": 0.7227,
      "step": 5950
    },
    {
      "epoch": 0.40922111580957576,
      "grad_norm": 2.696404457092285,
      "learning_rate": 0.0001727231391806484,
      "loss": 0.4563,
      "step": 6000
    },
    {
      "epoch": 0.4126312917746556,
      "grad_norm": 7.137048721313477,
      "learning_rate": 0.00017249579411630973,
      "loss": 0.489,
      "step": 6050
    },
    {
      "epoch": 0.4160414677397354,
      "grad_norm": 8.420626640319824,
      "learning_rate": 0.0001722684490519711,
      "loss": 0.6329,
      "step": 6100
    },
    {
      "epoch": 0.4194516437048152,
      "grad_norm": 2.6202187538146973,
      "learning_rate": 0.00017204110398763244,
      "loss": 0.6822,
      "step": 6150
    },
    {
      "epoch": 0.42286181966989494,
      "grad_norm": 9.227581024169922,
      "learning_rate": 0.0001718137589232938,
      "loss": 0.5228,
      "step": 6200
    },
    {
      "epoch": 0.42627199563497475,
      "grad_norm": 1.6557345390319824,
      "learning_rate": 0.00017158641385895513,
      "loss": 0.5858,
      "step": 6250
    },
    {
      "epoch": 0.42968217160005456,
      "grad_norm": 3.262540578842163,
      "learning_rate": 0.00017135906879461648,
      "loss": 0.4723,
      "step": 6300
    },
    {
      "epoch": 0.4330923475651344,
      "grad_norm": 6.707336902618408,
      "learning_rate": 0.00017113172373027781,
      "loss": 0.6107,
      "step": 6350
    },
    {
      "epoch": 0.4365025235302142,
      "grad_norm": 9.387163162231445,
      "learning_rate": 0.00017090437866593917,
      "loss": 0.5901,
      "step": 6400
    },
    {
      "epoch": 0.43991269949529394,
      "grad_norm": 2.892360210418701,
      "learning_rate": 0.00017067703360160053,
      "loss": 0.5828,
      "step": 6450
    },
    {
      "epoch": 0.44332287546037374,
      "grad_norm": 3.085547685623169,
      "learning_rate": 0.00017044968853726185,
      "loss": 0.4915,
      "step": 6500
    },
    {
      "epoch": 0.44673305142545355,
      "grad_norm": 3.894258975982666,
      "learning_rate": 0.0001702223434729232,
      "loss": 0.5506,
      "step": 6550
    },
    {
      "epoch": 0.45014322739053336,
      "grad_norm": 1.3758810758590698,
      "learning_rate": 0.00016999499840858454,
      "loss": 0.5532,
      "step": 6600
    },
    {
      "epoch": 0.45355340335561317,
      "grad_norm": 5.8421173095703125,
      "learning_rate": 0.00016976765334424592,
      "loss": 0.6145,
      "step": 6650
    },
    {
      "epoch": 0.4569635793206929,
      "grad_norm": 2.1508877277374268,
      "learning_rate": 0.00016954030827990725,
      "loss": 0.6049,
      "step": 6700
    },
    {
      "epoch": 0.46037375528577273,
      "grad_norm": 1.6894422769546509,
      "learning_rate": 0.0001693129632155686,
      "loss": 0.5186,
      "step": 6750
    },
    {
      "epoch": 0.46378393125085254,
      "grad_norm": 3.097674608230591,
      "learning_rate": 0.00016908561815122994,
      "loss": 0.5236,
      "step": 6800
    },
    {
      "epoch": 0.46719410721593235,
      "grad_norm": 5.993048191070557,
      "learning_rate": 0.0001688582730868913,
      "loss": 0.53,
      "step": 6850
    },
    {
      "epoch": 0.47060428318101216,
      "grad_norm": 4.520483016967773,
      "learning_rate": 0.00016863092802255265,
      "loss": 0.5311,
      "step": 6900
    },
    {
      "epoch": 0.4740144591460919,
      "grad_norm": 6.196635723114014,
      "learning_rate": 0.000168403582958214,
      "loss": 0.5267,
      "step": 6950
    },
    {
      "epoch": 0.4774246351111717,
      "grad_norm": 8.493480682373047,
      "learning_rate": 0.00016817623789387533,
      "loss": 0.5832,
      "step": 7000
    },
    {
      "epoch": 0.48083481107625153,
      "grad_norm": 3.5250840187072754,
      "learning_rate": 0.0001679488928295367,
      "loss": 0.5679,
      "step": 7050
    },
    {
      "epoch": 0.48424498704133134,
      "grad_norm": 6.400791645050049,
      "learning_rate": 0.00016772154776519802,
      "loss": 0.6053,
      "step": 7100
    },
    {
      "epoch": 0.48765516300641115,
      "grad_norm": 5.314202785491943,
      "learning_rate": 0.00016749420270085937,
      "loss": 0.5184,
      "step": 7150
    },
    {
      "epoch": 0.4910653389714909,
      "grad_norm": 5.697413921356201,
      "learning_rate": 0.00016726685763652073,
      "loss": 0.5078,
      "step": 7200
    },
    {
      "epoch": 0.4944755149365707,
      "grad_norm": 2.152207612991333,
      "learning_rate": 0.00016703951257218206,
      "loss": 0.4559,
      "step": 7250
    },
    {
      "epoch": 0.4978856909016505,
      "grad_norm": 1.1217572689056396,
      "learning_rate": 0.0001668121675078434,
      "loss": 0.5594,
      "step": 7300
    },
    {
      "epoch": 0.5012958668667303,
      "grad_norm": 9.571451187133789,
      "learning_rate": 0.00016658482244350474,
      "loss": 0.6012,
      "step": 7350
    },
    {
      "epoch": 0.5047060428318101,
      "grad_norm": 1.8719607591629028,
      "learning_rate": 0.00016635747737916612,
      "loss": 0.5044,
      "step": 7400
    },
    {
      "epoch": 0.50811621879689,
      "grad_norm": 4.435731887817383,
      "learning_rate": 0.00016613013231482745,
      "loss": 0.6225,
      "step": 7450
    },
    {
      "epoch": 0.5115263947619697,
      "grad_norm": 2.373957872390747,
      "learning_rate": 0.0001659027872504888,
      "loss": 0.5815,
      "step": 7500
    },
    {
      "epoch": 0.5149365707270496,
      "grad_norm": 7.570865631103516,
      "learning_rate": 0.00016567544218615014,
      "loss": 0.618,
      "step": 7550
    },
    {
      "epoch": 0.5183467466921293,
      "grad_norm": 1.6939805746078491,
      "learning_rate": 0.0001654480971218115,
      "loss": 0.5299,
      "step": 7600
    },
    {
      "epoch": 0.5217569226572091,
      "grad_norm": 3.359837532043457,
      "learning_rate": 0.00016522075205747285,
      "loss": 0.6406,
      "step": 7650
    },
    {
      "epoch": 0.5251670986222889,
      "grad_norm": 6.281620502471924,
      "learning_rate": 0.00016499340699313418,
      "loss": 0.5339,
      "step": 7700
    },
    {
      "epoch": 0.5285772745873687,
      "grad_norm": 4.283625602722168,
      "learning_rate": 0.00016476606192879553,
      "loss": 0.4876,
      "step": 7750
    },
    {
      "epoch": 0.5319874505524486,
      "grad_norm": 1.9548975229263306,
      "learning_rate": 0.00016453871686445686,
      "loss": 0.5188,
      "step": 7800
    },
    {
      "epoch": 0.5353976265175283,
      "grad_norm": 5.0062079429626465,
      "learning_rate": 0.00016431137180011822,
      "loss": 0.6067,
      "step": 7850
    },
    {
      "epoch": 0.5388078024826081,
      "grad_norm": 3.0621142387390137,
      "learning_rate": 0.00016408402673577958,
      "loss": 0.5412,
      "step": 7900
    },
    {
      "epoch": 0.5422179784476879,
      "grad_norm": 1.0248709917068481,
      "learning_rate": 0.00016385668167144093,
      "loss": 0.4478,
      "step": 7950
    },
    {
      "epoch": 0.5456281544127677,
      "grad_norm": 5.944315433502197,
      "learning_rate": 0.00016362933660710226,
      "loss": 0.56,
      "step": 8000
    },
    {
      "epoch": 0.5490383303778475,
      "grad_norm": 4.7076311111450195,
      "learning_rate": 0.00016340199154276362,
      "loss": 0.442,
      "step": 8050
    },
    {
      "epoch": 0.5524485063429273,
      "grad_norm": 2.183709144592285,
      "learning_rate": 0.00016317464647842494,
      "loss": 0.5077,
      "step": 8100
    },
    {
      "epoch": 0.5558586823080071,
      "grad_norm": 4.422790050506592,
      "learning_rate": 0.00016294730141408633,
      "loss": 0.5276,
      "step": 8150
    },
    {
      "epoch": 0.5592688582730869,
      "grad_norm": 3.8972325325012207,
      "learning_rate": 0.00016271995634974766,
      "loss": 0.5599,
      "step": 8200
    },
    {
      "epoch": 0.5626790342381667,
      "grad_norm": 1.8844224214553833,
      "learning_rate": 0.000162492611285409,
      "loss": 0.574,
      "step": 8250
    },
    {
      "epoch": 0.5660892102032465,
      "grad_norm": 5.736959457397461,
      "learning_rate": 0.00016226526622107034,
      "loss": 0.5541,
      "step": 8300
    },
    {
      "epoch": 0.5694993861683263,
      "grad_norm": 1.2332019805908203,
      "learning_rate": 0.0001620379211567317,
      "loss": 0.6469,
      "step": 8350
    },
    {
      "epoch": 0.572909562133406,
      "grad_norm": 4.569895267486572,
      "learning_rate": 0.00016181057609239305,
      "loss": 0.5188,
      "step": 8400
    },
    {
      "epoch": 0.5763197380984859,
      "grad_norm": 2.686091184616089,
      "learning_rate": 0.00016158323102805438,
      "loss": 0.5934,
      "step": 8450
    },
    {
      "epoch": 0.5797299140635657,
      "grad_norm": 3.0451066493988037,
      "learning_rate": 0.00016135588596371574,
      "loss": 0.4826,
      "step": 8500
    },
    {
      "epoch": 0.5831400900286455,
      "grad_norm": 6.963432312011719,
      "learning_rate": 0.00016112854089937707,
      "loss": 0.5399,
      "step": 8550
    },
    {
      "epoch": 0.5865502659937253,
      "grad_norm": 3.1497836112976074,
      "learning_rate": 0.00016090119583503842,
      "loss": 0.6517,
      "step": 8600
    },
    {
      "epoch": 0.589960441958805,
      "grad_norm": 1.3391757011413574,
      "learning_rate": 0.00016067385077069978,
      "loss": 0.5299,
      "step": 8650
    },
    {
      "epoch": 0.5933706179238849,
      "grad_norm": 3.4283430576324463,
      "learning_rate": 0.00016044650570636113,
      "loss": 0.5935,
      "step": 8700
    },
    {
      "epoch": 0.5967807938889647,
      "grad_norm": 4.608673095703125,
      "learning_rate": 0.00016021916064202246,
      "loss": 0.5255,
      "step": 8750
    },
    {
      "epoch": 0.6001909698540445,
      "grad_norm": 6.921573162078857,
      "learning_rate": 0.00015999181557768382,
      "loss": 0.5284,
      "step": 8800
    },
    {
      "epoch": 0.6036011458191243,
      "grad_norm": 2.657743215560913,
      "learning_rate": 0.00015976447051334515,
      "loss": 0.4705,
      "step": 8850
    },
    {
      "epoch": 0.607011321784204,
      "grad_norm": 4.977388381958008,
      "learning_rate": 0.00015953712544900653,
      "loss": 0.5665,
      "step": 8900
    },
    {
      "epoch": 0.6104214977492839,
      "grad_norm": 6.316170692443848,
      "learning_rate": 0.00015930978038466786,
      "loss": 0.5412,
      "step": 8950
    },
    {
      "epoch": 0.6138316737143636,
      "grad_norm": 2.70988392829895,
      "learning_rate": 0.00015908243532032921,
      "loss": 0.5036,
      "step": 9000
    },
    {
      "epoch": 0.6172418496794435,
      "grad_norm": 6.782095432281494,
      "learning_rate": 0.00015885509025599054,
      "loss": 0.5581,
      "step": 9050
    },
    {
      "epoch": 0.6206520256445233,
      "grad_norm": 2.898824691772461,
      "learning_rate": 0.0001586277451916519,
      "loss": 0.5784,
      "step": 9100
    },
    {
      "epoch": 0.624062201609603,
      "grad_norm": 3.41677188873291,
      "learning_rate": 0.00015840040012731326,
      "loss": 0.5738,
      "step": 9150
    },
    {
      "epoch": 0.6274723775746829,
      "grad_norm": 8.091300964355469,
      "learning_rate": 0.00015817305506297458,
      "loss": 0.5708,
      "step": 9200
    },
    {
      "epoch": 0.6308825535397626,
      "grad_norm": 1.4837385416030884,
      "learning_rate": 0.00015794570999863594,
      "loss": 0.488,
      "step": 9250
    },
    {
      "epoch": 0.6342927295048425,
      "grad_norm": 1.3815741539001465,
      "learning_rate": 0.00015771836493429727,
      "loss": 0.549,
      "step": 9300
    },
    {
      "epoch": 0.6377029054699223,
      "grad_norm": 4.3227057456970215,
      "learning_rate": 0.00015749101986995862,
      "loss": 0.5048,
      "step": 9350
    },
    {
      "epoch": 0.641113081435002,
      "grad_norm": 3.8489980697631836,
      "learning_rate": 0.00015726367480561998,
      "loss": 0.5588,
      "step": 9400
    },
    {
      "epoch": 0.6445232574000819,
      "grad_norm": 7.162261962890625,
      "learning_rate": 0.00015703632974128134,
      "loss": 0.5237,
      "step": 9450
    },
    {
      "epoch": 0.6479334333651616,
      "grad_norm": 2.872513771057129,
      "learning_rate": 0.00015680898467694267,
      "loss": 0.5376,
      "step": 9500
    },
    {
      "epoch": 0.6513436093302415,
      "grad_norm": 5.652125358581543,
      "learning_rate": 0.00015658163961260402,
      "loss": 0.6202,
      "step": 9550
    },
    {
      "epoch": 0.6547537852953212,
      "grad_norm": 4.448047161102295,
      "learning_rate": 0.00015635429454826535,
      "loss": 0.5201,
      "step": 9600
    },
    {
      "epoch": 0.658163961260401,
      "grad_norm": 5.281510353088379,
      "learning_rate": 0.00015612694948392673,
      "loss": 0.4918,
      "step": 9650
    },
    {
      "epoch": 0.6615741372254809,
      "grad_norm": 6.330154895782471,
      "learning_rate": 0.00015589960441958806,
      "loss": 0.5564,
      "step": 9700
    },
    {
      "epoch": 0.6649843131905606,
      "grad_norm": 3.0484023094177246,
      "learning_rate": 0.00015567225935524942,
      "loss": 0.5096,
      "step": 9750
    },
    {
      "epoch": 0.6683944891556405,
      "grad_norm": 2.5767197608947754,
      "learning_rate": 0.00015544491429091075,
      "loss": 0.5072,
      "step": 9800
    },
    {
      "epoch": 0.6718046651207202,
      "grad_norm": 7.161823749542236,
      "learning_rate": 0.0001552175692265721,
      "loss": 0.5541,
      "step": 9850
    },
    {
      "epoch": 0.6752148410858,
      "grad_norm": 3.4963274002075195,
      "learning_rate": 0.00015499022416223346,
      "loss": 0.492,
      "step": 9900
    },
    {
      "epoch": 0.6786250170508799,
      "grad_norm": 9.90247631072998,
      "learning_rate": 0.0001547628790978948,
      "loss": 0.5493,
      "step": 9950
    },
    {
      "epoch": 0.6820351930159596,
      "grad_norm": 3.732794761657715,
      "learning_rate": 0.00015453553403355614,
      "loss": 0.5105,
      "step": 10000
    },
    {
      "epoch": 0.6854453689810395,
      "grad_norm": 3.563112735748291,
      "learning_rate": 0.00015430818896921747,
      "loss": 0.4928,
      "step": 10050
    },
    {
      "epoch": 0.6888555449461192,
      "grad_norm": 6.230532646179199,
      "learning_rate": 0.00015408084390487883,
      "loss": 0.5391,
      "step": 10100
    },
    {
      "epoch": 0.692265720911199,
      "grad_norm": 4.717284679412842,
      "learning_rate": 0.00015385349884054018,
      "loss": 0.6076,
      "step": 10150
    },
    {
      "epoch": 0.6956758968762788,
      "grad_norm": 1.5988492965698242,
      "learning_rate": 0.00015362615377620154,
      "loss": 0.579,
      "step": 10200
    },
    {
      "epoch": 0.6990860728413586,
      "grad_norm": 10.493505477905273,
      "learning_rate": 0.00015339880871186287,
      "loss": 0.4947,
      "step": 10250
    },
    {
      "epoch": 0.7024962488064385,
      "grad_norm": 4.584412097930908,
      "learning_rate": 0.00015317146364752422,
      "loss": 0.54,
      "step": 10300
    },
    {
      "epoch": 0.7059064247715182,
      "grad_norm": 3.211653709411621,
      "learning_rate": 0.00015294411858318555,
      "loss": 0.5456,
      "step": 10350
    },
    {
      "epoch": 0.709316600736598,
      "grad_norm": 3.413649559020996,
      "learning_rate": 0.00015271677351884694,
      "loss": 0.5011,
      "step": 10400
    },
    {
      "epoch": 0.7127267767016778,
      "grad_norm": 3.0088231563568115,
      "learning_rate": 0.00015248942845450826,
      "loss": 0.5831,
      "step": 10450
    },
    {
      "epoch": 0.7161369526667576,
      "grad_norm": 2.479114055633545,
      "learning_rate": 0.00015226208339016962,
      "loss": 0.5953,
      "step": 10500
    },
    {
      "epoch": 0.7195471286318375,
      "grad_norm": 3.7112340927124023,
      "learning_rate": 0.00015203473832583095,
      "loss": 0.4711,
      "step": 10550
    },
    {
      "epoch": 0.7229573045969172,
      "grad_norm": 3.3521804809570312,
      "learning_rate": 0.0001518073932614923,
      "loss": 0.4914,
      "step": 10600
    },
    {
      "epoch": 0.726367480561997,
      "grad_norm": 6.322518825531006,
      "learning_rate": 0.00015158004819715366,
      "loss": 0.5151,
      "step": 10650
    },
    {
      "epoch": 0.7297776565270768,
      "grad_norm": 5.79115104675293,
      "learning_rate": 0.000151352703132815,
      "loss": 0.5446,
      "step": 10700
    },
    {
      "epoch": 0.7331878324921566,
      "grad_norm": 9.393169403076172,
      "learning_rate": 0.00015112535806847635,
      "loss": 0.5594,
      "step": 10750
    },
    {
      "epoch": 0.7365980084572364,
      "grad_norm": 4.9113593101501465,
      "learning_rate": 0.00015089801300413767,
      "loss": 0.5369,
      "step": 10800
    },
    {
      "epoch": 0.7400081844223162,
      "grad_norm": 2.066735029220581,
      "learning_rate": 0.00015067066793979903,
      "loss": 0.5099,
      "step": 10850
    },
    {
      "epoch": 0.743418360387396,
      "grad_norm": 7.562488079071045,
      "learning_rate": 0.00015044332287546039,
      "loss": 0.5175,
      "step": 10900
    },
    {
      "epoch": 0.7468285363524758,
      "grad_norm": 3.956282377243042,
      "learning_rate": 0.00015021597781112174,
      "loss": 0.5777,
      "step": 10950
    },
    {
      "epoch": 0.7502387123175556,
      "grad_norm": 5.102017402648926,
      "learning_rate": 0.00014998863274678307,
      "loss": 0.6558,
      "step": 11000
    },
    {
      "epoch": 0.7536488882826354,
      "grad_norm": 1.9244688749313354,
      "learning_rate": 0.00014976128768244443,
      "loss": 0.4711,
      "step": 11050
    },
    {
      "epoch": 0.7570590642477152,
      "grad_norm": 4.8804097175598145,
      "learning_rate": 0.00014953394261810576,
      "loss": 0.5238,
      "step": 11100
    },
    {
      "epoch": 0.7604692402127949,
      "grad_norm": 3.57965350151062,
      "learning_rate": 0.0001493065975537671,
      "loss": 0.5541,
      "step": 11150
    },
    {
      "epoch": 0.7638794161778748,
      "grad_norm": 6.542465686798096,
      "learning_rate": 0.00014907925248942847,
      "loss": 0.5345,
      "step": 11200
    },
    {
      "epoch": 0.7672895921429546,
      "grad_norm": 3.4771485328674316,
      "learning_rate": 0.0001488519074250898,
      "loss": 0.5305,
      "step": 11250
    },
    {
      "epoch": 0.7706997681080344,
      "grad_norm": 3.2458817958831787,
      "learning_rate": 0.00014862456236075115,
      "loss": 0.5165,
      "step": 11300
    },
    {
      "epoch": 0.7741099440731142,
      "grad_norm": 2.2685558795928955,
      "learning_rate": 0.00014839721729641248,
      "loss": 0.4696,
      "step": 11350
    },
    {
      "epoch": 0.7775201200381939,
      "grad_norm": 8.451598167419434,
      "learning_rate": 0.00014816987223207386,
      "loss": 0.5592,
      "step": 11400
    },
    {
      "epoch": 0.7809302960032738,
      "grad_norm": 7.53580379486084,
      "learning_rate": 0.0001479425271677352,
      "loss": 0.496,
      "step": 11450
    },
    {
      "epoch": 0.7843404719683535,
      "grad_norm": 7.037713050842285,
      "learning_rate": 0.00014771518210339655,
      "loss": 0.5856,
      "step": 11500
    },
    {
      "epoch": 0.7877506479334334,
      "grad_norm": 3.354215621948242,
      "learning_rate": 0.00014748783703905788,
      "loss": 0.5214,
      "step": 11550
    },
    {
      "epoch": 0.7911608238985132,
      "grad_norm": 6.031603813171387,
      "learning_rate": 0.00014726049197471923,
      "loss": 0.5656,
      "step": 11600
    },
    {
      "epoch": 0.7945709998635929,
      "grad_norm": 2.9881956577301025,
      "learning_rate": 0.0001470331469103806,
      "loss": 0.4745,
      "step": 11650
    },
    {
      "epoch": 0.7979811758286728,
      "grad_norm": 3.825817108154297,
      "learning_rate": 0.00014680580184604194,
      "loss": 0.5411,
      "step": 11700
    },
    {
      "epoch": 0.8013913517937525,
      "grad_norm": 4.622636318206787,
      "learning_rate": 0.00014657845678170327,
      "loss": 0.5117,
      "step": 11750
    },
    {
      "epoch": 0.8048015277588324,
      "grad_norm": 1.8534528017044067,
      "learning_rate": 0.00014635111171736463,
      "loss": 0.5018,
      "step": 11800
    },
    {
      "epoch": 0.8082117037239122,
      "grad_norm": 1.8155694007873535,
      "learning_rate": 0.00014612376665302596,
      "loss": 0.4165,
      "step": 11850
    },
    {
      "epoch": 0.8116218796889919,
      "grad_norm": 2.4805243015289307,
      "learning_rate": 0.00014589642158868731,
      "loss": 0.4896,
      "step": 11900
    },
    {
      "epoch": 0.8150320556540718,
      "grad_norm": 3.8450238704681396,
      "learning_rate": 0.00014566907652434867,
      "loss": 0.5417,
      "step": 11950
    },
    {
      "epoch": 0.8184422316191515,
      "grad_norm": 5.4871673583984375,
      "learning_rate": 0.00014544173146001,
      "loss": 0.571,
      "step": 12000
    },
    {
      "epoch": 0.8218524075842314,
      "grad_norm": 4.1293625831604,
      "learning_rate": 0.00014521438639567135,
      "loss": 0.4732,
      "step": 12050
    },
    {
      "epoch": 0.8252625835493111,
      "grad_norm": 1.4948612451553345,
      "learning_rate": 0.00014498704133133268,
      "loss": 0.5644,
      "step": 12100
    },
    {
      "epoch": 0.8286727595143909,
      "grad_norm": 2.7908928394317627,
      "learning_rate": 0.00014475969626699407,
      "loss": 0.4763,
      "step": 12150
    },
    {
      "epoch": 0.8320829354794708,
      "grad_norm": 4.504905700683594,
      "learning_rate": 0.0001445323512026554,
      "loss": 0.4813,
      "step": 12200
    },
    {
      "epoch": 0.8354931114445505,
      "grad_norm": 6.458579063415527,
      "learning_rate": 0.00014430500613831675,
      "loss": 0.4842,
      "step": 12250
    },
    {
      "epoch": 0.8389032874096304,
      "grad_norm": 3.4847285747528076,
      "learning_rate": 0.00014407766107397808,
      "loss": 0.6585,
      "step": 12300
    },
    {
      "epoch": 0.8423134633747101,
      "grad_norm": 1.967301845550537,
      "learning_rate": 0.00014385031600963944,
      "loss": 0.4965,
      "step": 12350
    },
    {
      "epoch": 0.8457236393397899,
      "grad_norm": 3.2897496223449707,
      "learning_rate": 0.0001436229709453008,
      "loss": 0.506,
      "step": 12400
    },
    {
      "epoch": 0.8491338153048698,
      "grad_norm": 1.6241425275802612,
      "learning_rate": 0.00014339562588096215,
      "loss": 0.4738,
      "step": 12450
    },
    {
      "epoch": 0.8525439912699495,
      "grad_norm": 1.4080233573913574,
      "learning_rate": 0.00014316828081662348,
      "loss": 0.4185,
      "step": 12500
    },
    {
      "epoch": 0.8559541672350294,
      "grad_norm": 1.3446654081344604,
      "learning_rate": 0.00014294093575228483,
      "loss": 0.4199,
      "step": 12550
    },
    {
      "epoch": 0.8593643432001091,
      "grad_norm": 7.100772380828857,
      "learning_rate": 0.00014271359068794616,
      "loss": 0.573,
      "step": 12600
    },
    {
      "epoch": 0.8627745191651889,
      "grad_norm": 1.595511794090271,
      "learning_rate": 0.00014248624562360752,
      "loss": 0.471,
      "step": 12650
    },
    {
      "epoch": 0.8661846951302687,
      "grad_norm": 6.136267185211182,
      "learning_rate": 0.00014225890055926887,
      "loss": 0.5941,
      "step": 12700
    },
    {
      "epoch": 0.8695948710953485,
      "grad_norm": 4.194272518157959,
      "learning_rate": 0.0001420315554949302,
      "loss": 0.4732,
      "step": 12750
    },
    {
      "epoch": 0.8730050470604284,
      "grad_norm": 3.598219394683838,
      "learning_rate": 0.00014180421043059156,
      "loss": 0.5949,
      "step": 12800
    },
    {
      "epoch": 0.8764152230255081,
      "grad_norm": 2.889256238937378,
      "learning_rate": 0.00014157686536625289,
      "loss": 0.5763,
      "step": 12850
    },
    {
      "epoch": 0.8798253989905879,
      "grad_norm": 6.05254602432251,
      "learning_rate": 0.00014134952030191427,
      "loss": 0.4428,
      "step": 12900
    },
    {
      "epoch": 0.8832355749556677,
      "grad_norm": 2.8352386951446533,
      "learning_rate": 0.0001411221752375756,
      "loss": 0.5287,
      "step": 12950
    },
    {
      "epoch": 0.8866457509207475,
      "grad_norm": 6.208887577056885,
      "learning_rate": 0.00014089483017323695,
      "loss": 0.4819,
      "step": 13000
    },
    {
      "epoch": 0.8900559268858274,
      "grad_norm": 4.236801624298096,
      "learning_rate": 0.00014066748510889828,
      "loss": 0.4475,
      "step": 13050
    },
    {
      "epoch": 0.8934661028509071,
      "grad_norm": 4.340880870819092,
      "learning_rate": 0.00014044014004455964,
      "loss": 0.4552,
      "step": 13100
    },
    {
      "epoch": 0.8968762788159869,
      "grad_norm": 1.1489304304122925,
      "learning_rate": 0.000140212794980221,
      "loss": 0.5364,
      "step": 13150
    },
    {
      "epoch": 0.9002864547810667,
      "grad_norm": 3.3975205421447754,
      "learning_rate": 0.00013998544991588235,
      "loss": 0.4886,
      "step": 13200
    },
    {
      "epoch": 0.9036966307461465,
      "grad_norm": 3.8280599117279053,
      "learning_rate": 0.00013975810485154368,
      "loss": 0.4998,
      "step": 13250
    },
    {
      "epoch": 0.9071068067112263,
      "grad_norm": 4.130757808685303,
      "learning_rate": 0.00013953075978720504,
      "loss": 0.5086,
      "step": 13300
    },
    {
      "epoch": 0.9105169826763061,
      "grad_norm": 6.123864650726318,
      "learning_rate": 0.00013930341472286636,
      "loss": 0.5188,
      "step": 13350
    },
    {
      "epoch": 0.9139271586413859,
      "grad_norm": 2.714372396469116,
      "learning_rate": 0.00013907606965852772,
      "loss": 0.4832,
      "step": 13400
    },
    {
      "epoch": 0.9173373346064657,
      "grad_norm": 4.956056118011475,
      "learning_rate": 0.00013884872459418908,
      "loss": 0.5499,
      "step": 13450
    },
    {
      "epoch": 0.9207475105715455,
      "grad_norm": 5.8628034591674805,
      "learning_rate": 0.0001386213795298504,
      "loss": 0.4388,
      "step": 13500
    },
    {
      "epoch": 0.9241576865366253,
      "grad_norm": 3.811239242553711,
      "learning_rate": 0.00013839403446551176,
      "loss": 0.6102,
      "step": 13550
    },
    {
      "epoch": 0.9275678625017051,
      "grad_norm": 3.0883169174194336,
      "learning_rate": 0.0001381666894011731,
      "loss": 0.4952,
      "step": 13600
    },
    {
      "epoch": 0.9309780384667848,
      "grad_norm": 7.053252220153809,
      "learning_rate": 0.00013793934433683447,
      "loss": 0.5219,
      "step": 13650
    },
    {
      "epoch": 0.9343882144318647,
      "grad_norm": 3.4296107292175293,
      "learning_rate": 0.0001377119992724958,
      "loss": 0.5457,
      "step": 13700
    },
    {
      "epoch": 0.9377983903969445,
      "grad_norm": 1.939153790473938,
      "learning_rate": 0.00013748465420815716,
      "loss": 0.4683,
      "step": 13750
    },
    {
      "epoch": 0.9412085663620243,
      "grad_norm": 5.600878715515137,
      "learning_rate": 0.00013725730914381849,
      "loss": 0.3848,
      "step": 13800
    },
    {
      "epoch": 0.9446187423271041,
      "grad_norm": 0.530109167098999,
      "learning_rate": 0.00013702996407947984,
      "loss": 0.4993,
      "step": 13850
    },
    {
      "epoch": 0.9480289182921838,
      "grad_norm": 1.522309422492981,
      "learning_rate": 0.0001368026190151412,
      "loss": 0.5908,
      "step": 13900
    },
    {
      "epoch": 0.9514390942572637,
      "grad_norm": 2.523498058319092,
      "learning_rate": 0.00013657527395080255,
      "loss": 0.5064,
      "step": 13950
    },
    {
      "epoch": 0.9548492702223434,
      "grad_norm": 4.693229675292969,
      "learning_rate": 0.00013634792888646388,
      "loss": 0.5105,
      "step": 14000
    },
    {
      "epoch": 0.9582594461874233,
      "grad_norm": 4.601716041564941,
      "learning_rate": 0.00013612058382212524,
      "loss": 0.5015,
      "step": 14050
    },
    {
      "epoch": 0.9616696221525031,
      "grad_norm": 3.3283212184906006,
      "learning_rate": 0.00013589323875778657,
      "loss": 0.5062,
      "step": 14100
    },
    {
      "epoch": 0.9650797981175828,
      "grad_norm": 8.203328132629395,
      "learning_rate": 0.00013566589369344792,
      "loss": 0.536,
      "step": 14150
    },
    {
      "epoch": 0.9684899740826627,
      "grad_norm": 8.823223114013672,
      "learning_rate": 0.00013543854862910928,
      "loss": 0.4616,
      "step": 14200
    },
    {
      "epoch": 0.9719001500477424,
      "grad_norm": 1.8540843725204468,
      "learning_rate": 0.0001352112035647706,
      "loss": 0.5343,
      "step": 14250
    },
    {
      "epoch": 0.9753103260128223,
      "grad_norm": 3.9538235664367676,
      "learning_rate": 0.00013498385850043196,
      "loss": 0.5839,
      "step": 14300
    },
    {
      "epoch": 0.9787205019779021,
      "grad_norm": 6.376593112945557,
      "learning_rate": 0.0001347565134360933,
      "loss": 0.5146,
      "step": 14350
    },
    {
      "epoch": 0.9821306779429818,
      "grad_norm": 4.5149922370910645,
      "learning_rate": 0.00013452916837175467,
      "loss": 0.4567,
      "step": 14400
    },
    {
      "epoch": 0.9855408539080617,
      "grad_norm": 6.716584205627441,
      "learning_rate": 0.000134301823307416,
      "loss": 0.45,
      "step": 14450
    },
    {
      "epoch": 0.9889510298731414,
      "grad_norm": 5.664109230041504,
      "learning_rate": 0.00013407447824307736,
      "loss": 0.5194,
      "step": 14500
    },
    {
      "epoch": 0.9923612058382213,
      "grad_norm": 7.993327617645264,
      "learning_rate": 0.0001338471331787387,
      "loss": 0.4511,
      "step": 14550
    },
    {
      "epoch": 0.995771381803301,
      "grad_norm": 6.084619045257568,
      "learning_rate": 0.00013361978811440004,
      "loss": 0.584,
      "step": 14600
    },
    {
      "epoch": 0.9991815577683808,
      "grad_norm": 2.418008327484131,
      "learning_rate": 0.0001333924430500614,
      "loss": 0.4792,
      "step": 14650
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.8235248983349958,
      "eval_loss": 0.4742952585220337,
      "eval_runtime": 114.2809,
      "eval_samples_per_second": 114.044,
      "eval_steps_per_second": 7.132,
      "step": 14662
    }
  ],
  "logging_steps": 50,
  "max_steps": 43986,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.4116579982939904e+16,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
