{
  "best_global_step": 14662,
  "best_metric": 0.81930484155605,
  "best_model_checkpoint": "./distilbert_lora_email_priority_classifier_1\\checkpoint-14662",
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 14662,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0034101759650797983,
      "grad_norm": 4.650956630706787,
      "learning_rate": 0.00019933160551084437,
      "loss": 1.3834,
      "step": 50
    },
    {
      "epoch": 0.006820351930159597,
      "grad_norm": 4.924127101898193,
      "learning_rate": 0.0001986495703178284,
      "loss": 1.1241,
      "step": 100
    },
    {
      "epoch": 0.010230527895239394,
      "grad_norm": 2.728846788406372,
      "learning_rate": 0.00019796753512481245,
      "loss": 1.0605,
      "step": 150
    },
    {
      "epoch": 0.013640703860319193,
      "grad_norm": 3.4091343879699707,
      "learning_rate": 0.0001972854999317965,
      "loss": 1.0107,
      "step": 200
    },
    {
      "epoch": 0.01705087982539899,
      "grad_norm": 4.971828460693359,
      "learning_rate": 0.00019660346473878053,
      "loss": 0.961,
      "step": 250
    },
    {
      "epoch": 0.02046105579047879,
      "grad_norm": 9.292251586914062,
      "learning_rate": 0.00019592142954576457,
      "loss": 0.8935,
      "step": 300
    },
    {
      "epoch": 0.023871231755558588,
      "grad_norm": 3.8223965167999268,
      "learning_rate": 0.0001952393943527486,
      "loss": 0.8969,
      "step": 350
    },
    {
      "epoch": 0.027281407720638386,
      "grad_norm": 3.904846429824829,
      "learning_rate": 0.00019455735915973265,
      "loss": 0.8973,
      "step": 400
    },
    {
      "epoch": 0.03069158368571818,
      "grad_norm": 2.692168951034546,
      "learning_rate": 0.0001938753239667167,
      "loss": 0.8124,
      "step": 450
    },
    {
      "epoch": 0.03410175965079798,
      "grad_norm": 3.953627109527588,
      "learning_rate": 0.00019319328877370073,
      "loss": 0.8439,
      "step": 500
    },
    {
      "epoch": 0.03751193561587778,
      "grad_norm": 5.193812370300293,
      "learning_rate": 0.00019251125358068477,
      "loss": 0.8236,
      "step": 550
    },
    {
      "epoch": 0.04092211158095758,
      "grad_norm": 4.604742527008057,
      "learning_rate": 0.0001918292183876688,
      "loss": 0.7361,
      "step": 600
    },
    {
      "epoch": 0.04433228754603737,
      "grad_norm": 4.4189372062683105,
      "learning_rate": 0.00019114718319465285,
      "loss": 0.8137,
      "step": 650
    },
    {
      "epoch": 0.047742463511117175,
      "grad_norm": 6.032752513885498,
      "learning_rate": 0.0001904651480016369,
      "loss": 0.7133,
      "step": 700
    },
    {
      "epoch": 0.05115263947619697,
      "grad_norm": 3.4651100635528564,
      "learning_rate": 0.00018978311280862093,
      "loss": 0.7696,
      "step": 750
    },
    {
      "epoch": 0.05456281544127677,
      "grad_norm": 4.601372718811035,
      "learning_rate": 0.00018910107761560497,
      "loss": 0.7233,
      "step": 800
    },
    {
      "epoch": 0.05797299140635657,
      "grad_norm": 2.7173948287963867,
      "learning_rate": 0.00018841904242258901,
      "loss": 0.724,
      "step": 850
    },
    {
      "epoch": 0.06138316737143636,
      "grad_norm": 3.2744317054748535,
      "learning_rate": 0.00018773700722957306,
      "loss": 0.7915,
      "step": 900
    },
    {
      "epoch": 0.06479334333651617,
      "grad_norm": 6.957160472869873,
      "learning_rate": 0.0001870549720365571,
      "loss": 0.7413,
      "step": 950
    },
    {
      "epoch": 0.06820351930159596,
      "grad_norm": 6.471034049987793,
      "learning_rate": 0.00018637293684354114,
      "loss": 0.7185,
      "step": 1000
    },
    {
      "epoch": 0.07161369526667576,
      "grad_norm": 8.29167366027832,
      "learning_rate": 0.00018569090165052518,
      "loss": 0.8072,
      "step": 1050
    },
    {
      "epoch": 0.07502387123175557,
      "grad_norm": 4.713500022888184,
      "learning_rate": 0.00018500886645750922,
      "loss": 0.698,
      "step": 1100
    },
    {
      "epoch": 0.07843404719683536,
      "grad_norm": 3.854116678237915,
      "learning_rate": 0.00018432683126449326,
      "loss": 0.7649,
      "step": 1150
    },
    {
      "epoch": 0.08184422316191516,
      "grad_norm": 6.2115373611450195,
      "learning_rate": 0.0001836447960714773,
      "loss": 0.7694,
      "step": 1200
    },
    {
      "epoch": 0.08525439912699495,
      "grad_norm": 4.388579845428467,
      "learning_rate": 0.00018296276087846134,
      "loss": 0.7361,
      "step": 1250
    },
    {
      "epoch": 0.08866457509207475,
      "grad_norm": 3.0740408897399902,
      "learning_rate": 0.00018228072568544538,
      "loss": 0.7199,
      "step": 1300
    },
    {
      "epoch": 0.09207475105715456,
      "grad_norm": 8.646430969238281,
      "learning_rate": 0.00018159869049242942,
      "loss": 0.6451,
      "step": 1350
    },
    {
      "epoch": 0.09548492702223435,
      "grad_norm": 4.4778571128845215,
      "learning_rate": 0.00018091665529941346,
      "loss": 0.7291,
      "step": 1400
    },
    {
      "epoch": 0.09889510298731415,
      "grad_norm": 4.214262962341309,
      "learning_rate": 0.0001802346201063975,
      "loss": 0.6635,
      "step": 1450
    },
    {
      "epoch": 0.10230527895239394,
      "grad_norm": 5.964533805847168,
      "learning_rate": 0.00017955258491338154,
      "loss": 0.7521,
      "step": 1500
    },
    {
      "epoch": 0.10571545491747374,
      "grad_norm": 4.5395827293396,
      "learning_rate": 0.00017887054972036558,
      "loss": 0.7705,
      "step": 1550
    },
    {
      "epoch": 0.10912563088255355,
      "grad_norm": 7.316029071807861,
      "learning_rate": 0.00017818851452734962,
      "loss": 0.6939,
      "step": 1600
    },
    {
      "epoch": 0.11253580684763334,
      "grad_norm": 3.0576107501983643,
      "learning_rate": 0.00017750647933433366,
      "loss": 0.6685,
      "step": 1650
    },
    {
      "epoch": 0.11594598281271314,
      "grad_norm": 3.5058627128601074,
      "learning_rate": 0.0001768244441413177,
      "loss": 0.7799,
      "step": 1700
    },
    {
      "epoch": 0.11935615877779293,
      "grad_norm": 3.1397881507873535,
      "learning_rate": 0.00017614240894830174,
      "loss": 0.6706,
      "step": 1750
    },
    {
      "epoch": 0.12276633474287273,
      "grad_norm": 3.4533159732818604,
      "learning_rate": 0.00017546037375528579,
      "loss": 0.6467,
      "step": 1800
    },
    {
      "epoch": 0.12617651070795252,
      "grad_norm": 8.200682640075684,
      "learning_rate": 0.00017477833856226983,
      "loss": 0.7298,
      "step": 1850
    },
    {
      "epoch": 0.12958668667303233,
      "grad_norm": 5.146164417266846,
      "learning_rate": 0.00017409630336925387,
      "loss": 0.6493,
      "step": 1900
    },
    {
      "epoch": 0.13299686263811214,
      "grad_norm": 10.090374946594238,
      "learning_rate": 0.00017341426817623788,
      "loss": 0.7459,
      "step": 1950
    },
    {
      "epoch": 0.13640703860319192,
      "grad_norm": 3.905426263809204,
      "learning_rate": 0.00017273223298322195,
      "loss": 0.8022,
      "step": 2000
    },
    {
      "epoch": 0.13981721456827173,
      "grad_norm": 4.08079719543457,
      "learning_rate": 0.000172050197790206,
      "loss": 0.6855,
      "step": 2050
    },
    {
      "epoch": 0.1432273905333515,
      "grad_norm": 5.3953423500061035,
      "learning_rate": 0.00017136816259719003,
      "loss": 0.6217,
      "step": 2100
    },
    {
      "epoch": 0.14663756649843132,
      "grad_norm": 6.528414726257324,
      "learning_rate": 0.00017068612740417407,
      "loss": 0.7564,
      "step": 2150
    },
    {
      "epoch": 0.15004774246351113,
      "grad_norm": 4.280663013458252,
      "learning_rate": 0.00017000409221115808,
      "loss": 0.5976,
      "step": 2200
    },
    {
      "epoch": 0.1534579184285909,
      "grad_norm": 5.444402694702148,
      "learning_rate": 0.00016932205701814215,
      "loss": 0.6436,
      "step": 2250
    },
    {
      "epoch": 0.15686809439367072,
      "grad_norm": 5.351777076721191,
      "learning_rate": 0.0001686400218251262,
      "loss": 0.6727,
      "step": 2300
    },
    {
      "epoch": 0.1602782703587505,
      "grad_norm": 4.954500198364258,
      "learning_rate": 0.00016795798663211023,
      "loss": 0.6495,
      "step": 2350
    },
    {
      "epoch": 0.1636884463238303,
      "grad_norm": 5.804540157318115,
      "learning_rate": 0.00016727595143909427,
      "loss": 0.6415,
      "step": 2400
    },
    {
      "epoch": 0.16709862228891012,
      "grad_norm": 4.152841567993164,
      "learning_rate": 0.00016659391624607829,
      "loss": 0.6753,
      "step": 2450
    },
    {
      "epoch": 0.1705087982539899,
      "grad_norm": 3.4890270233154297,
      "learning_rate": 0.00016591188105306235,
      "loss": 0.7263,
      "step": 2500
    },
    {
      "epoch": 0.1739189742190697,
      "grad_norm": 2.7930243015289307,
      "learning_rate": 0.0001652298458600464,
      "loss": 0.5785,
      "step": 2550
    },
    {
      "epoch": 0.1773291501841495,
      "grad_norm": 4.945596694946289,
      "learning_rate": 0.00016454781066703043,
      "loss": 0.6213,
      "step": 2600
    },
    {
      "epoch": 0.1807393261492293,
      "grad_norm": 5.521026134490967,
      "learning_rate": 0.00016386577547401447,
      "loss": 0.6621,
      "step": 2650
    },
    {
      "epoch": 0.1841495021143091,
      "grad_norm": 7.337632179260254,
      "learning_rate": 0.0001631837402809985,
      "loss": 0.6062,
      "step": 2700
    },
    {
      "epoch": 0.1875596780793889,
      "grad_norm": 4.506790637969971,
      "learning_rate": 0.00016250170508798256,
      "loss": 0.648,
      "step": 2750
    },
    {
      "epoch": 0.1909698540444687,
      "grad_norm": 6.393661022186279,
      "learning_rate": 0.0001618196698949666,
      "loss": 0.6242,
      "step": 2800
    },
    {
      "epoch": 0.19438003000954848,
      "grad_norm": 5.501977920532227,
      "learning_rate": 0.00016113763470195064,
      "loss": 0.6641,
      "step": 2850
    },
    {
      "epoch": 0.1977902059746283,
      "grad_norm": 5.584268569946289,
      "learning_rate": 0.00016045559950893468,
      "loss": 0.6456,
      "step": 2900
    },
    {
      "epoch": 0.2012003819397081,
      "grad_norm": 3.1672675609588623,
      "learning_rate": 0.0001597735643159187,
      "loss": 0.5981,
      "step": 2950
    },
    {
      "epoch": 0.20461055790478788,
      "grad_norm": 3.1868653297424316,
      "learning_rate": 0.00015909152912290276,
      "loss": 0.6906,
      "step": 3000
    },
    {
      "epoch": 0.2080207338698677,
      "grad_norm": 7.952388286590576,
      "learning_rate": 0.0001584094939298868,
      "loss": 0.6847,
      "step": 3050
    },
    {
      "epoch": 0.21143090983494747,
      "grad_norm": 5.1617960929870605,
      "learning_rate": 0.0001577274587368708,
      "loss": 0.6535,
      "step": 3100
    },
    {
      "epoch": 0.21484108580002728,
      "grad_norm": 3.2011797428131104,
      "learning_rate": 0.00015704542354385488,
      "loss": 0.6539,
      "step": 3150
    },
    {
      "epoch": 0.2182512617651071,
      "grad_norm": 4.948228359222412,
      "learning_rate": 0.0001563633883508389,
      "loss": 0.6133,
      "step": 3200
    },
    {
      "epoch": 0.22166143773018687,
      "grad_norm": 6.912439346313477,
      "learning_rate": 0.00015568135315782296,
      "loss": 0.6807,
      "step": 3250
    },
    {
      "epoch": 0.22507161369526668,
      "grad_norm": 8.488619804382324,
      "learning_rate": 0.000154999317964807,
      "loss": 0.6631,
      "step": 3300
    },
    {
      "epoch": 0.22848178966034646,
      "grad_norm": 2.6388792991638184,
      "learning_rate": 0.00015431728277179102,
      "loss": 0.6174,
      "step": 3350
    },
    {
      "epoch": 0.23189196562542627,
      "grad_norm": 4.87584114074707,
      "learning_rate": 0.00015363524757877508,
      "loss": 0.6499,
      "step": 3400
    },
    {
      "epoch": 0.23530214159050608,
      "grad_norm": 6.150944232940674,
      "learning_rate": 0.0001529532123857591,
      "loss": 0.6131,
      "step": 3450
    },
    {
      "epoch": 0.23871231755558586,
      "grad_norm": 4.198827743530273,
      "learning_rate": 0.00015227117719274316,
      "loss": 0.5372,
      "step": 3500
    },
    {
      "epoch": 0.24212249352066567,
      "grad_norm": 2.0392067432403564,
      "learning_rate": 0.0001515891419997272,
      "loss": 0.6264,
      "step": 3550
    },
    {
      "epoch": 0.24553266948574545,
      "grad_norm": 3.4670565128326416,
      "learning_rate": 0.00015090710680671122,
      "loss": 0.5871,
      "step": 3600
    },
    {
      "epoch": 0.24894284545082526,
      "grad_norm": 5.712538242340088,
      "learning_rate": 0.00015022507161369529,
      "loss": 0.7268,
      "step": 3650
    },
    {
      "epoch": 0.25235302141590504,
      "grad_norm": 6.457401275634766,
      "learning_rate": 0.0001495430364206793,
      "loss": 0.6276,
      "step": 3700
    },
    {
      "epoch": 0.25576319738098485,
      "grad_norm": 3.3961546421051025,
      "learning_rate": 0.00014886100122766337,
      "loss": 0.5853,
      "step": 3750
    },
    {
      "epoch": 0.25917337334606466,
      "grad_norm": 2.5588583946228027,
      "learning_rate": 0.0001481789660346474,
      "loss": 0.5453,
      "step": 3800
    },
    {
      "epoch": 0.26258354931114447,
      "grad_norm": 5.013031005859375,
      "learning_rate": 0.00014749693084163142,
      "loss": 0.7149,
      "step": 3850
    },
    {
      "epoch": 0.2659937252762243,
      "grad_norm": 2.513658285140991,
      "learning_rate": 0.0001468148956486155,
      "loss": 0.5337,
      "step": 3900
    },
    {
      "epoch": 0.26940390124130403,
      "grad_norm": 3.9456241130828857,
      "learning_rate": 0.0001461328604555995,
      "loss": 0.6555,
      "step": 3950
    },
    {
      "epoch": 0.27281407720638384,
      "grad_norm": 4.685662269592285,
      "learning_rate": 0.00014545082526258357,
      "loss": 0.6494,
      "step": 4000
    },
    {
      "epoch": 0.27622425317146365,
      "grad_norm": 2.4544034004211426,
      "learning_rate": 0.0001447687900695676,
      "loss": 0.5864,
      "step": 4050
    },
    {
      "epoch": 0.27963442913654346,
      "grad_norm": 4.853707790374756,
      "learning_rate": 0.00014408675487655162,
      "loss": 0.5535,
      "step": 4100
    },
    {
      "epoch": 0.28304460510162327,
      "grad_norm": 3.771658420562744,
      "learning_rate": 0.0001434047196835357,
      "loss": 0.6221,
      "step": 4150
    },
    {
      "epoch": 0.286454781066703,
      "grad_norm": 5.254078388214111,
      "learning_rate": 0.0001427226844905197,
      "loss": 0.6851,
      "step": 4200
    },
    {
      "epoch": 0.28986495703178283,
      "grad_norm": 4.231786727905273,
      "learning_rate": 0.00014204064929750375,
      "loss": 0.5294,
      "step": 4250
    },
    {
      "epoch": 0.29327513299686264,
      "grad_norm": 5.343347072601318,
      "learning_rate": 0.0001413586141044878,
      "loss": 0.6611,
      "step": 4300
    },
    {
      "epoch": 0.29668530896194245,
      "grad_norm": 2.557615041732788,
      "learning_rate": 0.00014067657891147183,
      "loss": 0.5546,
      "step": 4350
    },
    {
      "epoch": 0.30009548492702226,
      "grad_norm": 2.845707416534424,
      "learning_rate": 0.0001399945437184559,
      "loss": 0.5803,
      "step": 4400
    },
    {
      "epoch": 0.303505660892102,
      "grad_norm": 4.8238019943237305,
      "learning_rate": 0.0001393125085254399,
      "loss": 0.5271,
      "step": 4450
    },
    {
      "epoch": 0.3069158368571818,
      "grad_norm": 4.461119174957275,
      "learning_rate": 0.00013863047333242395,
      "loss": 0.5665,
      "step": 4500
    },
    {
      "epoch": 0.31032601282226163,
      "grad_norm": 5.214630603790283,
      "learning_rate": 0.00013794843813940802,
      "loss": 0.5978,
      "step": 4550
    },
    {
      "epoch": 0.31373618878734144,
      "grad_norm": 5.70454740524292,
      "learning_rate": 0.00013726640294639203,
      "loss": 0.6142,
      "step": 4600
    },
    {
      "epoch": 0.31714636475242125,
      "grad_norm": 5.019055366516113,
      "learning_rate": 0.0001365843677533761,
      "loss": 0.5755,
      "step": 4650
    },
    {
      "epoch": 0.320556540717501,
      "grad_norm": 3.8805289268493652,
      "learning_rate": 0.0001359023325603601,
      "loss": 0.6828,
      "step": 4700
    },
    {
      "epoch": 0.3239667166825808,
      "grad_norm": 5.59073543548584,
      "learning_rate": 0.00013522029736734415,
      "loss": 0.6345,
      "step": 4750
    },
    {
      "epoch": 0.3273768926476606,
      "grad_norm": 2.9346184730529785,
      "learning_rate": 0.00013453826217432822,
      "loss": 0.6216,
      "step": 4800
    },
    {
      "epoch": 0.33078706861274043,
      "grad_norm": 2.691051721572876,
      "learning_rate": 0.00013385622698131223,
      "loss": 0.5292,
      "step": 4850
    },
    {
      "epoch": 0.33419724457782024,
      "grad_norm": 4.883913040161133,
      "learning_rate": 0.0001331741917882963,
      "loss": 0.6155,
      "step": 4900
    },
    {
      "epoch": 0.3376074205429,
      "grad_norm": 8.675580024719238,
      "learning_rate": 0.0001324921565952803,
      "loss": 0.5703,
      "step": 4950
    },
    {
      "epoch": 0.3410175965079798,
      "grad_norm": 1.6882177591323853,
      "learning_rate": 0.00013181012140226435,
      "loss": 0.6224,
      "step": 5000
    },
    {
      "epoch": 0.3444277724730596,
      "grad_norm": 6.218672275543213,
      "learning_rate": 0.00013112808620924842,
      "loss": 0.529,
      "step": 5050
    },
    {
      "epoch": 0.3478379484381394,
      "grad_norm": 3.875326156616211,
      "learning_rate": 0.00013044605101623243,
      "loss": 0.6051,
      "step": 5100
    },
    {
      "epoch": 0.35124812440321923,
      "grad_norm": 5.031089782714844,
      "learning_rate": 0.0001297640158232165,
      "loss": 0.6728,
      "step": 5150
    },
    {
      "epoch": 0.354658300368299,
      "grad_norm": 4.466359615325928,
      "learning_rate": 0.00012908198063020052,
      "loss": 0.6529,
      "step": 5200
    },
    {
      "epoch": 0.3580684763333788,
      "grad_norm": 6.175688743591309,
      "learning_rate": 0.00012839994543718456,
      "loss": 0.5987,
      "step": 5250
    },
    {
      "epoch": 0.3614786522984586,
      "grad_norm": 4.711555480957031,
      "learning_rate": 0.00012771791024416862,
      "loss": 0.5911,
      "step": 5300
    },
    {
      "epoch": 0.3648888282635384,
      "grad_norm": 6.1712517738342285,
      "learning_rate": 0.00012703587505115264,
      "loss": 0.5936,
      "step": 5350
    },
    {
      "epoch": 0.3682990042286182,
      "grad_norm": 4.316425323486328,
      "learning_rate": 0.00012635383985813668,
      "loss": 0.6234,
      "step": 5400
    },
    {
      "epoch": 0.371709180193698,
      "grad_norm": 4.719680309295654,
      "learning_rate": 0.00012567180466512072,
      "loss": 0.6132,
      "step": 5450
    },
    {
      "epoch": 0.3751193561587778,
      "grad_norm": 4.705702781677246,
      "learning_rate": 0.00012498976947210476,
      "loss": 0.5966,
      "step": 5500
    },
    {
      "epoch": 0.3785295321238576,
      "grad_norm": 7.5611066818237305,
      "learning_rate": 0.00012430773427908883,
      "loss": 0.6128,
      "step": 5550
    },
    {
      "epoch": 0.3819397080889374,
      "grad_norm": 2.368633270263672,
      "learning_rate": 0.00012362569908607284,
      "loss": 0.5856,
      "step": 5600
    },
    {
      "epoch": 0.3853498840540172,
      "grad_norm": 4.5140581130981445,
      "learning_rate": 0.00012294366389305688,
      "loss": 0.5325,
      "step": 5650
    },
    {
      "epoch": 0.38876006001909696,
      "grad_norm": 2.8268239498138428,
      "learning_rate": 0.00012226162870004092,
      "loss": 0.6625,
      "step": 5700
    },
    {
      "epoch": 0.3921702359841768,
      "grad_norm": 4.5069193840026855,
      "learning_rate": 0.00012157959350702498,
      "loss": 0.5703,
      "step": 5750
    },
    {
      "epoch": 0.3955804119492566,
      "grad_norm": 4.233426570892334,
      "learning_rate": 0.00012089755831400902,
      "loss": 0.6181,
      "step": 5800
    },
    {
      "epoch": 0.3989905879143364,
      "grad_norm": 5.632469654083252,
      "learning_rate": 0.00012021552312099304,
      "loss": 0.6498,
      "step": 5850
    },
    {
      "epoch": 0.4024007638794162,
      "grad_norm": 4.147462368011475,
      "learning_rate": 0.0001195334879279771,
      "loss": 0.5799,
      "step": 5900
    },
    {
      "epoch": 0.40581093984449595,
      "grad_norm": 4.0977678298950195,
      "learning_rate": 0.00011885145273496112,
      "loss": 0.6053,
      "step": 5950
    },
    {
      "epoch": 0.40922111580957576,
      "grad_norm": 4.127129077911377,
      "learning_rate": 0.00011816941754194516,
      "loss": 0.5486,
      "step": 6000
    },
    {
      "epoch": 0.4126312917746556,
      "grad_norm": 4.335511207580566,
      "learning_rate": 0.00011748738234892922,
      "loss": 0.5457,
      "step": 6050
    },
    {
      "epoch": 0.4160414677397354,
      "grad_norm": 3.2215158939361572,
      "learning_rate": 0.00011680534715591325,
      "loss": 0.596,
      "step": 6100
    },
    {
      "epoch": 0.4194516437048152,
      "grad_norm": 6.020462512969971,
      "learning_rate": 0.0001161233119628973,
      "loss": 0.5542,
      "step": 6150
    },
    {
      "epoch": 0.42286181966989494,
      "grad_norm": 2.6464805603027344,
      "learning_rate": 0.00011544127676988133,
      "loss": 0.5626,
      "step": 6200
    },
    {
      "epoch": 0.42627199563497475,
      "grad_norm": 1.5068004131317139,
      "learning_rate": 0.00011475924157686537,
      "loss": 0.5238,
      "step": 6250
    },
    {
      "epoch": 0.42968217160005456,
      "grad_norm": 3.298980236053467,
      "learning_rate": 0.00011407720638384942,
      "loss": 0.5736,
      "step": 6300
    },
    {
      "epoch": 0.4330923475651344,
      "grad_norm": 5.919875621795654,
      "learning_rate": 0.00011339517119083345,
      "loss": 0.5659,
      "step": 6350
    },
    {
      "epoch": 0.4365025235302142,
      "grad_norm": 3.8924498558044434,
      "learning_rate": 0.0001127131359978175,
      "loss": 0.6007,
      "step": 6400
    },
    {
      "epoch": 0.43991269949529394,
      "grad_norm": 5.162298202514648,
      "learning_rate": 0.00011203110080480153,
      "loss": 0.6011,
      "step": 6450
    },
    {
      "epoch": 0.44332287546037374,
      "grad_norm": 4.657707691192627,
      "learning_rate": 0.00011134906561178557,
      "loss": 0.5633,
      "step": 6500
    },
    {
      "epoch": 0.44673305142545355,
      "grad_norm": 2.891951322555542,
      "learning_rate": 0.00011066703041876962,
      "loss": 0.5358,
      "step": 6550
    },
    {
      "epoch": 0.45014322739053336,
      "grad_norm": 5.508028030395508,
      "learning_rate": 0.00010998499522575365,
      "loss": 0.5556,
      "step": 6600
    },
    {
      "epoch": 0.45355340335561317,
      "grad_norm": 7.165639877319336,
      "learning_rate": 0.0001093029600327377,
      "loss": 0.5534,
      "step": 6650
    },
    {
      "epoch": 0.4569635793206929,
      "grad_norm": 3.2173025608062744,
      "learning_rate": 0.00010862092483972173,
      "loss": 0.5629,
      "step": 6700
    },
    {
      "epoch": 0.46037375528577273,
      "grad_norm": 13.488101959228516,
      "learning_rate": 0.00010793888964670577,
      "loss": 0.5943,
      "step": 6750
    },
    {
      "epoch": 0.46378393125085254,
      "grad_norm": 5.629012107849121,
      "learning_rate": 0.00010725685445368983,
      "loss": 0.5135,
      "step": 6800
    },
    {
      "epoch": 0.46719410721593235,
      "grad_norm": 10.451211929321289,
      "learning_rate": 0.00010657481926067385,
      "loss": 0.5256,
      "step": 6850
    },
    {
      "epoch": 0.47060428318101216,
      "grad_norm": 9.353708267211914,
      "learning_rate": 0.00010589278406765791,
      "loss": 0.5646,
      "step": 6900
    },
    {
      "epoch": 0.4740144591460919,
      "grad_norm": 2.6488242149353027,
      "learning_rate": 0.00010521074887464194,
      "loss": 0.6341,
      "step": 6950
    },
    {
      "epoch": 0.4774246351111717,
      "grad_norm": 5.897425174713135,
      "learning_rate": 0.00010452871368162598,
      "loss": 0.5644,
      "step": 7000
    },
    {
      "epoch": 0.48083481107625153,
      "grad_norm": 3.759345054626465,
      "learning_rate": 0.00010384667848861003,
      "loss": 0.6381,
      "step": 7050
    },
    {
      "epoch": 0.48424498704133134,
      "grad_norm": 2.7842280864715576,
      "learning_rate": 0.00010316464329559406,
      "loss": 0.5831,
      "step": 7100
    },
    {
      "epoch": 0.48765516300641115,
      "grad_norm": 7.516535758972168,
      "learning_rate": 0.0001024826081025781,
      "loss": 0.5414,
      "step": 7150
    },
    {
      "epoch": 0.4910653389714909,
      "grad_norm": 2.411266565322876,
      "learning_rate": 0.00010180057290956212,
      "loss": 0.5004,
      "step": 7200
    },
    {
      "epoch": 0.4944755149365707,
      "grad_norm": 4.32659912109375,
      "learning_rate": 0.00010111853771654618,
      "loss": 0.5577,
      "step": 7250
    },
    {
      "epoch": 0.4978856909016505,
      "grad_norm": 11.781500816345215,
      "learning_rate": 0.00010043650252353023,
      "loss": 0.69,
      "step": 7300
    },
    {
      "epoch": 0.5012958668667303,
      "grad_norm": 5.525868892669678,
      "learning_rate": 9.975446733051426e-05,
      "loss": 0.5416,
      "step": 7350
    },
    {
      "epoch": 0.5047060428318101,
      "grad_norm": 3.8865575790405273,
      "learning_rate": 9.90724321374983e-05,
      "loss": 0.5192,
      "step": 7400
    },
    {
      "epoch": 0.50811621879689,
      "grad_norm": 3.840557813644409,
      "learning_rate": 9.839039694448234e-05,
      "loss": 0.5872,
      "step": 7450
    },
    {
      "epoch": 0.5115263947619697,
      "grad_norm": 3.024540662765503,
      "learning_rate": 9.770836175146638e-05,
      "loss": 0.5198,
      "step": 7500
    },
    {
      "epoch": 0.5149365707270496,
      "grad_norm": 3.4806625843048096,
      "learning_rate": 9.702632655845042e-05,
      "loss": 0.5875,
      "step": 7550
    },
    {
      "epoch": 0.5183467466921293,
      "grad_norm": 7.0779805183410645,
      "learning_rate": 9.634429136543446e-05,
      "loss": 0.6465,
      "step": 7600
    },
    {
      "epoch": 0.5217569226572091,
      "grad_norm": 3.5003528594970703,
      "learning_rate": 9.566225617241849e-05,
      "loss": 0.5787,
      "step": 7650
    },
    {
      "epoch": 0.5251670986222889,
      "grad_norm": 6.682493209838867,
      "learning_rate": 9.498022097940254e-05,
      "loss": 0.5161,
      "step": 7700
    },
    {
      "epoch": 0.5285772745873687,
      "grad_norm": 2.784795045852661,
      "learning_rate": 9.429818578638658e-05,
      "loss": 0.659,
      "step": 7750
    },
    {
      "epoch": 0.5319874505524486,
      "grad_norm": 5.780961513519287,
      "learning_rate": 9.361615059337062e-05,
      "loss": 0.5981,
      "step": 7800
    },
    {
      "epoch": 0.5353976265175283,
      "grad_norm": 2.0960235595703125,
      "learning_rate": 9.293411540035467e-05,
      "loss": 0.5777,
      "step": 7850
    },
    {
      "epoch": 0.5388078024826081,
      "grad_norm": 7.068978786468506,
      "learning_rate": 9.225208020733869e-05,
      "loss": 0.5936,
      "step": 7900
    },
    {
      "epoch": 0.5422179784476879,
      "grad_norm": 10.035943031311035,
      "learning_rate": 9.157004501432275e-05,
      "loss": 0.6085,
      "step": 7950
    },
    {
      "epoch": 0.5456281544127677,
      "grad_norm": 5.948573112487793,
      "learning_rate": 9.088800982130679e-05,
      "loss": 0.6013,
      "step": 8000
    },
    {
      "epoch": 0.5490383303778475,
      "grad_norm": 2.583730936050415,
      "learning_rate": 9.020597462829083e-05,
      "loss": 0.5675,
      "step": 8050
    },
    {
      "epoch": 0.5524485063429273,
      "grad_norm": 3.9786033630371094,
      "learning_rate": 8.952393943527487e-05,
      "loss": 0.5364,
      "step": 8100
    },
    {
      "epoch": 0.5558586823080071,
      "grad_norm": 6.482367038726807,
      "learning_rate": 8.88419042422589e-05,
      "loss": 0.563,
      "step": 8150
    },
    {
      "epoch": 0.5592688582730869,
      "grad_norm": 4.106327533721924,
      "learning_rate": 8.815986904924295e-05,
      "loss": 0.5408,
      "step": 8200
    },
    {
      "epoch": 0.5626790342381667,
      "grad_norm": 1.82575261592865,
      "learning_rate": 8.747783385622699e-05,
      "loss": 0.4978,
      "step": 8250
    },
    {
      "epoch": 0.5660892102032465,
      "grad_norm": 2.854848861694336,
      "learning_rate": 8.679579866321103e-05,
      "loss": 0.5715,
      "step": 8300
    },
    {
      "epoch": 0.5694993861683263,
      "grad_norm": 5.640099048614502,
      "learning_rate": 8.611376347019506e-05,
      "loss": 0.5797,
      "step": 8350
    },
    {
      "epoch": 0.572909562133406,
      "grad_norm": 6.124927520751953,
      "learning_rate": 8.54317282771791e-05,
      "loss": 0.5453,
      "step": 8400
    },
    {
      "epoch": 0.5763197380984859,
      "grad_norm": 4.834043025970459,
      "learning_rate": 8.474969308416315e-05,
      "loss": 0.5554,
      "step": 8450
    },
    {
      "epoch": 0.5797299140635657,
      "grad_norm": 2.196300506591797,
      "learning_rate": 8.406765789114719e-05,
      "loss": 0.5836,
      "step": 8500
    },
    {
      "epoch": 0.5831400900286455,
      "grad_norm": 5.784862518310547,
      "learning_rate": 8.338562269813123e-05,
      "loss": 0.5039,
      "step": 8550
    },
    {
      "epoch": 0.5865502659937253,
      "grad_norm": 1.847355604171753,
      "learning_rate": 8.270358750511526e-05,
      "loss": 0.5516,
      "step": 8600
    },
    {
      "epoch": 0.589960441958805,
      "grad_norm": 1.5940364599227905,
      "learning_rate": 8.20215523120993e-05,
      "loss": 0.5493,
      "step": 8650
    },
    {
      "epoch": 0.5933706179238849,
      "grad_norm": 1.9647265672683716,
      "learning_rate": 8.133951711908336e-05,
      "loss": 0.4822,
      "step": 8700
    },
    {
      "epoch": 0.5967807938889647,
      "grad_norm": 4.652100563049316,
      "learning_rate": 8.06574819260674e-05,
      "loss": 0.5557,
      "step": 8750
    },
    {
      "epoch": 0.6001909698540445,
      "grad_norm": 6.329176902770996,
      "learning_rate": 7.997544673305142e-05,
      "loss": 0.4294,
      "step": 8800
    },
    {
      "epoch": 0.6036011458191243,
      "grad_norm": 5.771829605102539,
      "learning_rate": 7.929341154003546e-05,
      "loss": 0.5932,
      "step": 8850
    },
    {
      "epoch": 0.607011321784204,
      "grad_norm": 8.813058853149414,
      "learning_rate": 7.86113763470195e-05,
      "loss": 0.6002,
      "step": 8900
    },
    {
      "epoch": 0.6104214977492839,
      "grad_norm": 3.290952682495117,
      "learning_rate": 7.792934115400356e-05,
      "loss": 0.6195,
      "step": 8950
    },
    {
      "epoch": 0.6138316737143636,
      "grad_norm": 6.463291168212891,
      "learning_rate": 7.72473059609876e-05,
      "loss": 0.4248,
      "step": 9000
    },
    {
      "epoch": 0.6172418496794435,
      "grad_norm": 1.7211240530014038,
      "learning_rate": 7.656527076797163e-05,
      "loss": 0.5767,
      "step": 9050
    },
    {
      "epoch": 0.6206520256445233,
      "grad_norm": 2.781705617904663,
      "learning_rate": 7.588323557495567e-05,
      "loss": 0.5196,
      "step": 9100
    },
    {
      "epoch": 0.624062201609603,
      "grad_norm": 1.250685691833496,
      "learning_rate": 7.52012003819397e-05,
      "loss": 0.5752,
      "step": 9150
    },
    {
      "epoch": 0.6274723775746829,
      "grad_norm": 4.680776119232178,
      "learning_rate": 7.451916518892376e-05,
      "loss": 0.5031,
      "step": 9200
    },
    {
      "epoch": 0.6308825535397626,
      "grad_norm": 2.7084715366363525,
      "learning_rate": 7.38371299959078e-05,
      "loss": 0.5289,
      "step": 9250
    },
    {
      "epoch": 0.6342927295048425,
      "grad_norm": 4.132769584655762,
      "learning_rate": 7.315509480289183e-05,
      "loss": 0.5765,
      "step": 9300
    },
    {
      "epoch": 0.6377029054699223,
      "grad_norm": 6.281797409057617,
      "learning_rate": 7.247305960987587e-05,
      "loss": 0.5854,
      "step": 9350
    },
    {
      "epoch": 0.641113081435002,
      "grad_norm": 1.6743223667144775,
      "learning_rate": 7.179102441685991e-05,
      "loss": 0.5422,
      "step": 9400
    },
    {
      "epoch": 0.6445232574000819,
      "grad_norm": 3.362123727798462,
      "learning_rate": 7.110898922384396e-05,
      "loss": 0.5941,
      "step": 9450
    },
    {
      "epoch": 0.6479334333651616,
      "grad_norm": 6.549856185913086,
      "learning_rate": 7.042695403082799e-05,
      "loss": 0.463,
      "step": 9500
    },
    {
      "epoch": 0.6513436093302415,
      "grad_norm": 3.057328462600708,
      "learning_rate": 6.974491883781203e-05,
      "loss": 0.5752,
      "step": 9550
    },
    {
      "epoch": 0.6547537852953212,
      "grad_norm": 1.884178876876831,
      "learning_rate": 6.906288364479607e-05,
      "loss": 0.5374,
      "step": 9600
    },
    {
      "epoch": 0.658163961260401,
      "grad_norm": 5.725998401641846,
      "learning_rate": 6.838084845178011e-05,
      "loss": 0.5478,
      "step": 9650
    },
    {
      "epoch": 0.6615741372254809,
      "grad_norm": 1.6064448356628418,
      "learning_rate": 6.769881325876417e-05,
      "loss": 0.5872,
      "step": 9700
    },
    {
      "epoch": 0.6649843131905606,
      "grad_norm": 6.622208595275879,
      "learning_rate": 6.701677806574819e-05,
      "loss": 0.5767,
      "step": 9750
    },
    {
      "epoch": 0.6683944891556405,
      "grad_norm": 4.125788688659668,
      "learning_rate": 6.633474287273223e-05,
      "loss": 0.5368,
      "step": 9800
    },
    {
      "epoch": 0.6718046651207202,
      "grad_norm": 3.332702875137329,
      "learning_rate": 6.565270767971627e-05,
      "loss": 0.5482,
      "step": 9850
    },
    {
      "epoch": 0.6752148410858,
      "grad_norm": 5.648167133331299,
      "learning_rate": 6.497067248670031e-05,
      "loss": 0.5869,
      "step": 9900
    },
    {
      "epoch": 0.6786250170508799,
      "grad_norm": 1.5721782445907593,
      "learning_rate": 6.428863729368436e-05,
      "loss": 0.4347,
      "step": 9950
    },
    {
      "epoch": 0.6820351930159596,
      "grad_norm": 10.398826599121094,
      "learning_rate": 6.36066021006684e-05,
      "loss": 0.5483,
      "step": 10000
    },
    {
      "epoch": 0.6854453689810395,
      "grad_norm": 3.208296298980713,
      "learning_rate": 6.292456690765244e-05,
      "loss": 0.4934,
      "step": 10050
    },
    {
      "epoch": 0.6888555449461192,
      "grad_norm": 4.96205472946167,
      "learning_rate": 6.224253171463648e-05,
      "loss": 0.5784,
      "step": 10100
    },
    {
      "epoch": 0.692265720911199,
      "grad_norm": 3.377537727355957,
      "learning_rate": 6.156049652162052e-05,
      "loss": 0.478,
      "step": 10150
    },
    {
      "epoch": 0.6956758968762788,
      "grad_norm": 4.564947605133057,
      "learning_rate": 6.0878461328604565e-05,
      "loss": 0.4941,
      "step": 10200
    },
    {
      "epoch": 0.6990860728413586,
      "grad_norm": 3.520151138305664,
      "learning_rate": 6.01964261355886e-05,
      "loss": 0.5118,
      "step": 10250
    },
    {
      "epoch": 0.7024962488064385,
      "grad_norm": 6.656129837036133,
      "learning_rate": 5.951439094257264e-05,
      "loss": 0.5612,
      "step": 10300
    },
    {
      "epoch": 0.7059064247715182,
      "grad_norm": 7.647152423858643,
      "learning_rate": 5.883235574955668e-05,
      "loss": 0.5951,
      "step": 10350
    },
    {
      "epoch": 0.709316600736598,
      "grad_norm": 5.958653926849365,
      "learning_rate": 5.8150320556540714e-05,
      "loss": 0.5451,
      "step": 10400
    },
    {
      "epoch": 0.7127267767016778,
      "grad_norm": 9.535198211669922,
      "learning_rate": 5.746828536352477e-05,
      "loss": 0.4921,
      "step": 10450
    },
    {
      "epoch": 0.7161369526667576,
      "grad_norm": 2.0177628993988037,
      "learning_rate": 5.67862501705088e-05,
      "loss": 0.498,
      "step": 10500
    },
    {
      "epoch": 0.7195471286318375,
      "grad_norm": 4.619517803192139,
      "learning_rate": 5.610421497749284e-05,
      "loss": 0.5109,
      "step": 10550
    },
    {
      "epoch": 0.7229573045969172,
      "grad_norm": 5.4467034339904785,
      "learning_rate": 5.542217978447688e-05,
      "loss": 0.4603,
      "step": 10600
    },
    {
      "epoch": 0.726367480561997,
      "grad_norm": 3.996352434158325,
      "learning_rate": 5.4740144591460916e-05,
      "loss": 0.544,
      "step": 10650
    },
    {
      "epoch": 0.7297776565270768,
      "grad_norm": 3.8672971725463867,
      "learning_rate": 5.4058109398444964e-05,
      "loss": 0.6102,
      "step": 10700
    },
    {
      "epoch": 0.7331878324921566,
      "grad_norm": 4.379242420196533,
      "learning_rate": 5.3376074205429004e-05,
      "loss": 0.4766,
      "step": 10750
    },
    {
      "epoch": 0.7365980084572364,
      "grad_norm": 5.862784385681152,
      "learning_rate": 5.2694039012413045e-05,
      "loss": 0.46,
      "step": 10800
    },
    {
      "epoch": 0.7400081844223162,
      "grad_norm": 3.097458839416504,
      "learning_rate": 5.201200381939708e-05,
      "loss": 0.5718,
      "step": 10850
    },
    {
      "epoch": 0.743418360387396,
      "grad_norm": 7.027818202972412,
      "learning_rate": 5.132996862638112e-05,
      "loss": 0.6057,
      "step": 10900
    },
    {
      "epoch": 0.7468285363524758,
      "grad_norm": 0.961347222328186,
      "learning_rate": 5.0647933433365167e-05,
      "loss": 0.54,
      "step": 10950
    },
    {
      "epoch": 0.7502387123175556,
      "grad_norm": 4.162463188171387,
      "learning_rate": 4.996589824034921e-05,
      "loss": 0.5581,
      "step": 11000
    },
    {
      "epoch": 0.7536488882826354,
      "grad_norm": 6.5088701248168945,
      "learning_rate": 4.928386304733325e-05,
      "loss": 0.5713,
      "step": 11050
    },
    {
      "epoch": 0.7570590642477152,
      "grad_norm": 2.9248955249786377,
      "learning_rate": 4.860182785431728e-05,
      "loss": 0.5306,
      "step": 11100
    },
    {
      "epoch": 0.7604692402127949,
      "grad_norm": 4.549586772918701,
      "learning_rate": 4.791979266130133e-05,
      "loss": 0.6117,
      "step": 11150
    },
    {
      "epoch": 0.7638794161778748,
      "grad_norm": 7.319525718688965,
      "learning_rate": 4.723775746828536e-05,
      "loss": 0.5105,
      "step": 11200
    },
    {
      "epoch": 0.7672895921429546,
      "grad_norm": 5.738351821899414,
      "learning_rate": 4.655572227526941e-05,
      "loss": 0.5274,
      "step": 11250
    },
    {
      "epoch": 0.7706997681080344,
      "grad_norm": 5.382073879241943,
      "learning_rate": 4.5873687082253444e-05,
      "loss": 0.5184,
      "step": 11300
    },
    {
      "epoch": 0.7741099440731142,
      "grad_norm": 5.193735122680664,
      "learning_rate": 4.5191651889237484e-05,
      "loss": 0.4961,
      "step": 11350
    },
    {
      "epoch": 0.7775201200381939,
      "grad_norm": 6.441908359527588,
      "learning_rate": 4.450961669622153e-05,
      "loss": 0.4802,
      "step": 11400
    },
    {
      "epoch": 0.7809302960032738,
      "grad_norm": 4.67043924331665,
      "learning_rate": 4.3827581503205565e-05,
      "loss": 0.526,
      "step": 11450
    },
    {
      "epoch": 0.7843404719683535,
      "grad_norm": 4.703309535980225,
      "learning_rate": 4.3145546310189606e-05,
      "loss": 0.522,
      "step": 11500
    },
    {
      "epoch": 0.7877506479334334,
      "grad_norm": 2.3293673992156982,
      "learning_rate": 4.2463511117173646e-05,
      "loss": 0.5507,
      "step": 11550
    },
    {
      "epoch": 0.7911608238985132,
      "grad_norm": 4.88871431350708,
      "learning_rate": 4.178147592415769e-05,
      "loss": 0.5017,
      "step": 11600
    },
    {
      "epoch": 0.7945709998635929,
      "grad_norm": 8.45776081085205,
      "learning_rate": 4.109944073114173e-05,
      "loss": 0.5445,
      "step": 11650
    },
    {
      "epoch": 0.7979811758286728,
      "grad_norm": 5.590054988861084,
      "learning_rate": 4.041740553812577e-05,
      "loss": 0.5079,
      "step": 11700
    },
    {
      "epoch": 0.8013913517937525,
      "grad_norm": 5.460699081420898,
      "learning_rate": 3.973537034510981e-05,
      "loss": 0.522,
      "step": 11750
    },
    {
      "epoch": 0.8048015277588324,
      "grad_norm": 6.064262390136719,
      "learning_rate": 3.905333515209385e-05,
      "loss": 0.5399,
      "step": 11800
    },
    {
      "epoch": 0.8082117037239122,
      "grad_norm": 5.100581169128418,
      "learning_rate": 3.837129995907789e-05,
      "loss": 0.5667,
      "step": 11850
    },
    {
      "epoch": 0.8116218796889919,
      "grad_norm": 3.451298713684082,
      "learning_rate": 3.768926476606193e-05,
      "loss": 0.4233,
      "step": 11900
    },
    {
      "epoch": 0.8150320556540718,
      "grad_norm": 4.423320293426514,
      "learning_rate": 3.700722957304597e-05,
      "loss": 0.5002,
      "step": 11950
    },
    {
      "epoch": 0.8184422316191515,
      "grad_norm": 3.1996169090270996,
      "learning_rate": 3.632519438003001e-05,
      "loss": 0.506,
      "step": 12000
    },
    {
      "epoch": 0.8218524075842314,
      "grad_norm": 14.234532356262207,
      "learning_rate": 3.564315918701405e-05,
      "loss": 0.6262,
      "step": 12050
    },
    {
      "epoch": 0.8252625835493111,
      "grad_norm": 1.7803430557250977,
      "learning_rate": 3.496112399399809e-05,
      "loss": 0.4936,
      "step": 12100
    },
    {
      "epoch": 0.8286727595143909,
      "grad_norm": 4.276773929595947,
      "learning_rate": 3.427908880098213e-05,
      "loss": 0.4548,
      "step": 12150
    },
    {
      "epoch": 0.8320829354794708,
      "grad_norm": 2.504307270050049,
      "learning_rate": 3.3597053607966174e-05,
      "loss": 0.4721,
      "step": 12200
    },
    {
      "epoch": 0.8354931114445505,
      "grad_norm": 7.63356351852417,
      "learning_rate": 3.291501841495021e-05,
      "loss": 0.5049,
      "step": 12250
    },
    {
      "epoch": 0.8389032874096304,
      "grad_norm": 5.366413116455078,
      "learning_rate": 3.2232983221934255e-05,
      "loss": 0.5826,
      "step": 12300
    },
    {
      "epoch": 0.8423134633747101,
      "grad_norm": 5.6204752922058105,
      "learning_rate": 3.1550948028918295e-05,
      "loss": 0.4524,
      "step": 12350
    },
    {
      "epoch": 0.8457236393397899,
      "grad_norm": 5.204686164855957,
      "learning_rate": 3.0868912835902336e-05,
      "loss": 0.4704,
      "step": 12400
    },
    {
      "epoch": 0.8491338153048698,
      "grad_norm": 4.693800449371338,
      "learning_rate": 3.0186877642886373e-05,
      "loss": 0.5541,
      "step": 12450
    },
    {
      "epoch": 0.8525439912699495,
      "grad_norm": 3.7460267543792725,
      "learning_rate": 2.9504842449870414e-05,
      "loss": 0.4791,
      "step": 12500
    },
    {
      "epoch": 0.8559541672350294,
      "grad_norm": 1.577236533164978,
      "learning_rate": 2.8822807256854458e-05,
      "loss": 0.4299,
      "step": 12550
    },
    {
      "epoch": 0.8593643432001091,
      "grad_norm": 5.456058979034424,
      "learning_rate": 2.8140772063838495e-05,
      "loss": 0.5127,
      "step": 12600
    },
    {
      "epoch": 0.8627745191651889,
      "grad_norm": 5.028600692749023,
      "learning_rate": 2.745873687082254e-05,
      "loss": 0.543,
      "step": 12650
    },
    {
      "epoch": 0.8661846951302687,
      "grad_norm": 4.790678977966309,
      "learning_rate": 2.6776701677806576e-05,
      "loss": 0.5712,
      "step": 12700
    },
    {
      "epoch": 0.8695948710953485,
      "grad_norm": 4.220641613006592,
      "learning_rate": 2.6094666484790613e-05,
      "loss": 0.4889,
      "step": 12750
    },
    {
      "epoch": 0.8730050470604284,
      "grad_norm": 5.424318313598633,
      "learning_rate": 2.5412631291774657e-05,
      "loss": 0.5538,
      "step": 12800
    },
    {
      "epoch": 0.8764152230255081,
      "grad_norm": 3.9882211685180664,
      "learning_rate": 2.4730596098758698e-05,
      "loss": 0.4991,
      "step": 12850
    },
    {
      "epoch": 0.8798253989905879,
      "grad_norm": 6.338980674743652,
      "learning_rate": 2.4048560905742738e-05,
      "loss": 0.5502,
      "step": 12900
    },
    {
      "epoch": 0.8832355749556677,
      "grad_norm": 3.1798129081726074,
      "learning_rate": 2.336652571272678e-05,
      "loss": 0.5695,
      "step": 12950
    },
    {
      "epoch": 0.8866457509207475,
      "grad_norm": 6.5289788246154785,
      "learning_rate": 2.268449051971082e-05,
      "loss": 0.584,
      "step": 13000
    },
    {
      "epoch": 0.8900559268858274,
      "grad_norm": 9.186223030090332,
      "learning_rate": 2.200245532669486e-05,
      "loss": 0.4858,
      "step": 13050
    },
    {
      "epoch": 0.8934661028509071,
      "grad_norm": 3.206188201904297,
      "learning_rate": 2.1320420133678897e-05,
      "loss": 0.4743,
      "step": 13100
    },
    {
      "epoch": 0.8968762788159869,
      "grad_norm": 5.798360824584961,
      "learning_rate": 2.0638384940662938e-05,
      "loss": 0.5226,
      "step": 13150
    },
    {
      "epoch": 0.9002864547810667,
      "grad_norm": 5.66102409362793,
      "learning_rate": 1.9956349747646978e-05,
      "loss": 0.4966,
      "step": 13200
    },
    {
      "epoch": 0.9036966307461465,
      "grad_norm": 5.558487415313721,
      "learning_rate": 1.9274314554631022e-05,
      "loss": 0.5399,
      "step": 13250
    },
    {
      "epoch": 0.9071068067112263,
      "grad_norm": 5.610267162322998,
      "learning_rate": 1.8592279361615063e-05,
      "loss": 0.5063,
      "step": 13300
    },
    {
      "epoch": 0.9105169826763061,
      "grad_norm": 8.231721878051758,
      "learning_rate": 1.79102441685991e-05,
      "loss": 0.5671,
      "step": 13350
    },
    {
      "epoch": 0.9139271586413859,
      "grad_norm": 3.7642412185668945,
      "learning_rate": 1.722820897558314e-05,
      "loss": 0.5433,
      "step": 13400
    },
    {
      "epoch": 0.9173373346064657,
      "grad_norm": 6.02592658996582,
      "learning_rate": 1.654617378256718e-05,
      "loss": 0.4924,
      "step": 13450
    },
    {
      "epoch": 0.9207475105715455,
      "grad_norm": 9.914741516113281,
      "learning_rate": 1.586413858955122e-05,
      "loss": 0.4777,
      "step": 13500
    },
    {
      "epoch": 0.9241576865366253,
      "grad_norm": 2.5167136192321777,
      "learning_rate": 1.5182103396535264e-05,
      "loss": 0.5086,
      "step": 13550
    },
    {
      "epoch": 0.9275678625017051,
      "grad_norm": 11.391505241394043,
      "learning_rate": 1.4500068203519301e-05,
      "loss": 0.6198,
      "step": 13600
    },
    {
      "epoch": 0.9309780384667848,
      "grad_norm": 4.471643447875977,
      "learning_rate": 1.3818033010503341e-05,
      "loss": 0.5794,
      "step": 13650
    },
    {
      "epoch": 0.9343882144318647,
      "grad_norm": 7.734982490539551,
      "learning_rate": 1.3135997817487384e-05,
      "loss": 0.4314,
      "step": 13700
    },
    {
      "epoch": 0.9377983903969445,
      "grad_norm": 4.202435493469238,
      "learning_rate": 1.2453962624471423e-05,
      "loss": 0.4639,
      "step": 13750
    },
    {
      "epoch": 0.9412085663620243,
      "grad_norm": 1.2485597133636475,
      "learning_rate": 1.1771927431455463e-05,
      "loss": 0.4508,
      "step": 13800
    },
    {
      "epoch": 0.9446187423271041,
      "grad_norm": 6.124668121337891,
      "learning_rate": 1.1089892238439505e-05,
      "loss": 0.523,
      "step": 13850
    },
    {
      "epoch": 0.9480289182921838,
      "grad_norm": 4.722700595855713,
      "learning_rate": 1.0407857045423544e-05,
      "loss": 0.5537,
      "step": 13900
    },
    {
      "epoch": 0.9514390942572637,
      "grad_norm": 3.050891637802124,
      "learning_rate": 9.725821852407585e-06,
      "loss": 0.4788,
      "step": 13950
    },
    {
      "epoch": 0.9548492702223434,
      "grad_norm": 2.3516623973846436,
      "learning_rate": 9.043786659391625e-06,
      "loss": 0.5351,
      "step": 14000
    },
    {
      "epoch": 0.9582594461874233,
      "grad_norm": 8.163704872131348,
      "learning_rate": 8.361751466375666e-06,
      "loss": 0.524,
      "step": 14050
    },
    {
      "epoch": 0.9616696221525031,
      "grad_norm": 11.719694137573242,
      "learning_rate": 7.679716273359706e-06,
      "loss": 0.4918,
      "step": 14100
    },
    {
      "epoch": 0.9650797981175828,
      "grad_norm": 13.361838340759277,
      "learning_rate": 6.997681080343746e-06,
      "loss": 0.5274,
      "step": 14150
    },
    {
      "epoch": 0.9684899740826627,
      "grad_norm": 5.151565074920654,
      "learning_rate": 6.315645887327787e-06,
      "loss": 0.4434,
      "step": 14200
    },
    {
      "epoch": 0.9719001500477424,
      "grad_norm": 4.783565044403076,
      "learning_rate": 5.633610694311827e-06,
      "loss": 0.5765,
      "step": 14250
    },
    {
      "epoch": 0.9753103260128223,
      "grad_norm": 1.047197699546814,
      "learning_rate": 4.951575501295867e-06,
      "loss": 0.4324,
      "step": 14300
    },
    {
      "epoch": 0.9787205019779021,
      "grad_norm": 4.595932960510254,
      "learning_rate": 4.269540308279908e-06,
      "loss": 0.4841,
      "step": 14350
    },
    {
      "epoch": 0.9821306779429818,
      "grad_norm": 3.0177667140960693,
      "learning_rate": 3.5875051152639477e-06,
      "loss": 0.5692,
      "step": 14400
    },
    {
      "epoch": 0.9855408539080617,
      "grad_norm": 8.052188873291016,
      "learning_rate": 2.9054699222479883e-06,
      "loss": 0.5229,
      "step": 14450
    },
    {
      "epoch": 0.9889510298731414,
      "grad_norm": 3.5134165287017822,
      "learning_rate": 2.2234347292320284e-06,
      "loss": 0.5264,
      "step": 14500
    },
    {
      "epoch": 0.9923612058382213,
      "grad_norm": 10.2735595703125,
      "learning_rate": 1.5413995362160688e-06,
      "loss": 0.5377,
      "step": 14550
    },
    {
      "epoch": 0.995771381803301,
      "grad_norm": 8.025547981262207,
      "learning_rate": 8.593643432001091e-07,
      "loss": 0.54,
      "step": 14600
    },
    {
      "epoch": 0.9991815577683808,
      "grad_norm": 5.137548446655273,
      "learning_rate": 1.7732915018414952e-07,
      "loss": 0.4685,
      "step": 14650
    },
    {
      "epoch": 1.0,
      "eval_accuracy": 0.81930484155605,
      "eval_loss": 0.4972057044506073,
      "eval_runtime": 115.8035,
      "eval_samples_per_second": 112.544,
      "eval_steps_per_second": 7.038,
      "step": 14662
    }
  ],
  "logging_steps": 50,
  "max_steps": 14662,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.4092361227884996e+16,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
